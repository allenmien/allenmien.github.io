<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[距离度量方法]]></title>
    <url>%2F2020%2F08%2F16%2F%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[距离度量方法欧氏距离 曼哈顿距离顾名思义，在曼哈顿街区要从一个十字路口开车到另一个十字路口，驾驶距离显然不是两点间的直线距离。这个实际驾驶距离就是“曼哈顿距离”。曼哈顿距离也称为“城市街区距离”(City Block distance)。 切比雪夫距离 闵可夫斯基距离 标准化欧氏距离 标准化欧氏距离是针对欧氏距离的缺点而作的一种改进。标准欧氏距离的思路：既然数据各维分量的分布不一样，那先将各个分量都“标准化”到均值、方差相等。假设样本集X的均值(mean)为m，标准差(standard deviation)为s，X的“标准化变量”表示为： 马氏距离马氏距离是基于样本分布的一种距离。物理意义就是在规范化的主成分空间中的欧氏距离。所谓规范化的主成分空间就是利用主成分分析对一些数据进行主成分分解。再对所有主成分分解轴做归一化，形成新的坐标轴。由这些坐标轴张成的空间就是规范化的主成分空间。 定义：有M个样本向量X1~Xm，协方差矩阵记为S，均值记为向量μ，则其中样本向量X到μ的马氏距离表示为： 欧式距离&amp;马氏距离： 马氏距离的特点： 量纲无关，排除变量之间的相关性的干扰； 马氏距离的计算是建立在总体样本的基础上的，如果拿同样的两个样本，放入两个不同的总体中，最后计算得出的两个样本间的马氏距离通常是不相同的，除非这两个总体的协方差矩阵碰巧相同； 计算马氏距离过程中，要求总体样本数大于样本的维数，否则得到的总体样本协方差矩阵逆矩阵不存在，这种情况下，用欧式距离计算即可。 余弦距离几何中，夹角余弦可用来衡量两个向量方向的差异；机器学习中，借用这一概念来衡量样本向量之间的差异。 夹角余弦取值范围为[-1,1]。余弦越大表示两个向量的夹角越小，余弦越小表示两向量的夹角越大。当两个向量的方向重合时余弦取最大值1，当两个向量的方向完全相反余弦取最小值-1。 汉明距离定义 两个等长字符串s1与s2的汉明距离为：将其中一个变为另外一个所需要作的最小字符替换次数。 汉明重量 是字符串相对于同样长度的零字符串的汉明距离，也就是说，它是字符串中非零的元素个数。 因此，如果向量空间中的元素a和b之间的汉明距离等于它们汉明重量的差a-b。 杰卡德距离杰卡德相似系数(Jaccard similarity coefficient)：两个集合A和B的交集元素在A，B的并集中所占的比例，称为两个集合的杰卡德相似系数。 杰卡德距离(Jaccard Distance)：与杰卡德相似系数相反，用两个集合中不同元素占所有元素的比例。 相关距离相关系数：是衡量随机变量X与Y相关程度的一种方法，相关系数的取值范围是[-1,1]。相关系数的绝对值越大，则表明X与Y相关度越高。当X与Y线性相关时，相关系数取值为1（正线性相关）或-1（负线性相关）： 信息熵 以上的距离度量方法度量的皆为两个样本（向量）之间的距离，而信息熵描述的是整个系统内部样本之间的一个距离，或者称之为系统内样本分布的集中程度（一致程度）、分散程度、混乱程度（不一致程度）。系统内样本分布越分散(或者说分布越平均)，信息熵就越大。分布越有序（或者说分布越集中），信息熵就越小。]]></content>
  </entry>
  <entry>
    <title><![CDATA[深度学习中的人工神经网络]]></title>
    <url>%2F2020%2F08%2F16%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[人工神经网络 根据目标，去帮你做特征工程 然后，做出了特征工程后，其实最后一层其实是一个简单的线性分类器 神经网络的来源 只有一层的神经网络，是和线性回归是一样的 添加少量的隐层，变成了浅层神经网络，也就是SNN 增加中间层，变为深度神经网络，也就是DNN 神经网络为什么在分类问题效果比较好原则上，神经网络是可以同时解决回归和分类问题的。只是回归类问题比较难做到高的准确度。 对于分类问题解决的比较好。 当LR和SVM对于非线性可分，怎么处理呢？其实做好特征映射或者特征工程，其实也是可以做的。 但是问题在于，你不知道特征映射怎么做。 我们希望机器自己能够去构建一些特征，而不是人或者遍历等很大开销去做这个事情。 例如下面： 当一条直线很难去分隔这4个点时，我们可以尝试使用两条直线来分割这4个数据。其中两条直线之间的为一类，两条直线之外的为一类。 神经网络理解 两条线之间的交集，可以认为是一类。同样的，假如一条直线很难将数据集分来的话，我们可以构建很多条直线，并求交集，这样在这些交集内的就是一类，交集外的是另一类。 那么直线的条数是多少呢，其实就是第一隐层的神经元数量。而由样本点连接第一隐层的线就待表对应的权重也就是参数。 那么第二隐层做的是什么呢，他其实是把第一隐层划分出的分类，并集在一起。可以理解为第一隐层是交集，第二隐层的并集 在进行计算是，每一个隐层的Loss Function都要经过一个sgmod函数的转化，得到Z值，而这个Z值是作为下一个隐层的输入进行的。 而最终形成的决策边界是什么的，即是第一层隐层的w，b组成的很多条直线，而如果这些直线足够多，就形成了一条曲线，或者平面，或者等等的一个边界。 最终的目标也就是通过反向传播和随机梯度下降的方法，不停的迭代，使得Loss Function小于一定的值，这样就代表收敛，也就是把样本库进行很好的分类的w和b已经求出，也就是这个模型已经求出。 那么拿这个模型，对于新的数据，就可以根据这个分类器进行分类。 线性的分类器做非线性的分割 神经元完成逻辑【与】 对于下面的两个直线，可以有两组权重组成两个神经元，在P1和P2之间的即是我们想要的数据，也就是P1和P2两个神经元的交集。 那么如何表示两个神经元的交集呢？如上面的逻辑与，我们可以对第一隐层的结果到输出层，设置权重z=20x1 + 20x2 -30 (此处只是假设参数是这样，代表肯定可以能够找到这样的一组数值来解释两个神经元之间的交集关系和交集结果 ) 也就是说，第一次的从输入层到隐层的权重和输入的（x1, x2）计算出的结果，来代表输入的0或1，第二次的从隐层到输出层的权重代表的是对0和1做交集。也就是说第一次的从输入层到隐层做的是模拟出两条直线，第二次的从隐层到输出层做的是描述出这两个直线的方向，也就是这两条直线范围之间的数据。 神经元完成逻辑【或】 神经元分类的解释 逻辑【与】和逻辑【或】的组合 像每一个绿色，其实是多个线性分类器的与 而多个绿色，其实是多个色块的或 我觉得这样的就是上述的神经元再加一个隐层，也就是输入层，逻辑与隐层，逻辑或隐层，输出层 神经网络的结构 神经网络的激活函数 神经网络之BP算法 Loss Function：算出的结果和真实结果之间的差距 目标是不断的缩小 Loss Function “正向传播”求损失，“反向传播”回传误差 SGD：随机梯度下降。梯度下降是在全部的范围内找下降梯度最大的方向。而随机梯度下降只是找了一批的样本。 如果使用梯度下降，由于不是一个凸函数，所以可能有多个局部最优解，使用梯度下降有可能在一个局部最优解里就出来了。 因为随机梯度是找的随机的一批，并非全部，所以只是这一批的的局部下降最快的方向，所以有可能会跳出凸函数的局部最优解。 人工神经网络例子先制造一批数据123456789import numpy as npfrom sklearn.datasets import make_moonsimport matplotlib.pyplot as pltplt.ion()np.random.seed()X, y = make_moons(200, noise=0.20)plt.scatter(X[:, 0], X[:, 1], s=40, c=y, cmap=plt.cm.Spectral)plt.show() 使用传统的逻辑回归进行分类12345678910111213141516171819202122232425262728from sklearn.linear_model import LogisticRegressionCVdef plot_descision_boundary(pred_func): #设定最大最小值，附加一点点边缘填充 x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5 y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5 h = 0.01 xx, yy = np.meshgrid( np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) # 用预测函数预测一下 Z = pred_func(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) #画图 plt.contour(xx, yy, Z, cmap=plt.cm.Spectral) plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)# 逻辑斯特回归的分类效果clf = LogisticRegressionCV()clf.fit(X, y)# 决策边界plot_descision_boundary(lambda x: clf.predict(x))plt.title("Logistic Regression")plt.show() 使用人工神经网络进行分类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798num_examples = len(X)nn_input_dim = 2nn_output_dim = 2# 梯度下降参数epsilon = 0.01reg_lambda = 0.01# 定义损失函数def calculate_loss(model): W1, b1, W2, b2 = model["W1"], model["b1"], model["W2"], model["b2"] # 向前推进，前向运算 # 第一个隐层 z1 = X.dot(W1) + b1 # 激活函数 a1 = np.tanh(z1) # 第二个隐层 z2 = a1.dot(W2) + b2 # 指数运算 exp_scores = np.exp(z2) # 概率 probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # 计算损失 corect_logprobs = -np.log(probs[range(num_examples), y]) data_loss = np.sum(corect_logprobs) # 正则化 data_loss += reg_lambda / 2 * (np.square(W1) + np.sum(np.square(W2))) return 1. / num_examples * data_loss# 完整的训练建模函数定义def build_model(nn_hdim, num_passes=20000, print_loss=False): ''' 参数： 1.nn_hdim:隐层节点个数 2.num_passes:梯度下降迭代次数 3.print_loss:设定为True的话，每1000次迭代输出一次loss的当前值 ''' # 随机初始化权重 np.random.seed(0) W1 = np.random.rand(nn_input_dim, nn_hdim) / np.sqrt(nn_input_dim) b1 = np.zeros((1, nn_hdim)) W2 = np.random.rand(nn_hdim, nn_output_dim) / np.sqrt(nn_hdim) b2 = np.zeros((1, nn_output_dim)) # 最后学到的模型 model = &#123;&#125; # 开始梯度下降 for i in xrange(0, num_passes): # 向前计算loss z1 = X.dot(W1) + b1 a1 = np.tanh(z1) z2 = a1.dot(W2) + b2 exp_scores = np.exp(z2) probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # 反向传播 delta3 = probs delta3[range(num_examples), y] -= 1 dW2 = (a1.T).dot(delta3) db2 = np.sum(delta3, axis=0, keepdims=True) delta2 = delta3.dot(W2.T) * (1 - np.power(a1, 2)) dW1 = np.dot(X.T, delta2) db1 = np.sum(delta2, axis=0) # 加上正则化项 dW2 += reg_lambda * W2 dW1 += reg_lambda * W1 # 梯度下降更新参数 W1 += -epsilon * dW1 b1 += -epsilon * db1 W2 += -epsilon * dW2 b2 += -epsilon * db2 # 得到的模型其实是这些权重 model = &#123;'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2&#125; # 打印中间过程 # if print_loss and i % 1000 == 0: # print 'Loss after interation %i: %f' % (i, calculate_loss(model)) return model# 定义判定结果的函数def predict(model, x): W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2'] # 前向运算 z1 = x.dot(W1) + b1 a1 = np.tanh(z1) z2 = a1.dot(W2) + b2 exp_scores = np.exp(z2) # 计算概率输出最大概率对应的类别 probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) return np.argmax(probs, axis=1) 1234567# 建立3个隐层的神经网络model = build_model(3, print_loss=True)# 画出决策/判定边界plot_descision_boundary(lambda x: predict(model, x))plt.title('Decision Boundary for hidden layer size 3')plt.show() 123456789# 不同的隐层对结果的影响plt.figure(figsize=(16, 32))hidden_layer_dimensions = [1, 2, 3, 4, 5, 20, 50]for i, nn_hdim in enumerate(hidden_layer_dimensions): plt.subplot(5, 2, i + 1) plt.title('Hidden Layer size %d' % nn_hdim) model = build_model(nn_hdim) plot_descision_boundary(lambda x: predict(model, x))plt.show()]]></content>
  </entry>
  <entry>
    <title><![CDATA[深度学习中的卷积神经网络]]></title>
    <url>%2F2020%2F08%2F16%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[卷积神经网络神经网络到卷积神经网络(CNN) 当层次开始非常多的时候，计算量会非常大。 过拟合的问题。最根本的原因的神经元过多了。 问题： 把w的个数降下来。 又有很强的学习能力。 卷积神经网络之层级结构 数据输入层（Input layer） 卷积计算层（CONV layer） 激励层(ReLU layer) 池化层(Pooling layer) 全连接层（FC layer） 数据输入层三种常见的图像数据处理方式 去均值 把输入数据各个纬度都中心化到0 归一化 幅度归一化到同样的范围 PCA/白化 用PCA降维 白化是对数据每个特征轴上的幅度归一化 卷积层（CONV layer） 局部关联 窗口滑动（receptive field）滑动，filter对全局数据计算 权重共享 概念理解 某个神经元，只负责观测某一个维度的信息，再做汇总。 一个框在左右上下滑，直到所有的数据都在这个框中至少出现过一次。 整个面是32 * 32维的，窗口大小为3 * 3的，那么对于第一个神经元，对于视野内的9个像素点，有9个连接，也就是有9个权重 当移动窗口进行移动的时候，按照道理，换了不同的数据，9条连接的权重应该是会变化的。 卷积做了一件事件，他让这个权重，不管窗口的移动位置，是保持不变的。 上述说的都是一个神经元做的事情。 那么对于第二个神经元，他做的事情和第一个神经元做的事情是相同的，只是这9个 连接的权重与第一个神经元不同。 意义 第一个神经元在观测颜色，第二个神经元在观测纹理，第三个神经元在观测轮廓。 这只是一个通俗的解释，实际的意思是，每一个神经元都在观测某一个维度，但是不一定是在物理上是可以解释的。 这样的话，确实是在降低w和b的个数，也确实是在减少参数。 几个概念深度（depth）下一层的神经元的个数 步长(stride)举例： 3 * 3 的窗口进行滑动的时候，不一定的边和边紧挨着进行滑动的。 比方下一个滑动的位置竖向上，滑动了一个单元，而实际上有2列是重复的。 那么这个步长其实就是1 他只滑动了一个位置 填充值(zero-padding) 他的意思的说你设定好了窗口和步长，但是整个32 * 32并不一定被3 * 3和步长所整除，所以一定会出现滑不下去的情况。 为了解决这个问题，在滑不下去的时候，就在32 * 32的外面补个两列0,使得是能够继续滑下去的。 激励层进行下一层传递的时候，信息的约束。 常用的激活函数 Sigmoid Tanh(双曲正切 ) ReLU(修正线性单元–最常用) Leaky ReLU（下面的几个都是ReLU的几个变种） ELU Maxout SigmoidSigmoid函数的问题在于，当值非常大或者非常小的时候，斜率非常的小，趋近于0。 所以反向传播的时候，迭层之后可能为0。就起不到进行传播的作用了。 Tanh ReLU Leaky ReLU ELU Maxout 实际经验 不要用sigmoid！不要用sigmoid！不要用sigmoid！ 首先试RELU，因为快，但要小心点 如果2失效，请用Leaky ReLU或者Maxout 某些情况下tanh倒是有不错的结果，但是很少 池化层 夹在连续的卷积层中间 池化层是没有参数的。 他做了一个下采样，或者数据压缩，减少过拟合。 下采样的时候是会取一个数据窗口的。 在这个数据窗口中，经过池化层的作用之后，只输出一个值。 两种常用的方式 Max pooling(用的比较多) 可以理解为在窗口内取最大值 Average pooling 可以理解为在窗口内，取平均值 全连接层 两层之间的所有神经元都有权重连接 通常全连接在卷积神经元的网络尾部 CNN的结构 INPUT [[CONV -&gt; RELU]N -&gt; POOL?]\M [FC -&gt; RELU]*K FC 训练算法 先定义Loss Function 找到最小化损失函数的w和b，CNN中使用的算法是SGD Softmax是逻辑回归的升级版，是多分类的 还是使用BP算法，反向传播 总结 BP算法利用链式法则，逐级相乘直到求出dW和db 利用SGD随机梯度下降，迭代和更新W和 b 优缺点优点 共享卷积核，对高维数据处理无压力 无需手动选取特征，训练好权重，即得特征 分类效果好 缺点 需要调参，需要大样本量，训练最好要GPU 物理涵义不明确 典型的CNN案例实际上，我们大部分的工作都是在下面的 一些神经网络上进行的训练，并没有重新搭建新的神经网络。 LeNet，这是最早用于数字识别的CNN AlexNet，2012 ILSVRC比赛远超第2名的CNN，比LeNet更深，用多层小卷积层叠加替换单大卷积层。 ZF Net，2013 ILSVRC比赛冠军 GoogLeNet，2014 ILSVRC比赛冠军 VGGNet，2014 ILSVRC比赛中的模型，图像识别略差于GoogLeNet，但是在很多图像转化学习问题(比如object detection)上效果奇好 ResNet，2015年 ILSVRC比赛的冠军，微软打造，层次极深(152层) LeNet AlexNet fine-tuning概念使用已用于其他目标，训练好模型的权重或者部分权重，作为初始值开始训练 原因 自己从头训练卷积神经网络容易出现问题 fine-tuning能很快收敛到一个较理想的状态 做法 复用相同层的权重，新定义层取随机权重初始值（前面的层可以使用别人的权重，后面的层，可以自己添加随机初始化权重） 调大新定义层的学习率，调小复用层学习率 常用框架 Caffe 源于Berkeley的主流CV工具包，支持C++,python,matlab Model Zoo中有大量预训练好的模型供使用 Mxnet Chentianqi团队主导的package 对显存利用率非常高 定义网络层简单 TensorFlow Google的深度学习框架 TensorBoard可视化很方便，实时监控 数据和模型并行化好，速度快]]></content>
  </entry>
  <entry>
    <title><![CDATA[维比特算法]]></title>
    <url>%2F2020%2F08%2F16%2F%E7%BB%B4%E6%AF%94%E7%89%B9%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[维比特算法问题动态规划最佳的路径： 路径总数为： 3*3*3=27 每次从头算到尾，每一条路径需要经过4次加法，共： 27*4 = 108 维比特算法的推演假设我们处于位置t（有n个选择），如果我们想要知道t+1（m个选择）步应该怎么走，我们需要知道从(0-t)共n个路径的值（这个路径的值已经是前面的最小的了）是多少。还需要知道从t到t+1的对于n中的每一个，对应的m个选择的路径是多少。这样对于位置t上的每一个n的（0, t）已知，（t , t+1）已知，就可以通过计算得到对于t+1时刻的m个选择中的每一个，(0, t+1)的最短路径分别是什么了。 简单来讲，就是当处于t时刻时，分别独立计算t时刻，经过n个神经元中的任何一个时，该神经元的历史最小值和未来的最小值。 从A到B现在我们在A点，因为下一步如果我们在（B1, B2, B3）点，去思考B1该往哪一个(C1, C2,C3)走时，需要知道B1之前的所有路径的最短的值。 PS : 这里会有疑惑，如果我已经到了B1，选择往(C1, C2,C3)走时，只要选择最短的一个比方说5不就行了，为什么还需要计算A-B1最短的值呢？ 原因是：我们其中在在每一步时，不仅仅是做判断，很重要的是记录下路径的值，这样我们才能在最后一步的时候，把全部的值，也就是全局的值加起来，进行计算全局的最小值，也就是最短路径。 假如，我们已经选了B1-C1=5作为选择，但是还是要计算A-B1=6的 A-B1-C1 = 6+5 =11 而：A-B2 =7, B2-C2 = 3 A-B2-C2 = 7+ 3 =10 我们需要通过这种方式去进行全局的判断。 也就是说，我们每一步的选择都不是选择，而只是在为最后一步的最终判决积累数据而已。 简单来讲，如果我们已经处于t时刻（n）, 我们只需要计算出t+1时刻（m）,m个t+1时刻的最短路径值为下一步积累数据就好了。 在A-B的例子中： 我们在A时刻，我们需要计算B时刻，3个选择，过往的最短路径值是多少就可以了，那么经过B1, B2, B3的最短路径值为： 计算经过B1的路径： A-B1 = 6 计算经过B2的路径： A-B2 = 7 计算经过B3的路径： A-B3 = 5 计算复杂度 = 3次加法 min(B1, B2, B3) = [6，7，5] 所有通过B1的路径中：A-B1是最短的，为6； 所有通过B2的路径中：A-B2是最短的，为7； 所有通过B3的路径中：A-B3是最短的，为5； 下一步计算时，与A再无关系，只基于B1、B2、B3三个出发点和（6，7，5）对应的初始值（历史值）进行计算： B1（A-B1=6）; B2（A-B2=7）; B3（A-B3=5）; 从B到C 我们在B时刻，我们需要计算C时刻，3个选择，经过C1、C2、C3的最短路径就好了： 计算经过C1的路径： A-B1-C1 = 6+5 = 11 A-B2-C1 = 7+4 = 11 A-B3-C1 = 5+4 = 9 计算经过C2的路径： A-B1-C2 = 6+6 = 12 A-B2-C2 = 7+3 = 10 A-B3-C2 = 5+6 = 11 计算经过C3的路径： A-B1-C3 = 6+9 = 15 A-B2-C3 = 7+7 = 14 A-B3-C3 = 5+6 = 11 计算复杂度 = 3*3 = 9 次加法 那么C1, C2,C3的最短路径为： min(C1, C2, C3) = [A-B3-C1=5+4=9, A-B2-C2=7+3=10, A-B3-C3=5+4=11] 所有通过C1的路径中：A-B3-C1是最短的，为9； 所有通过C2的路径中：A-B2-C2是最短的，为10； 所有通过C3的路径中：A-B3-C3是最短的，为11； 下一步计算时，与A、B再无关系，只基于C1、C2、C3三个出发点和（9，10，11）对应的初始值（历史值）进行计算： C1（9）; C2（10）; C3（11）; 从C到D 我们现在处在C时刻，需要计算D时刻，3个选择，经过D1、D2、D3的最短路径： 计算经过D1的路径： C1-D1 = (A-B3-C1) - D1 = 9 + 7 = 16 C2-D1 = (A-B2-C2) - D1 = 10 + 5 = 15 C3-D1 = (A-B3-C3) - D1 = 11 + 5 = 16 计算经过D2的路径： C1-D2 = (A-B3-C1) - D2 = 9 + 8 = 17 C2-D2 = (A-B2-C2) - D2 = 10 + 4 = 14 C3-D2 = (A-B3-C3) - D2 = 11 + 7 = 18 计算经过D3的路径： C1-D3 = (A-B3-C1) - D3 = 9 + 3 = 12 C2-D3 = (A-B2-C2) - D3 = 10 + 3 = 13 C3-D3 = (A-B3-C3) - D3 = 11 + 6 = 17 计算复杂度 = 3 *3 = 9 次加法 min(D1, D2,D3) = [A-B2-C2-D1=10+5=15，A-B2-C2-D2=10+4=14, A-B3-C1-D3=9+3=12 ] 所有通过D1的路径中：A-B2-C2-D1是最短的； 所有通过D2的路径中：A-B2-C2-D2是最短的； 所有通过D3的路径中：A-B3-C1-D3是最短的； 下一步计算时，与A、B、C再无关系，只基于D1、D2、D3三个出发点和（9，10，11）对应的初始值（历史值）进行计算： D1（15）; D2（14）; D3（12）; 从D到E 现在我们处于D时刻，需要计算E时刻，1个选择： 计算经过E的路径： D1-E = A-B2-C2-D1-E = 15 + 4 = 19 D2-E = A-B2-C2-D2-E = 14 + 8 = 22 D3-E = A-B3-C1-D3-E = 12 + 5 = 17 计算复杂度 = 3次加法 最算路径为： min(E) = A-B3-C1-D3-E = 12+5=17 总结计算复杂度维比特计算复杂度 = 3+3*3+3*3+3 = 24 次加法 可以看到相对于开始的108次加法，我们只是需要24次即可。 而且在可见的未来，如果中间增加了一层，原来的遍历算法需要增加： （3*3*3*3）4 = 108\3 = 324 基本上是每一层的个数乘以原来所有的路径 可是维比特是计算复杂度只是： 3 + 3*3 + 3*3 + 3*3 +3 = 24+9 = 33 对比一下，遍历算法的本质上叠成，而维比特算法的本质是相加. 维比特降低计算复杂度的直观理解我们拿从C到D举例： 其中的D1 : 计算经过D1的路径： C1-D1 = (A-B3-C1) - D1 = 9 + 7 = 16 C2-D1 = (A-B2-C2) - D1 = 10 + 5 = 15 C3-D1 = (A-B3-C3) - D1 = 11 + 5 = 16 此时我们计算C1-D1，只用了一次加法计算，但是实际上，如果真正的计算经过D1的路径有多少条呢？ 3*3 = 9 条，也就是说，如果我们不把历史记录下来，每一次我们多走一步都要重新计算的话，我们每次都要指数级的计算叠层每一种可能。 而维比特相当于，把历史记录下来，并取最短的那个也记录下来，以后再计算的时候，都基于这个最短的计算，其他的都不计算了。 比如我们计算C1-D1的时候，我们只计算(A-B3-C1) - D1 = 9 + 7 = 16 的路径。 却理都不理：(A-B1-C1)-D1 = 11+7 = 18 的路径。 而且以后的，凡是经过D1的的路径，都再也理都不理(A-B1-C1)-D1 这种可能了。 因为我们已经证实，在所有C1-D1之前的经过C1的路径中，已经没有比A-B3更小的了。也就没有必要再去计算别的可能了。 可以看到通过这种方式，如果层数增加，这种算法省去掉不需要的计算步骤是非常非常非常可观的。 维比特也就通过这种基于历史和未来的一小步的方法，使得在全局范围可控并全局最优，且计算量可以接受。]]></content>
  </entry>
  <entry>
    <title><![CDATA[自然语言处理中的TD-IDF、PageRank、TextRank]]></title>
    <url>%2F2020%2F08%2F16%2F%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B8%AD%E7%9A%84TD-IDF%E3%80%81PageRank%E3%80%81TextRank%2F</url>
    <content type="text"><![CDATA[TF-IDF、PageRank、TextRankTF-IDF什么是TF-IDF 字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。 词频 (term frequency, TF) 指的是某一个给定的词语在该文件中出现的次数。 逆向文件频率 (inverse document frequency, IDF) TF-IDF PageRank算法来源 算法原理 算法实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980# -*-coding:utf-8-*-"""@Time : 2018/7/25 9:23@Author : Mark@File : page_rank.py"""# -*- coding: utf-8 -*-from pygraph.classes.digraph import digraphclass PRIterator: __doc__ = '''计算一张图中的PR值''' def __init__(self, dg): self.damping_factor = 0.85 # 阻尼系数,即α self.max_iterations = 100 # 最大迭代次数 self.min_delta = 0.00001 # 确定迭代是否结束的参数,即ϵ self.graph = dg def page_rank(self): # 先将图中没有出链的节点改为对所有节点都有出链 for node in self.graph.nodes(): if len(self.graph.neighbors(node)) == 0: for node2 in self.graph.nodes(): digraph.add_edge(self.graph, (node, node2)) nodes = self.graph.nodes() graph_size = len(nodes) if graph_size == 0: return &#123;&#125; page_rank = dict.fromkeys(nodes, 1.0 / graph_size) # 给每个节点赋予初始的PR值 damping_value = ( 1.0 - self.damping_factor) / graph_size # 公式中的(1−α)/N部分 flag = False for i in range(self.max_iterations): change = 0 for node in nodes: rank = 0 for incident_page in self.graph.incidents(node): # 遍历所有“入射”的页面 rank += self.damping_factor * ( page_rank[incident_page] / len( self.graph.neighbors(incident_page))) rank += damping_value change += abs(page_rank[node] - rank) # 绝对值 page_rank[node] = rank print("This is NO.%s iteration" % (i + 1)) print(page_rank) if change &lt; self.min_delta: flag = True break if flag: print("finished in %s iterations!" % node) else: print("finished out of 100 iterations!") return page_rankif __name__ == '__main__': dg = digraph() dg.add_nodes(["A", "B", "C", "D", "E"]) dg.add_edge(("A", "B")) dg.add_edge(("A", "C")) dg.add_edge(("A", "D")) dg.add_edge(("B", "D")) dg.add_edge(("C", "E")) dg.add_edge(("D", "E")) dg.add_edge(("B", "E")) dg.add_edge(("E", "A")) pr = PRIterator(dg) page_ranks = pr.page_rank() print("The final page rank is\n", page_ranks) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354This is NO.1 iteration&#123;'A': 0.2, 'C': 0.08666666666666667, 'B': 0.08666666666666667, 'E': 0.31050000000000005, 'D': 0.1235&#125;This is NO.2 iteration&#123;'A': 0.29392500000000005, 'C': 0.11327875000000001, 'B': 0.11327875000000001, 'E': 0.27940540625, 'D': 0.16142221875&#125;This is NO.3 iteration&#123;'A': 0.26749459531249997, 'C': 0.10579013533854167, 'B': 0.10579013533854167, 'E': 0.3020913084941407, 'D': 0.15075094285742188&#125;This is NO.4 iteration&#123;'A': 0.2867776122200196, 'C': 0.1112536567956722, 'B': 0.1112536567956722, 'E': 0.2999867138432907, 'D': 0.1585364609338329&#125;This is NO.5 iteration&#123;'A': 0.2849887067667971, 'C': 0.11074680025059253, 'B': 0.11074680025059253, 'E': 0.30595816211326343, 'D': 0.15781419035709435&#125;This is NO.6 iteration&#123;'A': 0.29006443779627394, 'C': 0.11218492404227762, 'B': 0.11218492404227762, 'E': 0.3071778399574342, 'D': 0.1598635167602456&#125;This is NO.7 iteration&#123;'A': 0.2911011639638191, 'C': 0.11247866312308208, 'B': 0.11247866312308208, 'E': 0.3092942847281384, 'D': 0.16028209495039195&#125;This is NO.8 iteration&#123;'A': 0.2929001420189177, 'C': 0.11298837357202668, 'B': 0.11298837357202668, 'E': 0.31029995701216717, 'D': 0.161008432340138&#125;This is NO.9 iteration&#123;'A': 0.29375496346034213, 'C': 0.11323057298043027, 'B': 0.11323057298043027, 'E': 0.31122614803916593, 'D': 0.16135356649711313&#125;This is NO.10 iteration&#123;'A': 0.29454222583329104, 'C': 0.11345363065276579, 'B': 0.11345363065276579, 'E': 0.3118039106048226, 'D': 0.16167142368019125&#125;This is NO.11 iteration&#123;'A': 0.2950333240140992, 'C': 0.1135927751373281, 'B': 0.1135927751373281, 'E': 0.3122514984282559, 'D': 0.16186970457069255&#125;This is NO.12 iteration&#123;'A': 0.29541377366401755, 'C': 0.11370056920480498, 'B': 0.11370056920480498, 'E': 0.312557474621215, 'D': 0.1620233111168471&#125;This is NO.13 iteration&#123;'A': 0.2956738534280328, 'C': 0.11377425847127598, 'B': 0.11377425847127598, 'E': 0.3127819940001969, 'D': 0.16212831832156827&#125;This is NO.14 iteration&#123;'A': 0.29586469490016737, 'C': 0.1138283302217141, 'B': 0.1138283302217141, 'E': 0.3129401916060185, 'D': 0.16220537056594256&#125;This is NO.15 iteration&#123;'A': 0.29599916286511574, 'C': 0.11386642947844947, 'B': 0.11386642947844947, 'E': 0.31305426256607427, 'D': 0.16225966200679048&#125;This is NO.16 iteration&#123;'A': 0.29609612318116313, 'C': 0.11389390156799623, 'B': 0.11389390156799623, 'E': 0.3131354372049671, 'D': 0.16229880973439462&#125;This is NO.17 iteration&#123;'A': 0.2961651216242221, 'C': 0.11391345112686294, 'B': 0.11391345112686294, 'E': 0.3131936384609857, 'D': 0.16232666785577968&#125;This is NO.18 iteration&#123;'A': 0.29621459269183786, 'C': 0.11392746792935407, 'B': 0.11392746792935407, 'E': 0.3132351892873392, 'D': 0.16234664179932953&#125;This is NO.19 iteration&#123;'A': 0.2962499108942383, 'C': 0.11393747475336752, 'B': 0.11393747475336752, 'E': 0.31326492583997373, 'D': 0.1623609015235487&#125;This is NO.20 iteration&#123;'A': 0.2962751869639777, 'C': 0.11394463630646035, 'B': 0.11394463630646035, 'E': 0.3132861775857534, 'D': 0.162371106736706&#125;This is NO.21 iteration&#123;'A': 0.2962932509478904, 'C': 0.11394975443523561, 'B': 0.11394975443523561, 'E': 0.31330137763112553, 'D': 0.16237840007021073&#125;This is NO.22 iteration&#123;'A': 0.29630617098645673, 'C': 0.11395341511282941, 'B': 0.11395341511282941, 'E': 0.31331224432853666, 'D': 0.1623836165357819&#125;This is NO.23 iteration&#123;'A': 0.2963154076792562, 'C': 0.11395603217578926, 'B': 0.11395603217578926, 'E': 0.31332001507954593, 'D': 0.16238734585049966&#125;This is NO.24 iteration&#123;'A': 0.29632201281761406, 'C': 0.11395790363165731, 'B': 0.11395790363165731, 'E': 0.3133255711032878, 'D': 0.16239001267511166&#125;This is NO.25 iteration&#123;'A': 0.29632673543779464, 'C': 0.11395924170737515, 'B': 0.11395924170737515, 'E': 0.31332954395074825, 'D': 0.1623919194330096&#125;This is NO.26 iteration&#123;'A': 0.29633011235813606, 'C': 0.11396019850147188, 'B': 0.11396019850147188, 'E': 0.31333238460743484, 'D': 0.16239328286459742&#125;finished in D iterations!('The final page rank is\n', &#123;'A': 0.29633011235813606, 'C': 0.11396019850147188, 'B': 0.11396019850147188, 'E': 0.31333238460743484, 'D': 0.16239328286459742&#125;) TextRank原理进行关键词提取时，TextRank算法思想和PageRank算法类似，不同的是，TextRank中时以词为节点，以共现关系建立起节点之间的链接。 分词，并去除停用词等噪音 对于每一个词，前后5个词，共10个作为和它link的词。但是比方说，“程序员”在文章中出现了两次，那就是和程序员这个词link的此为这10个词的并集 算法就类似PageRank，进行迭代，直至收敛 算法实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105# -*-coding:utf-8-*-"""@Time : 2018/7/25 12:03@Author : Mark@File : text_rank.py"""import sysfrom pygraph.classes.digraph import digraphreload(sys)sys.setdefaultencoding('utf-8')class PRIterator: __doc__ = '''计算一张图中的PR值''' def __init__(self, dg): self.damping_factor = 0.85 # 阻尼系数,即α self.max_iterations = 100 # 最大迭代次数 self.min_delta = 0.00001 # 确定迭代是否结束的参数,即ϵ self.graph = dg def page_rank(self): # 先将图中没有出链的节点改为对所有节点都有出链 for node in self.graph.nodes(): if len(self.graph.neighbors(node)) == 0: for node2 in self.graph.nodes(): digraph.add_edge(self.graph, (node, node2)) nodes = self.graph.nodes() graph_size = len(nodes) if graph_size == 0: return &#123;&#125; page_rank = dict.fromkeys(nodes, 1.0 / graph_size) # 给每个节点赋予初始的PR值 damping_value = ( 1.0 - self.damping_factor) / graph_size # 公式中的(1−α)/N部分 flag = False for i in range(self.max_iterations): change = 0 for node in nodes: rank = 0 for incident_page in self.graph.incidents(node): # 遍历所有“入射”的页面 rank += self.damping_factor * ( page_rank[incident_page] / len( self.graph.neighbors(incident_page))) rank += damping_value change += abs(page_rank[node] - rank) # 绝对值 page_rank[node] = rank print("This is NO.%s iteration" % (i + 1)) if change &lt; self.min_delta: flag = True break if flag: print("finished in %s iterations!" % node) else: print("finished out of 100 iterations!") return page_rankif __name__ == '__main__': dg = digraph() words_dict = &#123; u"开发": [u"专业", u"程序员", u"维护", u"英文", u"程序", u"人员"], u"软件": [u"程序员", u"分为", u"界限", u"高级", u"中国", u"特别", u"人员"], u"程序员": [u"开发", u"软件", u"分析员", u"维护", u"系统", u"项目", u"经理", u"分为", u"英文", u"程序", u"专业", u"设计", u"高级", u"人员", u"中国"], u"分析员": [u"程序员", u"系统", u"项目", u"经理", u"高级"], u"维护": [u"专业", u"开发", u"程序员", u"分为", u"英文", u"程序", u"人员"], u"系统": [u"程序员", u"分析员", u"项目", u"经理", u"分为", u"高级"], u"项目": [u"程序员", u"分析员", u"系统", u"经理", u"高级"], u"经理": [u"程序员", u"分析员", u"系统", u"项目"], u"分为": [u"专业", u"软件", u"设计", u"程序员", u"维护", u"系统", u"高级", u"程序", u"中国", u"特别", u"人员"], u"英文": [u"专业", u"开发", u"程序员", u"维护", u"程序"], u"程序": [u"专业", u"开发", u"设计", u"程序员", u"编码", u"维护", u"界限", u"分为", u"英文", u"特别", u"人员"], u"特别": [u"软件", u"编码", u"分为", u"界限", u"程序", u"中国", u"人员"], u"专业": [u"开发", u"程序员", u"维护", u"分为", u"英文", u"程序", u"人员"], u"设计": [u"程序员", u"编码", u"分为", u"程序", u"人员"], u"编码": [u"设计", u"界限", u"程序", u"中国", u"特别", u"人员"], u"界限": [u"软件", u"编码", u"程序", u"中国", u"特别", u"人员"], u"高级": [u"程序员", u"软件", u"分析员", u"系统", u"项目", u"分为", u"人员"], u"中国": [u"程序员", u"软件", u"编码", u"分为", u"界限", u"特别", u"人员"], u"人员": [u"开发", u"程序员", u"软件", u"维护", u"分为", u"程序", u"特别", u"专业", u"设计", u"编码", u"界限", u"高级", u"中国"] &#125; words_key_list = list() for k, v in words_dict.iteritems(): words_key_list.append(k) dg.add_nodes(words_key_list) for k, v in words_dict.iteritems(): for i in v: dg.add_edge((k, i)) pr = PRIterator(dg) page_ranks = pr.page_rank() sort_text_rank = sorted(page_ranks.items(), key=lambda d: d[1], reverse=True) for k, v in sort_text_rank: print str(k) + u":" + str(v) print sort_text_rank 1234567891011121314151617181920212223242526272829303132333435363738394041This is NO.1 iterationThis is NO.2 iterationThis is NO.3 iterationThis is NO.4 iterationThis is NO.5 iterationThis is NO.6 iterationThis is NO.7 iterationThis is NO.8 iterationThis is NO.9 iterationThis is NO.10 iterationThis is NO.11 iterationThis is NO.12 iterationThis is NO.13 iterationThis is NO.14 iterationThis is NO.15 iterationThis is NO.16 iterationThis is NO.17 iterationThis is NO.18 iterationThis is NO.19 iterationThis is NO.20 iterationThis is NO.21 iterationfinished in 编码 iterations!程序员:0.101580333895人员:0.0859679826211分为:0.0740249633386程序:0.0740144395754高级:0.0514255757372软件:0.0493472878223中国:0.0492889214488特别:0.049256278494专业:0.0491852387529维护:0.0491848961483系统:0.0466872602095编码:0.0436170552749界限:0.0433726145516开发:0.043302777825项目:0.0406679129266分析员:0.040667762853英文:0.037449970767设计:0.0368904751286经理:0.034092209713 引用声明TF-IDF原理及使用 PageRank算法–从原理到实现 基于TextRank的关键词提取算法]]></content>
  </entry>
  <entry>
    <title><![CDATA[吴恩达机器学习笔记（一）]]></title>
    <url>%2F2020%2F08%2F16%2F%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Machine LearningIntroductionWhat is machine learningTom Mitchell (1998) Well-posed Learning Problem: A computer program is said to learn from experience E with respect to some task T and some performance measure P, if itsperformance on T, as measured by P, improves with experience E. 学习算法 监督学习 非监督学习 监督学习例子1： 如果一个人有房子，面积750，他想知道能卖多少钱？ 可以用一条直线进行拟合，也可以用二元曲线进行拟合。 监督学习是指我们给算法一个数据集，其中包含了正确的答案。 这是一个回归问题。 例子2： 假设你想看医疗记录，并且设法预测乳腺癌是恶性的还是良性的。 假设某人发现了一个乳腺肿瘤，机器学习的问题，是你能否估计出，这个肿瘤是良性的还是恶性的概率。 这是一个分类问题。 分类是指，我们设法预测一个离散值输出，0 or 1， 在这个问题中，只是使用了肿瘤大小这一个特征来表述，实际可能会有多个特征和多个属性。 机器学习做的，是画一条线，区分你的朋友是良性还是恶性肿瘤。 机器学习算法，不仅能处理两个到五个特征，而且能处理无穷多个特征。 无监督学习 我们得到一个数据集，却不知道拿他来干什么，能在其中找到某种结构吗？ 机器学习可以判定该数据集包含两个不同的簇。这就是聚类算法。 一个聚类算法的应用例子就是谷歌新闻。谷歌自动的搜索新闻，并把他们分成一个又一个的专题。 模型描述线性回归 单变量线性回归代价函数 目标即是求代价函数的最小值，也就是能最拟合的模型。 也称平方误差函数，平方误差代价函数。 平方误差代价函数对大多数问题，都是一个合理的选择。是解决回归问题最常用的手段。 代价函数（一） 代价函数（二） 当为3个参数时： 梯度下降法 线性回归的梯度下降 Batch 梯度下降算法 根据不同的初始起点，有可能到达不同的位置 线性回归的代价函数总是一个弓形函数，也就是凸函数 这个函数没有局部最优解，只有全局最优解，只要使用线性回归，总会收敛 线性回归回顾矩阵和向量 矩阵的加法和乘法运算 多变量线性回归多功能 多元线性回归 多元梯度下降法 多元梯度下降法演练1特征缩放 均值归一化 元梯度下降法2–学习率 特征和多项式回归正规方程]]></content>
  </entry>
  <entry>
    <title><![CDATA[机器学习中的PCA主成分分析]]></title>
    <url>%2F2020%2F08%2F16%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[PCA主成分分析法实践其实主成分分析法的实现方法非常明了，一步步的来就可以。但是想了解的更加透彻，知道为什么这样做是可以的，或许需要一步步的推导公式开始，先说实现方法。 数据 假设我们得到的2维数据如下： 第一步分别求x和y的平均值，然后对于所有的样例，都减去对应的均值。这里x的均值是1.81，y的均值是1.91，那么一个样例减去均值后即为（0.69,0.49），得到 第二步求特征协方差矩阵 对角线上分别是x和y的方差，非对角线上是协方差。协方差是衡量两个变量同时变化的变化程度。 第三步求协方差的特征值和特征向量 第四步将特征值按照从大到小的顺序排序，我们选择其中最大的那个，这里是1.28402771，对应的特征向量是 第五步将样本点投影到选取的特征向量上。 结束 对比PCA之前： PCA之后 其实PCA的本质就是减少坐标轴、旋转坐标轴，使得减少坐标轴之后、旋转坐标轴之后的数据特征的离散程度，尽可能的损失的少，尽可能的代表原来的数据的特征。 公式推导放在前面作为一些知识科普 开始推导 写在后面引用1.主成分分析法详解 2.【机器学习】主成分分析详解]]></content>
  </entry>
  <entry>
    <title><![CDATA[吴恩达deep-learning序列模型-序列模型与注意力机制]]></title>
    <url>%2F2020%2F08%2F16%2F%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B%E4%B8%8E%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[序列模型与注意力机制基础模型机器翻译（seq2seq）编码网络和解码网络 这是一个RNN的结构 首先，把法语输入一个RNN，能够得到一个向量。–编码网络 再把这个向量作为输入，输入一个RNN，得到输出的英文。–解码网络 图像描述(image2seq) 首先把图片输入一个CNN，得到一个向量。 再把这个向量作为输入，输入到一个RNN，得到图片对应的描述。 选择最可能的句子可以把机器翻译想象成一个条件语言模型 贪心算法 贪心算法在预测的时候，把编码生成的向量进行输入，然后先预测出第一个位置上最大概率的一个词，再预测第二个位置上最大概率的词，依次。。 这样的缺点是，仅仅是预测在第一个确定的情况下，第二个概率最大的词，这样虽然是概率最大的，但不一定是最好的。 比方说:图片中明显第一个的翻译会比较好。但是第二个最可能会被生成。因为在法语中，going 跟在 Jane is 后面的概率要比 visiting 跟在Jane is 后面的概率要大。 贪心算法的另外一个缺点是： 假如，字典都10,000个词。而我们要预测10个词的句子。 那么我们需要计算，10,000的10次方的概率，并从中挑选出概率最大的。这样会导致计算量过大。 定向搜索集束算法Beam Width ： for example, B = 3 Beam Search 不会只选出概率最大的那一个，他会选出3个。 当编码得到向量，输出第一个词的时候，选择概率最大的3个，而不是把10,000全部都保存下来。 计算第二个词的时候，是基于前面的3个词分别计算后面的10000个的概率的。所以共30000个的概率，从中选择前3的概率，保存下来，去进行第三个词的预测。 改进定向搜索Beam Search的目标就是最大化的字符相乘的概率，总是取前3个。 最开始的时候，是采用下面的公式进行评价的： 但是这样的问题在于，叠成的概率会非常的小，导致计算机在计算的时候4舌5入掉很多浮点数，所以取了对数，如下： 但是这样也是有问题的，因为每个队概率都是非常的小，所以会导致，每多一个单词，概率都会小很多，所以在前面做了一个归一化的处理，并根据经验，给了一个参数用于调节归一化的程度，如下： 如何选择Beam Width10 定向搜索的误差分析如果Beam Search出现错误怎么办？如果结果不理想，如何确认这个误差是由于Beam Search没搜索出来，还是因为RNN模型有问题呢？ 解释： 第一句：法语 第二句 ：人工翻译的结果，作为Target 第三句： 模型预测的Sequence Seq2Seq，是一个encoder和一个decoder，再加上Beam Search选出的Sequence。那么如果效果不理想，那么是RNN出了问题，还是Beam Search出了问题呢？ 确定误差出现在哪里的方法第一种：相当于，Beam Search 选出了一个最大的Sequence，但是这个选出的最大的还没有target到大，这就表明，其实是target理应存在在所有的Beam Width里面的，但是没有被搜索出来，这就说明，是Beam Search出了问题。 第二种：相当于，Beam Search已经选出了最大的概率的Sequence，当然正确的Target也是已经搜索出来了，只是在概率排名上，错误的比正确的高，这就说明Beam Search是生效的，但是RNN都模型是计算有偏差的。 遍历并比较变量每一预测的Sequence，与Target进行比较。只有出现大量的Bean Search的问题，才需要调整Beam Search 的Beam Width。如果RNN出现的问题较多，就可以去调节RNN。 Bleu ScoreBilingual evaluation understudy(双语评估系统) p_1:一元词组 p_2:二元词组 公式： 注意力模型直观理解 如果一个非常长的句子，需要把整个句子记住再进行翻译，效果不是很好。 人类去翻译一段话，是一句一句翻译的，也就是一段话，从前往后根据上下文进行翻译的。 注意力模型也是类似的概念。再进行翻译的时候，输入翻译到该词的附近的词的特征作为输入。 注意力模型我来总结一下我的理解： encoder其实是针对输入的句子训练一个RNN模型，这个RNN模型一般是双向是LSTM之类的，那么每一个词，每一步都是有a的。 训练一个另外的注意力模型，这个模型也是一个RNN。 这个RNN的输入有，S和C。 S最开始为0向量。 C为所有输入的词的a * 对这个词的注意力权重 注意力权重的训练方法也是一个小型的神经网络 最后是一个softmax 输入是改注意力模型该步的S和这个词的a 语音辩识 注意力模型 CTC损失函数 触发字检测]]></content>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法之贝叶斯分类初探]]></title>
    <url>%2F2020%2F08%2F16%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B9%8B%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%88%9D%E6%8E%A2%2F</url>
    <content type="text"><![CDATA[贝叶斯分类理论贝叶斯公式如下所言： 所求为后验概率，是通过样本无法数出来的。 公式推导基于样本独立，变为都是可以通过样本数出来的数据。 这样就可以根据现有的样本，样本所属的分类，就可以推测出新来的一个样本，所属的哪个分类的概率最大了。 例子1：数据源：123456789postingList = [[ &apos;my&apos;, &apos;dog&apos;, &apos;has&apos;, &apos;flea&apos;, &apos;problems&apos;, &apos;help&apos;, &apos;please&apos; ], [&apos;maybe&apos;, &apos;not&apos;, &apos;take&apos;, &apos;him&apos;, &apos;to&apos;, &apos;dog&apos;, &apos;park&apos;, &apos;stupid&apos;], [&apos;my&apos;, &apos;dalmation&apos;, &apos;is&apos;, &apos;so&apos;, &apos;cute&apos;, &apos;I&apos;, &apos;love&apos;, &apos;him&apos;], [&apos;stop&apos;, &apos;posting&apos;, &apos;stupid&apos;, &apos;worthless&apos;, &apos;garbage&apos;], [ &apos;mr&apos;, &apos;licks&apos;, &apos;ate&apos;, &apos;my&apos;, &apos;steak&apos;, &apos;how&apos;, &apos;to&apos;, &apos;stop&apos;, &apos;him&apos; ], [&apos;quit&apos;, &apos;buying&apos;, &apos;worthless&apos;, &apos;dog&apos;, &apos;food&apos;, &apos;stupid&apos;]] classVec = [0, 1, 0, 1, 0, 1] # 1 is abusive, 0 not 如下： 目标：1[&apos;love&apos;, &apos;my&apos;, &apos;dalmation&apos;] one-hot向量之后： 贝叶斯计算]]></content>
  </entry>
  <entry>
    <title><![CDATA[决策树]]></title>
    <url>%2F2020%2F08%2F16%2F%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[决策树ID3、C4.5、CART的区别ID3 ID3是分类树 ID3使用信息增益选择分裂的属性 ID3不剪枝 ID3处理连续型随机变量是构造区间，数据变成离散化的方式。 C4.5 C4.5是分类树 C4.5使用信息增益率作为分裂的标准 C4.5剪枝 C4.5处理连续型随机变量是构造区间，数据变成离散化的方式。 CART CART可以是分类树，也可以是回归树。 CART分类使用GINI值作为分裂的标准。回归采用最小方差作为标准。 CART是一颗二叉树。 CART剪枝。]]></content>
      <categories>
        <category>机器学习</category>
        <category>决策树</category>
      </categories>
      <tags>
        <tag>决策树</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[吴恩达deep-learning序列模型-循环序列模型]]></title>
    <url>%2F2020%2F08%2F16%2F%E5%90%B4%E6%81%A9%E8%BE%BEdeep-learning%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B-%E5%BE%AA%E7%8E%AF%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[循环序列模型为什么选择序列模型数学符号 循环神经网络模型如果采用传统的神经网络模型。 输入和输出在不同的样本中，可能不同 他的结果，不能共享 比方在一篇文章中，识别出来“Harry”是一个人名，他并不能共享到其他的样本中，标准另一个样本中那个的“Harry”也是一个人名。 那么循环神经网络是怎么做的呢？第一个样本输入神经网络，他会预测出，这是否是一个实体的结果。 循环神经网络做的是，当他读到第二个词的时候，他不仅仅是利用第二个词去预测结果。 他还会输入来自第一个词（时间步1）的信息。 具体来来讲，时间步1的激活值，会传递到时间步2。 吴恩达的循环神经网络的表示方法： 通常论文的循环神经网络的表示方法： 循环神经网络中的参数表示方法 通过时间的反向传播损失函数交叉熵 不同类型的循环神经网络many to many输入和输出相同命名实体识别 上面介绍的RNN为many to many 类型，输入和输出的数量的相同的。但是还有其他的类型。 输入和输出不同机器翻译 many to one情感分析，输入一篇文本，但是输出是，这片文章是消极还是积极。 one to one one to manymusic generation 第一个层输出的结果也会喂给下一层，作为输入。 语言模型和序列生成 对新序列采样基于词汇如果模型已经训练好了，那么能够从模型中得到什么呢？ 一种非正式的方法就是进行一次新序列采样。 如下图，已经得到一个基于语料的RNN模型： 我们的目的是生成一个新的单词序列，也就是一句话。 基于字符“Cat”词表会是[C, a, t] 优点不必担心未出现的字符 缺点 序列会非常长 句子之间的关系捕捉不是很好 计算起来成本比较高 趋势是，大多数的情况都是使用基于词汇的，但是特殊情况下会使用基于字符的。 基于字符会用在处理大量的未知词汇的应用。或者专有词汇的应用。 带有神经网络的梯度消失梯度消失例子： The cat , which already ate …… , was full. The cats , which already ate …… , were full. 上面的两个例子，cat和 was，cats和were 之间有长期的依赖关系。但是使用RNN很难建立这种长时间的联系。 因为训练很深的神经网络，会出现梯度消失的问题。 如果这是 一个非常深的神经网络，这个网络前向传播，再反向传播，从后面的输出误差得到的梯度，是很难传到前面的，很难影响到前面的权重的计算。 也就是说，一个y^3,其实是受他附件的影响比较大，但是对比较远的地方，影响就比较小。 最根本的原因是，一个输出的误差很难通过梯度下降的方法传播到 距离 他非常远的地方，并对他产生影响。 RNN的一个缺点就是不擅长处理长期依赖问题。 梯度爆炸在反向传播的时候，不仅仅会出现梯度指数型消失的问题，还会出现梯度指数型爆炸的问题。 梯度爆炸会使得参数变得非常大，以至于神经网络参数崩溃。 梯度修剪设置一个阈值，如果梯度大于阈值，缩放梯度向量，保证不会特别大。 GRU单元门控循环单元 由于细胞单元C的存在，使得GRU能够缓解反向传播过程中的梯度消失现象。 长短期记忆（LSTM） 双向RNN双向RNN不仅可以获取之前的信息，还可以获取未来的信息。 但是双向RNN的缺点在于，需要一个完整的序列。 对于语音识别，需要等到这句话说完，才可以进行BRNN。 但是对于自然语言处理，文本是非常适合用BRNN的。 BRNN 深层循环神经网络]]></content>
  </entry>
  <entry>
    <title><![CDATA[吴恩达deep-learning序列模型-自然语言处理与词嵌入]]></title>
    <url>%2F2020%2F08%2F16%2F%E5%90%B4%E6%81%A9%E8%BE%BEdeep-learning%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B8%8E%E8%AF%8D%E5%B5%8C%E5%85%A5%2F</url>
    <content type="text"><![CDATA[自然语言处理与词嵌入词汇表征one-hot word-embedding 可视化算法：t-SNE 使用词嵌入词嵌入的特性嵌入矩阵 学习词嵌入 Word2vecskip-gram比方说，先选定一个词，orange。在orange的上下文为10的窗口内，随机选定一个词进行预测，假设这个词为“juice”。那么正确的答案就应该是juice。然后预测，在orange出现的概率下，除了orange之外的词出现的概率，并把这些结果输入到softmax函数，最后算这个softmax的损失函数。通过计算损失函数的SGD进行反向传播，进而影响softmax的参数，进而得到词向量。 计算慢，主要是softmax的分母要计算出所有的和选定的词之外的概率。 二分类可以缓解这个问题。 负采样 负采样的方法 GloVe词向量 先找出一个上下文词C，和一个目标词T。 遍历训练集，得到T在上下文C的周围出现的次数。 情绪分类情绪分类的一个很大的难点在于你没有非常多的标注样本。 使用词嵌入可以使用比较小的样本集也可以达到比较好的效果。 使用文本词向量平均值 缺点对于“Completely lacking in good taste, good service, and good ambience” 这种负面情绪的文本，但是包含很多正面词汇的情况，处理不佳。 使用RNN构建一个情绪分类器many to one 词嵌入除偏 做法是把词分成像“grandmother”和“grandfather”这样的对性别没有偏见的词象限。 以及“babysister”和“doctor”这样的性别产生偏见的词象限。 由之前“he”、”she”、“male”、”female”等求差再求平均求出的性别的向量。 对于像“grandmother”和“grandfather”这样的词不用处理。 而对于“babysister”和”doctor”这样的会产生偏见的词，像性别这个纬度上投影，SVD，这样会缩减性别对他们的影响。]]></content>
  </entry>
  <entry>
    <title><![CDATA[TF-IDF、PageRank、TextRank]]></title>
    <url>%2F2020%2F08%2F16%2FTF-IDF%E5%92%8CTextRank%2F</url>
    <content type="text"><![CDATA[TF-IDF、PageRank、TextRankTF-IDF什么是TF-IDF 字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。 词频 (term frequency, TF) 指的是某一个给定的词语在该文件中出现的次数。 逆向文件频率 (inverse document frequency, IDF) TF-IDF PageRank算法来源 算法原理 算法实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980# -*-coding:utf-8-*-"""@Time : 2018/7/25 9:23@Author : Mark@File : page_rank.py"""# -*- coding: utf-8 -*-from pygraph.classes.digraph import digraphclass PRIterator: __doc__ = '''计算一张图中的PR值''' def __init__(self, dg): self.damping_factor = 0.85 # 阻尼系数,即α self.max_iterations = 100 # 最大迭代次数 self.min_delta = 0.00001 # 确定迭代是否结束的参数,即ϵ self.graph = dg def page_rank(self): # 先将图中没有出链的节点改为对所有节点都有出链 for node in self.graph.nodes(): if len(self.graph.neighbors(node)) == 0: for node2 in self.graph.nodes(): digraph.add_edge(self.graph, (node, node2)) nodes = self.graph.nodes() graph_size = len(nodes) if graph_size == 0: return &#123;&#125; page_rank = dict.fromkeys(nodes, 1.0 / graph_size) # 给每个节点赋予初始的PR值 damping_value = ( 1.0 - self.damping_factor) / graph_size # 公式中的(1−α)/N部分 flag = False for i in range(self.max_iterations): change = 0 for node in nodes: rank = 0 for incident_page in self.graph.incidents(node): # 遍历所有“入射”的页面 rank += self.damping_factor * ( page_rank[incident_page] / len( self.graph.neighbors(incident_page))) rank += damping_value change += abs(page_rank[node] - rank) # 绝对值 page_rank[node] = rank print("This is NO.%s iteration" % (i + 1)) print(page_rank) if change &lt; self.min_delta: flag = True break if flag: print("finished in %s iterations!" % node) else: print("finished out of 100 iterations!") return page_rankif __name__ == '__main__': dg = digraph() dg.add_nodes(["A", "B", "C", "D", "E"]) dg.add_edge(("A", "B")) dg.add_edge(("A", "C")) dg.add_edge(("A", "D")) dg.add_edge(("B", "D")) dg.add_edge(("C", "E")) dg.add_edge(("D", "E")) dg.add_edge(("B", "E")) dg.add_edge(("E", "A")) pr = PRIterator(dg) page_ranks = pr.page_rank() print("The final page rank is\n", page_ranks) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354This is NO.1 iteration&#123;'A': 0.2, 'C': 0.08666666666666667, 'B': 0.08666666666666667, 'E': 0.31050000000000005, 'D': 0.1235&#125;This is NO.2 iteration&#123;'A': 0.29392500000000005, 'C': 0.11327875000000001, 'B': 0.11327875000000001, 'E': 0.27940540625, 'D': 0.16142221875&#125;This is NO.3 iteration&#123;'A': 0.26749459531249997, 'C': 0.10579013533854167, 'B': 0.10579013533854167, 'E': 0.3020913084941407, 'D': 0.15075094285742188&#125;This is NO.4 iteration&#123;'A': 0.2867776122200196, 'C': 0.1112536567956722, 'B': 0.1112536567956722, 'E': 0.2999867138432907, 'D': 0.1585364609338329&#125;This is NO.5 iteration&#123;'A': 0.2849887067667971, 'C': 0.11074680025059253, 'B': 0.11074680025059253, 'E': 0.30595816211326343, 'D': 0.15781419035709435&#125;This is NO.6 iteration&#123;'A': 0.29006443779627394, 'C': 0.11218492404227762, 'B': 0.11218492404227762, 'E': 0.3071778399574342, 'D': 0.1598635167602456&#125;This is NO.7 iteration&#123;'A': 0.2911011639638191, 'C': 0.11247866312308208, 'B': 0.11247866312308208, 'E': 0.3092942847281384, 'D': 0.16028209495039195&#125;This is NO.8 iteration&#123;'A': 0.2929001420189177, 'C': 0.11298837357202668, 'B': 0.11298837357202668, 'E': 0.31029995701216717, 'D': 0.161008432340138&#125;This is NO.9 iteration&#123;'A': 0.29375496346034213, 'C': 0.11323057298043027, 'B': 0.11323057298043027, 'E': 0.31122614803916593, 'D': 0.16135356649711313&#125;This is NO.10 iteration&#123;'A': 0.29454222583329104, 'C': 0.11345363065276579, 'B': 0.11345363065276579, 'E': 0.3118039106048226, 'D': 0.16167142368019125&#125;This is NO.11 iteration&#123;'A': 0.2950333240140992, 'C': 0.1135927751373281, 'B': 0.1135927751373281, 'E': 0.3122514984282559, 'D': 0.16186970457069255&#125;This is NO.12 iteration&#123;'A': 0.29541377366401755, 'C': 0.11370056920480498, 'B': 0.11370056920480498, 'E': 0.312557474621215, 'D': 0.1620233111168471&#125;This is NO.13 iteration&#123;'A': 0.2956738534280328, 'C': 0.11377425847127598, 'B': 0.11377425847127598, 'E': 0.3127819940001969, 'D': 0.16212831832156827&#125;This is NO.14 iteration&#123;'A': 0.29586469490016737, 'C': 0.1138283302217141, 'B': 0.1138283302217141, 'E': 0.3129401916060185, 'D': 0.16220537056594256&#125;This is NO.15 iteration&#123;'A': 0.29599916286511574, 'C': 0.11386642947844947, 'B': 0.11386642947844947, 'E': 0.31305426256607427, 'D': 0.16225966200679048&#125;This is NO.16 iteration&#123;'A': 0.29609612318116313, 'C': 0.11389390156799623, 'B': 0.11389390156799623, 'E': 0.3131354372049671, 'D': 0.16229880973439462&#125;This is NO.17 iteration&#123;'A': 0.2961651216242221, 'C': 0.11391345112686294, 'B': 0.11391345112686294, 'E': 0.3131936384609857, 'D': 0.16232666785577968&#125;This is NO.18 iteration&#123;'A': 0.29621459269183786, 'C': 0.11392746792935407, 'B': 0.11392746792935407, 'E': 0.3132351892873392, 'D': 0.16234664179932953&#125;This is NO.19 iteration&#123;'A': 0.2962499108942383, 'C': 0.11393747475336752, 'B': 0.11393747475336752, 'E': 0.31326492583997373, 'D': 0.1623609015235487&#125;This is NO.20 iteration&#123;'A': 0.2962751869639777, 'C': 0.11394463630646035, 'B': 0.11394463630646035, 'E': 0.3132861775857534, 'D': 0.162371106736706&#125;This is NO.21 iteration&#123;'A': 0.2962932509478904, 'C': 0.11394975443523561, 'B': 0.11394975443523561, 'E': 0.31330137763112553, 'D': 0.16237840007021073&#125;This is NO.22 iteration&#123;'A': 0.29630617098645673, 'C': 0.11395341511282941, 'B': 0.11395341511282941, 'E': 0.31331224432853666, 'D': 0.1623836165357819&#125;This is NO.23 iteration&#123;'A': 0.2963154076792562, 'C': 0.11395603217578926, 'B': 0.11395603217578926, 'E': 0.31332001507954593, 'D': 0.16238734585049966&#125;This is NO.24 iteration&#123;'A': 0.29632201281761406, 'C': 0.11395790363165731, 'B': 0.11395790363165731, 'E': 0.3133255711032878, 'D': 0.16239001267511166&#125;This is NO.25 iteration&#123;'A': 0.29632673543779464, 'C': 0.11395924170737515, 'B': 0.11395924170737515, 'E': 0.31332954395074825, 'D': 0.1623919194330096&#125;This is NO.26 iteration&#123;'A': 0.29633011235813606, 'C': 0.11396019850147188, 'B': 0.11396019850147188, 'E': 0.31333238460743484, 'D': 0.16239328286459742&#125;finished in D iterations!('The final page rank is\n', &#123;'A': 0.29633011235813606, 'C': 0.11396019850147188, 'B': 0.11396019850147188, 'E': 0.31333238460743484, 'D': 0.16239328286459742&#125;) TextRank原理进行关键词提取时，TextRank算法思想和PageRank算法类似，不同的是，TextRank中时以词为节点，以共现关系建立起节点之间的链接。 分词，并去除停用词等噪音 对于每一个词，前后5个词，共10个作为和它link的词。但是比方说，“程序员”在文章中出现了两次，那就是和程序员这个词link的此为这10个词的并集 算法就类似PageRank，进行迭代，直至收敛 算法实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105# -*-coding:utf-8-*-"""@Time : 2018/7/25 12:03@Author : Mark@File : text_rank.py"""import sysfrom pygraph.classes.digraph import digraphreload(sys)sys.setdefaultencoding('utf-8')class PRIterator: __doc__ = '''计算一张图中的PR值''' def __init__(self, dg): self.damping_factor = 0.85 # 阻尼系数,即α self.max_iterations = 100 # 最大迭代次数 self.min_delta = 0.00001 # 确定迭代是否结束的参数,即ϵ self.graph = dg def page_rank(self): # 先将图中没有出链的节点改为对所有节点都有出链 for node in self.graph.nodes(): if len(self.graph.neighbors(node)) == 0: for node2 in self.graph.nodes(): digraph.add_edge(self.graph, (node, node2)) nodes = self.graph.nodes() graph_size = len(nodes) if graph_size == 0: return &#123;&#125; page_rank = dict.fromkeys(nodes, 1.0 / graph_size) # 给每个节点赋予初始的PR值 damping_value = ( 1.0 - self.damping_factor) / graph_size # 公式中的(1−α)/N部分 flag = False for i in range(self.max_iterations): change = 0 for node in nodes: rank = 0 for incident_page in self.graph.incidents(node): # 遍历所有“入射”的页面 rank += self.damping_factor * ( page_rank[incident_page] / len( self.graph.neighbors(incident_page))) rank += damping_value change += abs(page_rank[node] - rank) # 绝对值 page_rank[node] = rank print("This is NO.%s iteration" % (i + 1)) if change &lt; self.min_delta: flag = True break if flag: print("finished in %s iterations!" % node) else: print("finished out of 100 iterations!") return page_rankif __name__ == '__main__': dg = digraph() words_dict = &#123; u"开发": [u"专业", u"程序员", u"维护", u"英文", u"程序", u"人员"], u"软件": [u"程序员", u"分为", u"界限", u"高级", u"中国", u"特别", u"人员"], u"程序员": [u"开发", u"软件", u"分析员", u"维护", u"系统", u"项目", u"经理", u"分为", u"英文", u"程序", u"专业", u"设计", u"高级", u"人员", u"中国"], u"分析员": [u"程序员", u"系统", u"项目", u"经理", u"高级"], u"维护": [u"专业", u"开发", u"程序员", u"分为", u"英文", u"程序", u"人员"], u"系统": [u"程序员", u"分析员", u"项目", u"经理", u"分为", u"高级"], u"项目": [u"程序员", u"分析员", u"系统", u"经理", u"高级"], u"经理": [u"程序员", u"分析员", u"系统", u"项目"], u"分为": [u"专业", u"软件", u"设计", u"程序员", u"维护", u"系统", u"高级", u"程序", u"中国", u"特别", u"人员"], u"英文": [u"专业", u"开发", u"程序员", u"维护", u"程序"], u"程序": [u"专业", u"开发", u"设计", u"程序员", u"编码", u"维护", u"界限", u"分为", u"英文", u"特别", u"人员"], u"特别": [u"软件", u"编码", u"分为", u"界限", u"程序", u"中国", u"人员"], u"专业": [u"开发", u"程序员", u"维护", u"分为", u"英文", u"程序", u"人员"], u"设计": [u"程序员", u"编码", u"分为", u"程序", u"人员"], u"编码": [u"设计", u"界限", u"程序", u"中国", u"特别", u"人员"], u"界限": [u"软件", u"编码", u"程序", u"中国", u"特别", u"人员"], u"高级": [u"程序员", u"软件", u"分析员", u"系统", u"项目", u"分为", u"人员"], u"中国": [u"程序员", u"软件", u"编码", u"分为", u"界限", u"特别", u"人员"], u"人员": [u"开发", u"程序员", u"软件", u"维护", u"分为", u"程序", u"特别", u"专业", u"设计", u"编码", u"界限", u"高级", u"中国"] &#125; words_key_list = list() for k, v in words_dict.iteritems(): words_key_list.append(k) dg.add_nodes(words_key_list) for k, v in words_dict.iteritems(): for i in v: dg.add_edge((k, i)) pr = PRIterator(dg) page_ranks = pr.page_rank() sort_text_rank = sorted(page_ranks.items(), key=lambda d: d[1], reverse=True) for k, v in sort_text_rank: print str(k) + u":" + str(v) print sort_text_rank 1234567891011121314151617181920212223242526272829303132333435363738394041This is NO.1 iterationThis is NO.2 iterationThis is NO.3 iterationThis is NO.4 iterationThis is NO.5 iterationThis is NO.6 iterationThis is NO.7 iterationThis is NO.8 iterationThis is NO.9 iterationThis is NO.10 iterationThis is NO.11 iterationThis is NO.12 iterationThis is NO.13 iterationThis is NO.14 iterationThis is NO.15 iterationThis is NO.16 iterationThis is NO.17 iterationThis is NO.18 iterationThis is NO.19 iterationThis is NO.20 iterationThis is NO.21 iterationfinished in 编码 iterations!程序员:0.101580333895人员:0.0859679826211分为:0.0740249633386程序:0.0740144395754高级:0.0514255757372软件:0.0493472878223中国:0.0492889214488特别:0.049256278494专业:0.0491852387529维护:0.0491848961483系统:0.0466872602095编码:0.0436170552749界限:0.0433726145516开发:0.043302777825项目:0.0406679129266分析员:0.040667762853英文:0.037449970767设计:0.0368904751286经理:0.034092209713 引用声明TF-IDF原理及使用 PageRank算法–从原理到实现 基于TextRank的关键词提取算法]]></content>
  </entry>
  <entry>
    <title><![CDATA[深度学习中的优化器进化推导]]></title>
    <url>%2F2020%2F08%2F16%2F%E4%BC%98%E5%8C%96%E5%99%A8%E6%8E%A8%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[深度学习中的优化器进化推导指数加权平均说到优化器，需要先了解一下指数加权平均的基础。 什么是指数加权平均指数加权平均(exponentially weighted averges)，也叫指数加权移动平均，是一种常用的序列数据处理方式。 它的计算公式如下： 其中： θ_t：为第 t 天的实际观察值， V_t: 是要代替 θ_t 的估计值，也就是第 t 天的指数加权平均值， β： 为 V_{t-1} 的权重，是可调节的超参。( 0 &lt; β &lt; 1 ) 例如： 我们有这样一组气温数据，图中横轴为一年中的第几天，纵轴为气温： 直接看上面的数据图会发现噪音很多， 这时，我们可以用 指数加权平均 来提取这组数据的趋势， 按照前面的公式计算： 这里先设置 β = 0.9，首先初始化 V_0 ＝ 0，然后计算出每个 V_t： 将计算后得到的 V_t 表示出来，就得到红色线的数值： 可以看出，红色的数据比蓝色的原数据更加平滑，少了很多噪音，并且刻画了原数据的趋势。 指数加权平均，作为原数据的估计值，不仅可以 1. 抚平短期波动，起到了平滑的作用，2. 还能够将长线趋势或周期趋势显现出来。 所以应用比较广泛，在处理统计数据时，在股价等时间序列数据中，CTR 预估中，美团外卖的收入监控报警系统中的 hot-winter 异常点平滑，深度学习的优化算法中都有应用。 为什么在优化算法中使用指数加权平均上面提到了一些 指数加权平均 的应用，这里我们着重看一下在优化算法中的作用。 以 Momentum 梯度下降法为例， Momentum 梯度下降法，就是计算了梯度的指数加权平均数，并以此来更新权重，它的运行速度几乎总是快于标准的梯度下降算法。 这是为什么呢？ 让我们来看一下这个图， 例如这就是我们要优化的成本函数的形状，图中红点就代表我们要达到的最小值的位置， 假设我们从左下角这里出发开始用梯度下降法，那么蓝色曲线就是一步一步迭代，一步一步向最小值靠近的轨迹。 可以看出这种上下波动，减慢了梯度下降法的速度，而且无法使用更大的学习率，因为如果用较大的学习率，可能会偏离函数的范围。 如果有一种方法，可以使得在纵轴上，学习得慢一点，减少这些摆动，但是在横轴上，学习得快一些，快速地从左向右移移向红点最小值，那么训练的速度就可以加快很多。 这个方法就是动量 Momentum 梯度下降法，它在每次计算梯度的迭代中，对 dw 和 db 使用了指数加权平均法的思想 ： 这样我们就可以得到如图红色线的轨迹： 可以看到： 纵轴方向，平均过程中正负摆动相互抵消，平均值接近于零，摆动变小，学习放慢。 横轴方向，因为所有的微分都指向横轴方向，因此平均值仍然较大，向最小值运动更快了。 在抵达最小值的路上减少了摆动，加快了训练速度。 β 如何选择根据前面的计算式子： 将 V_{100} 展开得到： 这里可以看出，V_t 是对每天温度的加权平均，之所以称之为指数加权，是因为加权系数是随着时间以指数形式递减的，时间越靠近，权重越大，越靠前，权重越小。 再来看下面三种情况： 当 β = 0.9 时，指数加权平均最后的结果如图红色线所示，代表的是最近 10 天的平均温度值； 当 β = 0.98 时，指结果如图绿色线所示，代表的是最近 50 天的平均温度值； 当 β = 0.5 时，结果如下图黄色线所示，代表的是最近 2 天的平均温度值； β 越小，噪音越多，虽然能够很快的适应温度的变化，但是更容易出现奇异值。 β 越大，得到的曲线越平坦，因为多平均了几天的温度，这个曲线的波动更小。 但有个缺点是，因为只有 0.02 的权重给了当天的值，而之前的数值权重占了 0.98 ， 曲线进一步右移，在温度变化时就会适应地更缓慢一些，会出现一定延迟。 通过上面的内容可知，β 也是一个很重要的超参数，不同的值有不同的效果，需要调节来达到最佳效果，一般 0.9 的效果就很好。 优化器几种优化方法之间的关系 梯度下降 基本的梯度下降法每次使用所有训练样本的平均损失来更新参数； 因此，经典的梯度下降在每次对模型参数进行更新时，需要遍历所有数据； 当训练样本的数量很大时，这需要消耗相当大的计算资源，在实际应用中基本不可行。 随机梯度下降(SGD)随机梯度下降（SGD）每次使用单个样本的损失来近似平均损失 小批量随机梯度下降（BSGD） 为了降低随机梯度的方差，使模型迭代更加稳定，实践中会使用一批随机数据的损失来近似平均损失。 使用批训练的另一个主要目的，是为了利用高度优化的矩阵运算以及并行计算框架。 随机梯度下降存在的问题 随机梯度下降（SGD）放弃了梯度的准确性，仅采用一部分样本来估计当前的梯度；因此 SGD 对梯度的估计常常出现偏差，造成目标函数收敛不稳定，甚至不收敛的情况。 无论是经典的梯度下降还是随机梯度下降，都可能陷入局部极值点；除此之外，SGD 还可能遇到“峡谷”和“鞍点”两种情况 峡谷类似一个带有坡度的狭长小道，左右两侧是“峭壁”；在峡谷中，准确的梯度方向应该沿着坡的方向向下，但粗糙的梯度估计使其稍有偏离就撞向两侧的峭壁，然后在两个峭壁间来回震荡。 鞍点的形状类似一个马鞍，一个方向两头翘，一个方向两头垂，而中间区域近似平地；一旦优化的过程中不慎落入鞍点，优化很可能就会停滞下来。 指数加权平均例子 公式 Momentum在当前的梯度上做文章，对当钱的梯度做指数加权，但是学习率不变。 公式推导 图像示意 AdaGrad在学习率上做文章 优点当梯度比较大的时候，G也就比较大，但是学习率会降低，也就是在陡峭的地方学习的会慢一些。 但是当梯度比较小的时候，G比较小，学习率比较大，也就是在平缓的比方会下降的快一些，不会停滞不前。 缺点由于是累积，导致G最后会累积的很大，导致到最后学习率会趋近于0，即使当钱是有梯度的，也无法继续梯度下降了。 RMSprop还是在学习率上做文章，但是是对学习率做了指数加权。 Adam]]></content>
  </entry>
  <entry>
    <title><![CDATA[Two Sum]]></title>
    <url>%2F2020%2F08%2F16%2FTwo%20Sum%2F</url>
    <content type="text"><![CDATA[Two Sum题目 给定一个整数数组和一个目标值，找出数组中和为目标值的两个数。 你可以假设每个输入只对应一种答案，且同样的元素不能被重复利用。 示例: 给定 nums = [2, 7, 11, 15], target = 9 因为 nums[0] + nums[1] = 2 + 7 = 9所以返回 [0, 1] 解法123456789101112131415from typing import Listclass Solution(object): def twoSum(self, nums: List[int], target: int) -&gt; List[int]: record = dict() for i in range(len(nums)): n = target - nums[i] if n in record.keys(): return[record[n], i] record[nums[i]] = inums = [2, 7, 11, 15]target = 9print(Solution().twoSum(nums, target)) 解析这道题的思路是用空间换时间。 第一轮对nums的循环是不可避免的。 这个算法高效的原因在于对第一轮对每一次循环的结果，进行了保存，并没有浪费掉消耗的时间。 假设我们有一个record dict , key为循环的该值，value为该值的索引。 每一次的循环，我们先用target - 当前值，去record里找有没有key，找到就返回。 模拟一下： 第一轮：record = {}, i=0, n = 7, 在record中不存在，则record[2]= 0 第二轮：record = {2:0}, i=1, n= 2, 在record中存在，则 return [record[2], 2] = [0, 1] 图解如下]]></content>
  </entry>
  <entry>
    <title><![CDATA[Word2Vec]]></title>
    <url>%2F2020%2F08%2F16%2Fword2vec%2F</url>
    <content type="text"><![CDATA[Word2Vec神经网络结构Word2Vec的架构是一个三层的神经网络，如下图： 假如共有10000个词汇。 第一层：输入一个词，维度为：1*10000. 第二层：10000*300权重矩阵。(没有激活函数) 第三层：300*10000。(对10000个可能做softmax) 共参数：10000*300 + 300*10000 = 600万的参数。 词向量词向量何来？ 第二层： 对于任何一个onehot单词，乘以第二层的矩阵，其实是得到的第二层10000行中的一行，也就是其实第二层的权重矩阵中的每一行 ，对应的是一个单词的1*300维度词向量。 这也是为什么这个矩阵称为映射矩阵的原因，其实10000行其实是对高维的onehot做了一层映射，那么到第三层的预测的时候，其实是拿该词的1*300的词向量作为输入去进行相乘训练的，于是其实训练的其实就该词向量本身，这也是该矩阵能成为词向量的原因。 图示如下： 直觉上的理解： 如果两个不同的单词有着非常相似的“上下文”（也就是窗口单词很相似，比如“Kitty climbed the tree”和“Cat climbed the tree”），也就是输出很相似，那么通过我们的模型训练，这两个单词的嵌入向量将非常相似。 训练中的Trick词对类似”New York”专有名词，其实是可以作为一个词进行训练的。 高频词抽样高频词抽样原因 对于”the”这种词，并不会给”the”, “fox” 增加更多的语义信息。 由于”the”这种词在文中出现的频次非常多，包涵”the”的样本数量，远远大于我们需要学习”the”这个词需要的训练样本。 高频词抽样的结果 删除”the”后，窗口内再也不会出现”the”，减少了训练的计算资源。 删除”the”后，”the”作为input word的训练样本至少减少10个。 抽样率$Z(w_i)$ : 该词在语聊中出现的频次。 $P(w_i)$ ：代表保留某个单词的概率。$$P(w_i) = (\sqrt{\frac{Z(w_i)}{0.001}} + 1) \times \frac{0.001}{Z(w_i)}$$ $Z(w_i) &lt; = 0.0026$ 时，$P(w_i)$ = 1。 $Z(w_i) = 0.00746$ 时，$P(w_i)$ = 0.5。 $Z(w_i) = 1.0$ 时，$P(w_i)$ = 0.033。 负采样负采样每次训练，只更新除了”positive”之外，只更新”negative”最多20个词的权重。 对于小规模数据集，选择5-20个negative words会比较好，对于大规模数据集可以仅选择2-5个negative words。 这样权重就编程了300*21就好训练多了。 那么negative中的20个是怎么获得的呢？ 负采样频率单词在文本中出现的频次：$p(w_i)$ 该负样本被选中的概率为:$$P(w_i) = \frac{f(w_i)^{\frac{3}{4}}}{\sum_{j=0}^{n}(f(w_i)^{\frac{3}{4}})}$$ 工程实现 已经计算出每个单词的$p(w_i)$,乘以1亿，得出每个单词在1亿的数组中出现的次数。由此得到一个1亿的数组，包含了所有的单词(每个单词N个)。概率越大的词，在1亿数组中出现的次数越多，概率越小的词，在1亿数组中出现的次数越小。 在0-1亿中，生成一个随机数，做为1亿的数组的索引，来选择单词。也就是负样本的词。一个单词的负采样概率越大，在表中出现的次数就越多，被选中的概率就越大。]]></content>
  </entry>
  <entry>
    <title><![CDATA[奇异值分解]]></title>
    <url>%2F2020%2F08%2F16%2FSGD%E6%8E%A8%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[奇异值分解奇异值分解是一个有着很明显的物理意义的一种方法，它可以将一个比较复杂的矩阵用更小更简单的几个子矩阵的相乘来表示，这些小矩阵描述的是矩阵的重要的特性。 就像是描述一个人一样，给别人描述说这个人长得浓眉大眼，方脸，络腮胡，而且带个黑框的眼镜，这样寥寥的几个特征，就让别人脑海里面就有一个较为清楚的认识。 让机器学会抽取重要的特征。在机器学习领域，有相当多的应用与奇异值都可以扯上关系，比如做feature reduction的PCA，做数据压缩（以图像压缩为代表）的算法，还有做搜索引擎语义层次检索的LSI（Latent Semantic Indexing）。 奇异值分解详解PCA的实现一般有两种，一种是用特征值分解去实现的，一种是用奇异值分解去实现的。 特征值分解 特征值分解可以得到特征值与特征向量。 特征值表示的是这个特征到底有多重要。 而特征向量表示这个特征是什么。 可以将每一个特征向量理解为一个线性的子空间，我们可以利用这些线性的子空间干很多的事情。不过，特征值分解也有很多的局限，比如说变换的矩阵必须是方阵。 奇异值分解特征值分解是一个提取矩阵特征很不错的方法，但是它只是对方阵而言的。 奇异值分解是一个能适用于任意的矩阵的一种分解的方法： 假设A是一个N * M的矩阵，那么得到的U是一个N * N的方阵（里面的向量是正交的，U里面的向量称为左奇异向量），Σ是一个N * M的矩阵（除了对角线的元素都是0，对角线上的元素称为奇异值），V’(V的转置)是一个M* M的矩阵，里面的向量也是正交的，V里面的向量称为右奇异向量）]]></content>
  </entry>
  <entry>
    <title><![CDATA[Simhash]]></title>
    <url>%2F2020%2F08%2F16%2FSimhash%2F</url>
    <content type="text"><![CDATA[SimhashSimhash算法是为了解决文本相似性的。 Simhash流程实现 1、分词，把需要判断文本分词形成这个文章的特征单词。最后形成去掉噪音词的单词序列并为每个词加上权重，我们假设权重分为5个级别（1~5）。比如：“ 美国“51区”雇员称内部有9架飞碟，曾看见灰色外星人 ” ==&gt; 分词后为 “ 美国（4） 51区（5） 雇员（3） 称（1） 内部（2） 有（1） 9架（3） 飞碟（5） 曾（1） 看见（3） 灰色（4） 外星人（5）”，括号里是代表单词在整个句子里重要程度，数字越大越重要。 2、hash，通过hash算法把每个词变成hash值，比如“美国”通过hash算法计算为 100101,“51区”通过hash算法计算为 101011。这样我们的字符串就变成了一串串数字。 3、加权，通过 2步骤的hash生成结果，需要按照单词的权重形成加权数字串，比如“美国”的hash值为“100101”，通过加权计算为“4 -4 -4 4 -4 4”；“51区”的hash值为“101011”，通过加权计算为 “ 5 -5 5 -5 5 5”。 4、合并，把上面各个单词算出来的序列值累加，变成只有一个序列串。比如 “美国”的 “4 -4 -4 4 -4 4”，“51区”的 “ 5 -5 5 -5 5 5”， 把每一位进行累加， “4+5 -4+-5 -4+5 4+-5 -4+5 4+5” ==》 “9 -9 1 -1 1 9”。这里作为示例只算了两个单词的，真实计算需要把所有单词的序列串累加。 5、降维，把4步算出来的 “9 -9 1 -1 1 9” 变成 0 1 串，形成我们最终的simhash签名。 如果每一位大于0 记为 1，小于0 记为 0。最后算出结果为：“1 0 1 0 1 1”。 Simhash距离计算比较两个simhash的01有多少个不同。 通过海明距离（Hamming distance）就可以计算出两个simhash到底相似不相似。举例如下： 10101 和 00110 从第一位开始依次有第一位、第四、第五位不同，则海明距离为3。 Simhash存储和索引因为Simhash计算的是海量的数据，如果每个文章都是按照0101的去存储和比较的话，就要存很多完整长度的行，然后新来一条，都要和已经存储下来的每一行，去做位运算或者乘法运算之类的。 但是Simhash采用了一种很巧妙的方式。借鉴了HashMap中，对key做hash，新来的key去hash的table中寻址的方法。 存储： 将一个64位的simhash签名拆分成4个16位的二进制码。（图上红色的16位） 分别拿着4个16位二进制码查找当前对应位置上是否有元素。（放大后的16位） 对应位置没有元素，直接追加到链表上；对应位置有则直接追加到链表尾端。（图上的 S1 — SN） 查找： 将需要比较的simhash签名拆分成4个16位的二进制码。 分别拿着4个16位二进制码每一个去查找simhash集合对应位置上是否有元素。 如果有元素，则把链表拿出来顺序查找比较，直到simhash小于一定大小的值，整个过程完成。 原理：借鉴hashmap算法找出可以hash的key值，因为我们使用的simhash是局部敏感哈希，这个算法的特点是只要相似的字符串只有个别的位数是有差别变化。那这样我们可以推断两个相似的文本，至少有16位的simhash是一样的。具体选择16位、8位、4位，大家根据自己的数据测试选择，虽然比较的位数越小越精准，但是空间会变大。分为4个16位段的存储空间是单独simhash存储空间的4倍。之前算出5000w数据是 382 Mb，扩大4倍1.5G左右，还可以接受。 假设我们库中有5000万(2^26)的文本， 每来一个新文本我们都需要做5000万次的汉明距离计算， 这个计算量还是比较大的，假设签名为64位。 考虑目标文档3位以内变化的签名有 C(64, 3) 4万多可能， 然后 4万 * 5000万的hash查询， 时间复杂度太大了 我们可以将库中已有的5000万文章的签名表切成4个表， 分别存储将64byte切开的ABCD四段中一段， 每进来一篇新文章，同样的分割为ABCD字段， 去与对应的表精确匹配(这个过程可以设置4个并发比较)， 取每个表join后剩下的文档ID， 大概每个表产出备选文档ID 有5000万/2^16 = 1000左右的量级，要找到5000万文档中和目标文档汉明距离小于3的文档集， 那么库中的文档至少ABCD有一段和目标文档完全匹配，这样我们最多会有 4 * 2 ^(26 -16) 篇备选文档， 大概4000多次汉明距离的计算就可以了。 simhash用于比较大的长文本, 可以取汉明距离为3， 对于短文本的效果比较差， 汉明距离可以取大一些进行相似过滤]]></content>
  </entry>
  <entry>
    <title><![CDATA[SVM推导]]></title>
    <url>%2F2020%2F08%2F16%2FSVM%E6%8E%A8%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[SVM推导定义SVM的定义是在特征空间上间隔最大的线性分类器。 核函数，使它成为实质上的非线性分类器。 SVM的学习策略就是间隔最大化，形式化为求解一个凸二次规划问题，也等价于正则化的合页损失函数是最小化问题。 线性可分支持向量机假设给定一个特征空间上的训练数据集：$$T = {(x_1, y_1),(x_2,y_2),…,(x_N,y_N)}$$其中，$x\epsilon R^n, y_i \epsilon {+1, -1}, i=1,2,3…,N$ ，假设数据集是线性可分的。 目标是在特征空间上，找到一个分隔超平面，能够将所有的点正确的分类。 假设超平面为：$$\omega \cdot x + b = 0$$ 几何间隔对于给定的训练数据集$T$中任意一个点$x_i$，到超平面$(w, b)$的几何间隔为： 几何距离 = 点到超平面的距离*$y_i$ $$\gamma_i= y_i(\frac{w \cdot x_i + b}{||w||})$$ 对于数据集$T$中所有的点中，到超平面的几何间隔的最小的值为：$$\gamma = \mathop{\min}{i=1,….,N} \gamma_i = \min{i=1,..N} y_i\frac{w \cdot x_i + b}{||w||}$$ 约束1如果该平面能够把所有的数据点正确划分，那么：$$w \cdot x_i + b &gt; 0, y_i &gt; 0 \w \cdot x_i + b &lt; 0, y_i &lt; 0$$也就是：$$y_i(w \cdot x_i + b) &gt; 0$$ 约束2超平面参数$w, b$成比例的更改，不会影响几何间隔。此处为了推导方便，可以在距离超平面几何距离最近的这个点$x_i$上（Support Vector），让$w, b$成比例的更改，使得$|y_i(w \cdot x_i + b) =1|$, 那么对于$r$就是在$x_i$这点上： 可以理解：无论$w,b$如何成比例的更改，该超平面$w \cdot x + b =0$还是该超平面，所以点到超平面的几何距离不会变化 $$\gamma = \mathop{\min}_{i=1,….,N} \gamma_i = y_i\frac{w \cdot x_i + b}{||w||}$$ 如果数据集被正确划分，那么$y_i(w \cdot x_i +b) &gt; 0$， 而且几何距离最小的点$x_i$，距离超平面的距离为$|y_i(w \cdot x_i + b) =1|$，也就是$y_i(w \cdot x_i +b) =1$。那么对于所有的点有$y_i(w \cdot x_i +b) &gt;=1$。那么：$$r = \min_{i=1,..N} \gamma_i = \frac{|y_i(w \cdot x_i + b) |}{||w||} = \frac{1}{||w||}, \ \ \ \ \ \ \ y_i(w \cdot x_i + b) &gt;= 1$$ 间隔最大化我们需要最大化 该点到超平面的几何间隔$\gamma​$，即：$$\max r = \max \frac{1}{||w||} = \min \frac{1}{2} {||w||}^2$$ $$y_i(w \cdot x_i + b) &gt;= 1$$ 拉格朗日求解构造拉格朗日函数SVM的目标函数为，有多个不等式约束条件的优化为题，其拉格朗日函数可以写为： $$ L(\omega,b,\alpha) = \frac{{||w||^2}}{2} + \sum_{i=1}^{m}\alpha_i(1-y_i(\omega^Tx_i + b)) $$ 其中$\omega, b$为SVM待求的目标参数，$\alpha$为构建拉格朗日的参数。 对偶问题现在的目标是：$$\min_{\omega,b}[\max_{\alpha:\alpha_j \ge 0}L(\omega,b,\alpha)]$$也就是保证每一个$\alpha \ge 0$的情况下，最小的$L(\omega, b, \alpha)$的$\omega, b$； 转化为对偶问题：$$\max_{\alpha:\alpha \ge 0}[\min_{\omega,b}L(\omega,b,\alpha)]$$ 求解 先求解$\min_{w, b}L(\omega,b,\alpha)$： $$\min_{\omega,b}L(\omega,b,\alpha) = \min_{\omega, b}[\frac{1}{2}||w||^2 + \sum_{i=1}^{m}\alpha_i(1- y_i(\omega^Tx_i + b))]$$ 分别对$\omega, b$求导：$$\frac{\partial L}{\partial \omega} = \omega - \sum_{i=1}^{m}\alpha_iy_ix_i = 0 \rightarrow \omega = \sum_{i=1}^{m}\alpha_iy_ix_i$$ $$\frac{\partial L}{\partial b} = - \sum_{i=1}^{m}\alpha_iy_i = 0 \rightarrow \sum_{i=1}^{m}\alpha_iy_i = 0$$ 代入拉格朗日函数：$$\min_{\omega,b} L(\omega, b, \alpha) = \frac{1}{2}\Big [\sum_{i=1}^{m}\alpha_iy_ix_i \Big]^T \Big[\sum_{i=1}^{m}\alpha_iy_ix_i\Big] + \sum_{i=1}^m \alpha_i - (\sum_{i=1}^{m}\alpha_ix_iy_i)(\sum_{j=1}^{m}\alpha_jy_jx_j^T) -\sum_{i=1}^{m}\alpha_i y_i b \ = \frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_ix_iy_i\alpha_jy_jx_j^T+ \sum_{i=1}^m \alpha_i - \sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_ix_iy_i\alpha_jy_jx_j^T \= \sum_{i=1}^m \alpha_i - \frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_i\alpha_jy_iy_jx_i^Tx_j$$ 求解$\max {\alpha : \alpha \ge 0}\sum{i=1}^m \alpha_i - \frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_i\alpha_jy_iy_jx_i^Tx_j​$ 且：$\sum_{i=1}^{m}\alpha_iy_i = 0$$$N(\alpha_i, \alpha_j) = \sum_{i=1}^m \alpha_i - \frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_i\alpha_jy_iy_jx_i^Tx_j$$对$\alpha_i, \alpha_j$分别求导：$$\frac{\partial N}{\partial \alpha_j} = \sum_{i=1}^{m}\alpha_i-\frac{1}{2}\sum_{i=1}^{m}\alpha_iy_iy_jx_i^Tx_j = 0$$ $$\frac{\partial N}{\partial \alpha_i} = -\frac{1}{2}\sum_{j=1}^{m}\alpha_jy_iy_jx_i^Tx_j = 0$$ 可以分别求得$\alpha_i, \alpha_j$ ，把求得的$\alpha_i, \alpha_j$，带入$\omega = \sum_{i=1}^{m}\alpha_iy_ix_i$ ，就可以求得最大几何间隔的$\omega$了。 拉格朗日函数求解示例优化目标函数：$$\min_x f(x) = x_i^2 + x_j^2$$约束条件：$$h(x) = x_1 - x_2 -2 = 0$$ $$g(x) = (x_1 -2)^2 + x_2^2 -1 \ge 0$$ 构建拉格朗日函数：$$L(x,\alpha, \beta) = (x_1^2 + x_2^2) + \alpha(x_1 - x_2 -2) + \beta \Big[(x_1 -2)^2 + x_2^2 -1 \Big]$$先对$x_1, x_2$求导：$$\frac{\partial L}{\partial x_1} = 2x_1 + \alpha + 2 \beta (x_1-2) = 0 \rightarrow x_1 = \frac{4\beta -\alpha}{2\beta + 2}$$ $$\frac{\partial L}{\partial x_2} = 2x_2 - \alpha + 2\beta x_2 = 0 \rightarrow x_2 = \frac{\alpha}{2\beta +2}$$ 带入$L(x,\alpha, \beta)$：$$L(x, \alpha, \beta) = - \frac{\alpha^2 + 4\alpha + 2\beta^2 -6\beta}{2\beta + 1}$$再分别对$\alpha, \beta$求导：$$\frac{\partial L}{\partial\alpha}$$ $$\frac{\partial L}{\partial \beta}$$ 得到：$$\alpha = -2, \beta = \sqrt{2} -1$$带入$x_1, x_2$得：$$x_1 = 2 - \frac{\sqrt{2}}{2} , x_2 = - \frac{\sqrt{2}}{2}$$ 软间隔和松弛变量前面的推导是基于数据是线性可分的，但是实际情况是，数据可能并不是线性可分到。 但是这也分两种：第一种，数据点分布确实很复杂，需要通过核函数映射到高维空间，才能在高维空间可分；第二种，有一些噪点导致本来线性可分的数据，现在不可分了。 解决第二种问题点办法就是引入松弛变量。 图中蓝色的点就是噪音，导致本可以分开的数据点，现在不可分了。 我们之前线性可分的约束条件为：$$y_i(w^T x_i + b) \ge 1$$现在我们给他加一个松弛变量：$$y_i(w^T x_i + b) \ge 1 - \xi_i$$同样，目标函数变为:$$\min \frac{1}{2}||w||^2 + C\sum_{i=1}^{n}\xi_i$$拉格朗日函数为:$$L(\omega, b, \xi, \alpha, r) = \frac{1}{2}||w||^2 + C\sum_{i=1}^{n}\xi_i - \sum_{i=1}^{n}\alpha_i(y_i(\omega^Tx_i + b) - 1 + \xi_i) - \sum_{i=1}{n}r_i\xi_i$$同样对$\omega, b, \xi$求导：$$\frac{\partial L}{\partial \omega} = 0 \to \omega = \sum_i^{n}\alpha_ix_iy_i$$ $$\frac{\partial L}{\partial b} = 0 \to \sum_{i=1}^{n}\alpha_iy_i = 0$$ $$\frac{\partial L}{\partial \xi_i} = 0 \to C-\alpha_i -r_i = 0$$ 带入L得：$$L(\omega, b, \xi, \alpha, r) = \sum_{i=1}^{n}\alpha_i -\frac{1}{2}\alpha_i\alpha_jy_iy_j&lt;x_i,x_j&gt;$$ $$0 \le \alpha_i \le C, i =1,…n$$ $$\sum_{i=1}^{n}\alpha_iy_i = 0$$ 其中C为控制”寻找margin最大的超平面”和”保证数据点偏差最小”之间的权重，自己定义。]]></content>
  </entry>
  <entry>
    <title><![CDATA[PCA主成分分析法]]></title>
    <url>%2F2020%2F08%2F16%2FPCA%E6%8E%A8%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[PCA主成分分析法实践其实主成分分析法的实现方法非常明了，一步步的来就可以。但是想了解的更加透彻，知道为什么这样做是可以的，或许需要一步步的推导公式开始，先说实现方法。 数据 假设我们得到的2维数据如下： 第一步分别求x和y的平均值，然后对于所有的样例，都减去对应的均值。这里x的均值是1.81，y的均值是1.91，那么一个样例减去均值后即为（0.69,0.49），得到 第二步求特征协方差矩阵 对角线上分别是x和y的方差，非对角线上是协方差。协方差是衡量两个变量同时变化的变化程度。 第三步求协方差的特征值和特征向量 第四步将特征值按照从大到小的顺序排序，我们选择其中最大的那个，这里是1.28402771，对应的特征向量是 第五步将样本点投影到选取的特征向量上。 结束 对比PCA之前： PCA之后 其实PCA的本质就是减少坐标轴、旋转坐标轴，使得减少坐标轴之后、旋转坐标轴之后的数据特征的离散程度，尽可能的损失的少，尽可能的代表原来的数据的特征。 公式推导放在前面作为一些知识科普 开始推导 写在后面引用1.主成分分析法详解 2.【机器学习】主成分分析详解]]></content>
  </entry>
  <entry>
    <title><![CDATA[Python中的Gevent]]></title>
    <url>%2F2020%2F08%2F16%2FPython%E4%B8%AD%E7%9A%84Gevent%2F</url>
    <content type="text"><![CDATA[Gevent不同的网络模型 阻塞式单进程。 一个进程一个循环处理网络请求，当然性能也是最差的 阻塞式多进程。 每个请求开一个进程去处理，这样就能同时处理多个请求了， 不过缺点就是当请求数变大时CPU花在进程切换开销巨大，效率低下。 非阻塞式事件驱动。 也是多进程，不过使用一个主进程循环来检查是否有网络I/O事件发生，再来决定怎样处理。 省去了上下文切换、进程复制等成本，也不会有死锁、竞争的发生。 不过缺点是没有阻塞式进程直观 非阻塞式Coroutine(协程)。 它的本质也是事件驱动，只在单一循环上面检查事件的发生，但是加上了coroutine的概念。 gevent就是这种框架的代表。 协程 简单来讲，coroutine就是允许你暂时中断之后再继续执行的程序。事实上，python最基础的coroutine就是生成器。 下面代码示例解释了：第一次的next(bar)，执行到了yield，执行两次print后，再执行next(bar)， print(u’foo: 控制权又回到我手上了，叫我大笨蛋’)再次停留在yield，再次print(u’main: 老大我又来了’) 12345678910111213141516171819#!/usr/bin/env python# -*- coding: utf8 -*-def foo(): for i in range(10): # 暂时返回一个值，并将控制权交出去 yield i print(u'foo: 控制权又回到我手上了，叫我大笨蛋')bar = foo()# 执行coroutineprint(next(bar))print(u'main: 现在控制权在我手上，啦啦啦')print('main:hello baby!')# 回到刚刚foo这个coroutine中断的地方继续执行print(next(bar))print(u'main: 老大我又来了')print(next(bar))print(next(bar)) 0 main: 现在控制权在我手上，啦啦啦 main:hello baby! foo: 控制权又回到我手上了，叫我大笨蛋 1 main: 老大我又来了 foo: 控制权又回到我手上了，叫我大笨蛋 2 foo: 控制权又回到我手上了，叫我大笨蛋 3Gevent原理 事实上程序写的跟普通的阻塞式程序一样，但它是异步的 monkey.patch_all()，也就是猴子补丁。因为python内置的各种函数库和IO库一般都是阻塞式的，比如sleep()就会当前进程，而monkey就是负责将这些阻塞函数全部取代替换成gevent中相应的异步函数。 gevent打了monkey patch之后会设置python相应的模块设置成非阻塞，然后在内部实现epoll的机制，一旦调用非阻塞的IO(比如recv)都会立刻返回，并且设置一个回调函数，这个回调函数用于切换到当前子coroutine，设置好回掉函数之后就把控制权返回给主coroutine，主coroutine继续调度。一旦网络I/O准备就绪，epoll会触发之前设置的回调函数，从而引发主coroutine切换到子coroutine，做相应的操作。 joinall()的意思是等待列表中所有coroutine完成后再返回。 12345678910111213141516171819202122"""Spawn multiple workers and wait for them to complete"""urls = [ 'http://www.gevent.org/', 'http://www.baidu.com', 'http://www.python.org']import geventfrom gevent import monkey# patches stdlib (including socket and ssl modules) to cooperate with other greenletsmonkey.patch_all()import urllib2def print_head(url): print 'Starting %s' % url data = urllib2.urlopen(url).read() print '%s: %s bytes: %r' % (url, len(data), data[:50])# jobs = list()# for url in urls:# jobs.append(gevent.spawn(print_head, url))jobs = [gevent.spawn(print_head, url) for url in urls]gevent.joinall(jobs) The history saving thread hit an unexpected error (LoopExit(&apos;This operation would block forever&apos;, &lt;Hub at 0x102fbc050 select pending=0 ref=0&gt;)).History will not be written to the database. Starting http://www.gevent.org/ Starting http://www.baidu.com Starting http://www.python.org http://www.baidu.com: 112190 bytes: &apos;&lt;!DOCTYPE html&gt;\n&lt;!--STATUS OK--&gt;\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r&apos; http://www.python.org: 48853 bytes: &apos;&lt;!doctype html&gt;\n&lt;!--[if lt IE 7]&gt; &lt;html class=&quot;n&apos; http://www.gevent.org/: 8734 bytes: &apos;&lt;!DOCTYPE html PUBLIC &quot;-//W3C//DTD XHTML 1.0 Trans&apos;上下文切换gevent.spawn 的重要功能就是封装了greenlet里面的函数。 初始化的greenlet放在了threads这个list里面， 被传递给了 gevent.joinall 这个函数，它会阻塞主程序来执行所有的greenlet。在异步执行的情况下，所有任务的执行顺序是完全随机的。每一个greenlet的都不会阻塞其他greenlet的执行。 简单示例12345678910111213141516171819import geventdef foo(): print('1') gevent.sleep(0) print('2')def bar(): print('3') gevent.sleep(0) print('4')gevent.joinall([ gevent.spawn(foo), gevent.spawn(bar),]) 1 3 2 4 [&lt;Greenlet at 0x10dd045f0&gt;, &lt;Greenlet at 0x10dd04870&gt;]执行顺序示例代码1234567891011121314151617181920212223242526import geventimport randomdef task(pid): """ Some non-deterministic task """ gevent.sleep(random.randint(0, 2) * 0.001) print('Task', pid, 'done')def synchronous(): for i in range(1, 10): task(i)def asynchronous(): threads = [gevent.spawn(task, i) for i in xrange(10)] gevent.joinall(threads)print('不采用协程的执行顺序，Synchronous:')synchronous()print('采用协程的执行顺序，Asynchronous:')asynchronous() 不采用协程的执行顺序，Synchronous: (&apos;Task&apos;, 1, &apos;done&apos;) (&apos;Task&apos;, 2, &apos;done&apos;) (&apos;Task&apos;, 3, &apos;done&apos;) (&apos;Task&apos;, 4, &apos;done&apos;) (&apos;Task&apos;, 5, &apos;done&apos;) (&apos;Task&apos;, 6, &apos;done&apos;) (&apos;Task&apos;, 7, &apos;done&apos;) (&apos;Task&apos;, 8, &apos;done&apos;) (&apos;Task&apos;, 9, &apos;done&apos;) 采用协程的执行顺序，Asynchronous: (&apos;Task&apos;, 1, &apos;done&apos;) (&apos;Task&apos;, 4, &apos;done&apos;) (&apos;Task&apos;, 5, &apos;done&apos;) (&apos;Task&apos;, 7, &apos;done&apos;) (&apos;Task&apos;, 6, &apos;done&apos;) (&apos;Task&apos;, 9, &apos;done&apos;) (&apos;Task&apos;, 3, &apos;done&apos;) (&apos;Task&apos;, 0, &apos;done&apos;) (&apos;Task&apos;, 2, &apos;done&apos;) (&apos;Task&apos;, 8, &apos;done&apos;)生成greenlet我们通过gevent.spawn来包装了对于greenlet的生成， 另外我们还能可以通过创建Greenlet的子类，并且重写 _run 方法来实现我的理解是：可以自定义gevent.spawn，协程内部的执行功能 123456789101112131415161718import geventfrom gevent import Greenletclass MyGreenlet(Greenlet): def __init__(self, message, n): Greenlet.__init__(self) self.message = message self.n = n def _run(self): gevent.sleep(self.n) print(self.message)g = MyGreenlet("Hi there!", 3)g.start()g.join() Hi there!​ Greenlet 的状态greenlet在执行的时候也会出错。Greenlet有可能会无法抛出异常，停止失败，或者消耗了太多的系统资源。greenlet的内部状态通常是一个依赖时间的参数。greenlet有一些标记来让你能够监控greenlet的状态 started – 标志greenlet是否已经启动 ready – 标志greenlet是否已经被终止 successful() – 标志greenlet是否已经被终止，并且没有抛出异常 value – 由greenlet返回的值 exception – 在greenlet里面没有被捕获的异常 123456789101112131415161718192021222324252627282930import geventdef win(): return 'You win!'def fail(): raise Exception('You fail at failing.')winner = gevent.spawn(win)loser = gevent.spawn(fail)print(winner.started) # Trueprint(loser.started) # True# Exceptions raised in the Greenlet, stay inside the Greenlet.try: gevent.joinall([winner, loser])except Exception as e: print('This will never be reached')print(winner.value) # 'You win!'print(loser.value) # Noneprint(winner.ready()) # Trueprint(loser.ready()) # Trueprint(winner.successful()) # Trueprint(loser.successful()) # False# The exception raised in fail, will not propogate outside the# greenlet. A stack trace will be printed to stdout but it# will not unwind the stack of the parent.print(loser.exception) True True You win! None True True True False You fail at failing. Traceback (most recent call last): File &quot;C:\ProgramData\Anaconda2\lib\site-packages\gevent\greenlet.py&quot;, line 536, in run result = self._run(*self.args, **self.kwargs) File &quot;&lt;ipython-input-8-e403e0ec5a71&gt;&quot;, line 9, in fail raise Exception(&apos;You fail at failing.&apos;) Exception: You fail at failing. Wed Nov 22 10:44:28 2017 &lt;Greenlet at 0x65a6a60L: fail&gt; failed with Exception​ 终止程序在主程序收到一个SIGQUIT 之后会阻塞程序的执行让Greenlet无法继续执行。 这会导致僵尸进程的产生，需要在操作系统中将这些僵尸进程清除掉。 123456789101112import geventimport signaldef run_forever(): gevent.sleep(1000)if __name__ == '__main__': gevent.signal(signal.SIGQUIT, gevent.shutdown) thread = gevent.spawn(run_forever) thread.join() --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) &lt;ipython-input-10-ebfbe3246457&gt; in &lt;module&gt;() 8 9 if __name__ == &apos;__main__&apos;: ---&gt; 10 gevent.signal(signal.SIGQUIT, gevent.shutdown) 11 thread = gevent.spawn(run_forever) 12 thread.join() AttributeError: &apos;module&apos; object has no attribute &apos;SIGQUIT&apos;超时在gevent中支持对于coroutine的超时控制，还能使用with上下文 如果协程sleep的时间，小于设置的wait时间，则没有问题 1234567891011import geventfrom gevent import Timeouttime_to_wait = 6 # secondsclass TooLong(Exception): passwith Timeout(time_to_wait, TooLong): gevent.sleep(5) 如果协程sleep的时间，大于设置的wait时间，则会报异常 1234567891011import geventfrom gevent import Timeouttime_to_wait = 2 # secondsclass TooLong(Exception): passwith Timeout(time_to_wait, TooLong): gevent.sleep(5) --------------------------------------------------------------------------- TooLong Traceback (most recent call last) &lt;ipython-input-13-ac2197c4c150&gt; in &lt;module&gt;() 9 10 with Timeout(time_to_wait, TooLong): ---&gt; 11 gevent.sleep(5) C:\ProgramData\Anaconda2\lib\site-packages\gevent\hub.py in sleep(seconds, ref) 167 waiter.get() 168 else: --&gt; 169 hub.wait(loop.timer(seconds, ref=ref)) 170 171 C:\ProgramData\Anaconda2\lib\site-packages\gevent\hub.py in wait(self, watcher) 649 watcher.start(waiter.switch, unique) 650 try: --&gt; 651 result = waiter.get() 652 if result is not unique: 653 raise InvalidSwitchError(&apos;Invalid switch into %s: %r (expected %r)&apos; % (getcurrent(), result, unique)) C:\ProgramData\Anaconda2\lib\site-packages\gevent\hub.py in get(self) 897 self.greenlet = getcurrent() 898 try: --&gt; 899 return self.hub.switch() 900 finally: 901 self.greenlet = None C:\ProgramData\Anaconda2\lib\site-packages\gevent\hub.py in switch(self) 628 if switch_out is not None: 629 switch_out() --&gt; 630 return RawGreenlet.switch(self) 631 632 def switch_out(self): TooLong: 猴子补丁Python的运行时里面允许能够大部分的对象都是可以修改的，包括模块，类和方法。这通常是一个坏主意， 然而在极端的情况下，当有一个库需要加入一些Python基本的功能的时候，monkey patch就能派上用场了。 在上面的例子里，gevent能够改变基础库里的一些使用IO阻塞模型的库比如socket，ssl，threading等等并且把它们改成协程的执行方式。 1234567891011121314import socketprint(socket.socket)print "After monkey patch"from gevent import monkeymonkey.patch_socket()print(socket.socket)import selectprint(select.select)monkey.patch_select()print "After monkey patch"print(select.select) &lt;class &apos;socket._socketobject&apos;&gt; After monkey patch &lt;class &apos;gevent._socket2.socket&apos;&gt; &lt;built-in function select&gt; After monkey patch &lt;function select at 0x000000000632CF28&gt;事件有时候我们还需要在多个greenlet直接进行通信，比如某些操作的同步。事件(event)是一个在Greenlet之间异步通信的形式。 12345678910111213141516171819202122232425262728293031323334import geventfrom gevent.event import Event'''Illustrates the use of events'''evt = Event()def setter(): '''After 3 seconds, wake all threads waiting on the value of evt''' print('setter: Hey wait for me, I will sleep 3s') gevent.sleep(3) print("setter: I'm done") evt.set()def waiter(): '''After 3 seconds the get call will unblock''' print("waiter: I'll wait for you") evt.wait() # blocking print("waiter: It's my turn")def main(): gevent.joinall([ gevent.spawn(setter), gevent.spawn(waiter), gevent.spawn(waiter), gevent.spawn(waiter), ])if __name__ == '__main__': main() setter: Hey wait for me, I will sleep 3s waiter: I&apos;ll wait for you waiter: I&apos;ll wait for you waiter: I&apos;ll wait for you setter: I&apos;m done waiter: It&apos;s my turn waiter: It&apos;s my turn waiter: It&apos;s my turn1234567891011121314151617181920212223242526272829import geventfrom gevent.event import AsyncResulta = AsyncResult()def setter(): """ After 3 seconds set the result of a. """ print 'setter start' gevent.sleep(3) print 'setter start to set' a.set('Hello!')def waiter(): """ After 3 seconds the get call will unblock after the setter puts a value into the AsyncResult. """ print 'waiter start' print 'waiter start to get' print(a.get())gevent.joinall([ gevent.spawn(setter), gevent.spawn(waiter),]) getter start to get setter start to set Hello! [&lt;Greenlet at 0x62aa6d0L&gt;, &lt;Greenlet at 0x6367c28L&gt;]队列队列是一个排序的数据集合，它有常见的put / get操作，但是它是以在Greenlet之间可以安全操作的方式来实现的。举例来说，如果一个Greenlet从队列中取出一项，此项就不会被同时执行的其它Greenlet再取到了。 阻塞put/getcase1下面的case，在遇到sleep能够执行完毕的原因在于，join方法，会让boss，无论sleep多久，该协程执行完毕之后，再执行下面的joinall,所以可以正常退出。 1234567891011121314151617181920212223242526import geventfrom gevent.queue import Queuetasks = Queue()def worker(n): while not tasks.empty(): task = tasks.get() print('Worker %s got task %s' % (n, task))# gevent.sleep(0) print('Quitting time!')def boss(): for i in range(1, 25): if i == 10: gevent.sleep(10) tasks.put(i)gevent.spawn(boss).join()gevent.joinall([ gevent.spawn(worker, 'steve'), gevent.spawn(worker, 'john'), gevent.spawn(worker, 'nancy'),]) Worker steve got task 1 Worker steve got task 2 Worker steve got task 3 Worker steve got task 4 Worker steve got task 5 Worker steve got task 6 Worker steve got task 7 Worker steve got task 8 Worker steve got task 9 Worker steve got task 10 Worker steve got task 11 Worker steve got task 12 Worker steve got task 13 Worker steve got task 14 Worker steve got task 15 Worker steve got task 16 Worker steve got task 17 Worker steve got task 18 Worker steve got task 19 Worker steve got task 20 Worker steve got task 21 Worker steve got task 22 Worker steve got task 23 Worker steve got task 24 Quitting time! Quitting time! Quitting time! [&lt;Greenlet at 0x627c210L&gt;, &lt;Greenlet at 0x627c2a8L&gt;, &lt;Greenlet at 0x627c340L&gt;]case2boss发送者模型，和消费者worker模型，同时在协程里时：boss会先put 1-9，sleep 10秒时，交出控制权，worker开始运行，等sleep 10 秒过后，继续put，直到全部put完毕，才会get。协程之间，如果不用sleep交出控制权（不阻塞）的话，会一直执行同一个，直到运行结束。 1234567891011121314151617181920212223242526import geventfrom gevent.queue import Queuetasks = Queue()def worker(n): while True: task = tasks.get() print('Worker %s got task %s' % (n, task)) print('Quitting time!')def boss(): for i in range(1, 25): if i == 10: gevent.sleep(10) tasks.put(i)task_list = [ gevent.spawn(boss), gevent.spawn(worker, 'steve'), gevent.spawn(worker, 'john'), gevent.spawn(worker, 'nancy'),]gevent.joinall(task_list) Worker steve got task 1 Worker steve got task 2 Worker steve got task 3 Worker steve got task 4 Worker steve got task 5 Worker steve got task 6 Worker steve got task 7 Worker steve got task 8 Worker steve got task 9 Worker steve got task 10 Worker steve got task 11 Worker steve got task 12 Worker steve got task 13 Worker steve got task 14 Worker steve got task 15 Worker steve got task 16 Worker steve got task 17 Worker steve got task 18 Worker steve got task 19 Worker steve got task 20 Worker steve got task 21 Worker steve got task 22 Worker steve got task 23 Worker steve got task 24 --------------------------------------------------------------------------- LoopExit Traceback (most recent call last) &lt;ipython-input-6-8e3bd6d0d8c4&gt; in &lt;module&gt;() 29 30 # boss() ---&gt; 31 gevent.joinall(task_list) C:\ProgramData\Anaconda2\lib\site-packages\gevent\greenlet.pyc in joinall(greenlets, timeout, raise_error, count) 647 &quot;&quot;&quot; 648 if not raise_error: --&gt; 649 return wait(greenlets, timeout=timeout, count=count) 650 651 done = [] C:\ProgramData\Anaconda2\lib\site-packages\gevent\hub.pyc in wait(objects, timeout, count) 1036 if objects is None: 1037 return get_hub().join(timeout=timeout) -&gt; 1038 return list(iwait(objects, timeout, count)) 1039 1040 C:\ProgramData\Anaconda2\lib\site-packages\gevent\hub.pyc in iwait(objects, timeout, count) 983 984 for _ in xrange(count): --&gt; 985 item = waiter.get() 986 waiter.clear() 987 if item is _NONE: C:\ProgramData\Anaconda2\lib\site-packages\gevent\hub.pyc in get(self) 937 def get(self): 938 if not self._values: --&gt; 939 Waiter.get(self) 940 Waiter.clear(self) 941 C:\ProgramData\Anaconda2\lib\site-packages\gevent\hub.pyc in get(self) 897 self.greenlet = getcurrent() 898 try: --&gt; 899 return self.hub.switch() 900 finally: 901 self.greenlet = None C:\ProgramData\Anaconda2\lib\site-packages\gevent\hub.pyc in switch(self) 628 if switch_out is not None: 629 switch_out() --&gt; 630 return RawGreenlet.switch(self) 631 632 def switch_out(self): LoopExit: (&apos;This operation would block forever&apos;, &lt;Hub at 0x627c048 select default pending=0 ref=0&gt;)下面的代码，等同于上面的效果 123456789101112131415161718192021222324252627import geventfrom gevent.queue import Queuetasks = Queue()def worker(n): while True: task = tasks.get() print('Worker %s got task %s' % (n, task)) print('Quitting time!')def boss(): for i in range(1, 25): if i == 10: gevent.sleep(10) tasks.put(i)task_list = [ gevent.spawn(worker, 'steve'), gevent.spawn(worker, 'john'), gevent.spawn(worker, 'nancy'),]boss()gevent.joinall(task_list) Worker steve got task 1 Worker steve got task 2 Worker steve got task 3 Worker steve got task 4 Worker steve got task 5 Worker steve got task 6 Worker steve got task 7 Worker steve got task 8 Worker steve got task 9 Worker steve got task 10 Worker steve got task 11 Worker steve got task 12 Worker steve got task 13 Worker steve got task 14 Worker steve got task 15 Worker steve got task 16 Worker steve got task 17 Worker steve got task 18 Worker steve got task 19 Worker steve got task 20 Worker steve got task 21 Worker steve got task 22 Worker steve got task 23 Worker steve got task 24 --------------------------------------------------------------------------- LoopExit Traceback (most recent call last) &lt;ipython-input-7-00652269a509&gt; in &lt;module&gt;() 26 27 boss() ---&gt; 28 gevent.joinall(task_list) C:\ProgramData\Anaconda2\lib\site-packages\gevent\greenlet.pyc in joinall(greenlets, timeout, raise_error, count) 647 &quot;&quot;&quot; 648 if not raise_error: --&gt; 649 return wait(greenlets, timeout=timeout, count=count) 650 651 done = [] C:\ProgramData\Anaconda2\lib\site-packages\gevent\hub.pyc in wait(objects, timeout, count) 1036 if objects is None: 1037 return get_hub().join(timeout=timeout) -&gt; 1038 return list(iwait(objects, timeout, count)) 1039 1040 C:\ProgramData\Anaconda2\lib\site-packages\gevent\hub.pyc in iwait(objects, timeout, count) 983 984 for _ in xrange(count): --&gt; 985 item = waiter.get() 986 waiter.clear() 987 if item is _NONE: C:\ProgramData\Anaconda2\lib\site-packages\gevent\hub.pyc in get(self) 937 def get(self): 938 if not self._values: --&gt; 939 Waiter.get(self) 940 Waiter.clear(self) 941 C:\ProgramData\Anaconda2\lib\site-packages\gevent\hub.pyc in get(self) 897 self.greenlet = getcurrent() 898 try: --&gt; 899 return self.hub.switch() 900 finally: 901 self.greenlet = None C:\ProgramData\Anaconda2\lib\site-packages\gevent\hub.pyc in switch(self) 628 if switch_out is not None: 629 switch_out() --&gt; 630 return RawGreenlet.switch(self) 631 632 def switch_out(self): LoopExit: (&apos;This operation would block forever&apos;, &lt;Hub at 0x627c048 select default pending=0 ref=0&gt;)case3下面的case不同于case2，boss相当于并没有采用协程的概念，即使sleep了，也全部放入队列里之后，才会执行下面的消费者协程 123456789101112131415161718192021222324252627import geventfrom gevent.queue import Queuetasks = Queue()def worker(n): while True: task = tasks.get() print('Worker %s got task %s' % (n, task)) print('Quitting time!')def boss(): for i in range(1, 25): if i == 10: gevent.sleep(10) tasks.put(i)boss()task_list = [ gevent.spawn(worker, 'steve'), gevent.spawn(worker, 'john'), gevent.spawn(worker, 'nancy'),]gevent.joinall(task_list) Worker steve got task 1 Worker steve got task 2 Worker steve got task 3 Worker steve got task 4 Worker steve got task 5 Worker steve got task 6 Worker steve got task 7 Worker steve got task 8 Worker steve got task 9 Worker steve got task 10 Worker steve got task 11 Worker steve got task 12 Worker steve got task 13 Worker steve got task 14 Worker steve got task 15 Worker steve got task 16 Worker steve got task 17 Worker steve got task 18 Worker steve got task 19 Worker steve got task 20 Worker steve got task 21 Worker steve got task 22 Worker steve got task 23 Worker steve got task 24 --------------------------------------------------------------------------- LoopExit Traceback (most recent call last) &lt;ipython-input-8-e77d06944ce9&gt; in &lt;module&gt;() 26 gevent.spawn(worker, &apos;nancy&apos;), 27 ] ---&gt; 28 gevent.joinall(task_list) C:\ProgramData\Anaconda2\lib\site-packages\gevent\greenlet.pyc in joinall(greenlets, timeout, raise_error, count) 647 &quot;&quot;&quot; 648 if not raise_error: --&gt; 649 return wait(greenlets, timeout=timeout, count=count) 650 651 done = [] C:\ProgramData\Anaconda2\lib\site-packages\gevent\hub.pyc in wait(objects, timeout, count) 1036 if objects is None: 1037 return get_hub().join(timeout=timeout) -&gt; 1038 return list(iwait(objects, timeout, count)) 1039 1040 C:\ProgramData\Anaconda2\lib\site-packages\gevent\hub.pyc in iwait(objects, timeout, count) 983 984 for _ in xrange(count): --&gt; 985 item = waiter.get() 986 waiter.clear() 987 if item is _NONE: C:\ProgramData\Anaconda2\lib\site-packages\gevent\hub.pyc in get(self) 937 def get(self): 938 if not self._values: --&gt; 939 Waiter.get(self) 940 Waiter.clear(self) 941 C:\ProgramData\Anaconda2\lib\site-packages\gevent\hub.pyc in get(self) 897 self.greenlet = getcurrent() 898 try: --&gt; 899 return self.hub.switch() 900 finally: 901 self.greenlet = None C:\ProgramData\Anaconda2\lib\site-packages\gevent\hub.pyc in switch(self) 628 if switch_out is not None: 629 switch_out() --&gt; 630 return RawGreenlet.switch(self) 631 632 def switch_out(self): LoopExit: (&apos;This operation would block forever&apos;, &lt;Hub at 0x627c048 select default pending=0 ref=0&gt;)case4正确的退出方式应该是这样的！！！ 1234567891011121314151617181920212223242526272829import geventfrom gevent.queue import Queuetasks = Queue(maxsize=5)Thread_SIZE = 3 def worker(): while True: try: task = tasks.get() if task == "stop": break print('Worker got task %s' % (task)) except Exception as e: print u"error : " + str(e) print('Quitting time!')def boss(): for i in range(1, 10): tasks.put(i) for i in range(Thread_SIZE): tasks.put("stop") task_list = list()task_list.append(gevent.spawn(boss))for i in range(Thread_SIZE): task_list.append(gevent.spawn(worker)) gevent.joinall(task_list) Worker got task 1 Worker got task 2 Worker got task 3 Worker got task 4 Worker got task 5 Worker got task 6 Worker got task 7 Worker got task 8 Worker got task 9 Quitting time! Quitting time! Quitting time! [&lt;Greenlet at 0x652a210L&gt;, &lt;Greenlet at 0x652a178L&gt;, &lt;Greenlet at 0x652a2a8L&gt;, &lt;Greenlet at 0x652a340L&gt;]阻塞 put_nowait和get_nowait在操作不能完成时抛出gevent.queue.Empty或gevent.queue.Full异常 queue 最大长度 不设置Queue(maxsize=3)，在发送协程没有阻塞的时候，持续发送，直到完毕。 123456789101112131415161718192021222324252627282930313233import geventfrom gevent.queue import Queue, Emptytasks = Queue()def worker(n): try: while True: task = tasks.get(timeout=1) # decrements queue size by 1 print('Worker %s got task %s' % (n, task)) gevent.sleep(0) except Empty: print('Quitting time!') def boss(): """ Boss will wait to hand out work until a individual worker is free since the maxsize of the task queue is 3. """ for i in xrange(1,10): tasks.put(i) print u"tasks size : " + str(tasks.qsize()) print('Assigned all work in iteration 1') for i in xrange(10,20): tasks.put(i) print u"tasks size : " + str(tasks.qsize()) print('Assigned all work in iteration 2') gevent.joinall([ gevent.spawn(boss), gevent.spawn(worker, 'steve'), gevent.spawn(worker, 'john'), gevent.spawn(worker, 'bob'),]) tasks size : 1 tasks size : 2 tasks size : 3 tasks size : 4 tasks size : 5 tasks size : 6 tasks size : 7 tasks size : 8 tasks size : 9 Assigned all work in iteration 1 tasks size : 10 tasks size : 11 tasks size : 12 tasks size : 13 tasks size : 14 tasks size : 15 tasks size : 16 tasks size : 17 tasks size : 18 tasks size : 19 Assigned all work in iteration 2 Worker steve got task 1 Worker john got task 2 Worker bob got task 3 Worker steve got task 4 Worker john got task 5 Worker bob got task 6 Worker steve got task 7 Worker john got task 8 Worker bob got task 9 Worker steve got task 10 Worker john got task 11 Worker bob got task 12 Worker steve got task 13 Worker john got task 14 Worker bob got task 15 Worker steve got task 16 Worker john got task 17 Worker bob got task 18 Worker steve got task 19 Quitting time! Quitting time! Quitting time! [&lt;Greenlet at 0x1055f1690&gt;, &lt;Greenlet at 0x1055f1870&gt;, &lt;Greenlet at 0x1055f1f50&gt;, &lt;Greenlet at 0x1055f12d0&gt;] 设置Queue(maxsize=3)，当发送协程queue为3时，会暂停，交出控制权，让消费者去消耗，小于3时，再put. 123456789101112131415161718192021222324252627282930313233import geventfrom gevent.queue import Queue, Emptytasks = Queue(maxsize=3)def worker(n): try: while True: task = tasks.get(timeout=10) # decrements queue size by 1 print('Worker %s got task %s' % (n, task)) gevent.sleep(0) except Empty: print('Quitting time!') def boss(): """ Boss will wait to hand out work until a individual worker is free since the maxsize of the task queue is 3. """ for i in xrange(1,10): tasks.put(i) print u"tasks size : " + str(tasks.qsize()) print('Assigned all work in iteration 1') for i in xrange(10,20): tasks.put(i) print u"tasks size : " + str(tasks.qsize()) print('Assigned all work in iteration 2') gevent.joinall([ gevent.spawn(boss), gevent.spawn(worker, 'steve'), gevent.spawn(worker, 'john'), gevent.spawn(worker, 'bob'),]) tasks size : 1 tasks size : 2 tasks size : 3 Worker steve got task 1 Worker john got task 2 Worker bob got task 3 tasks size : 1 tasks size : 2 tasks size : 3 Worker steve got task 4 Worker john got task 5 Worker bob got task 6 tasks size : 1 tasks size : 2 tasks size : 3 Assigned all work in iteration 1 Worker steve got task 7 Worker john got task 8 Worker bob got task 9 tasks size : 1 tasks size : 2 tasks size : 3 Worker steve got task 10 Worker john got task 11 Worker bob got task 12 tasks size : 1 tasks size : 2 tasks size : 3 Worker steve got task 13 Worker john got task 14 Worker bob got task 15 tasks size : 1 tasks size : 2 tasks size : 3 Worker steve got task 16 Worker john got task 17 Worker bob got task 18 tasks size : 1 Assigned all work in iteration 2 Worker steve got task 19 Quitting time! Quitting time! Quitting time! [&lt;Greenlet at 0x1055f1410&gt;, &lt;Greenlet at 0x1055e3c30&gt;, &lt;Greenlet at 0x1055e3e10&gt;, &lt;Greenlet at 0x1055e3550&gt;]queue timeout能够保证在get不到的10秒时间里，正常退出 组123456789101112131415161718import geventfrom gevent.pool import Groupdef talk(msg): for i in xrange(3): print(msg) g1 = gevent.spawn(talk, 'bar')g2 = gevent.spawn(talk, 'foo')g3 = gevent.spawn(talk, 'fizz')group = Group()group.add(g1)group.add(g2)print u"g1, g2 开始join"group.join()group.add(g3)print u"g3 开始join"group.join() g1, g2 开始join bar bar bar foo foo foo fizz fizz fizz g3 开始join True池12345678910111213141516171819from gevent.pool import Poolclass SocketPool(object): def __init__(self): self.pool = Pool(1000) self.pool.start() def listen(self, socket): while True: socket.recv() def add_handler(self, socket): if self.pool.full(): raise Exception("At maximum pool size") else: self.pool.spawn(self.listen, socket) def shutdown(self): self.pool.kill() 123456789import geventfrom gevent.pool import Poolpool = Pool(2)def hello_from(n): print('Size of pool %s' % len(pool)) pool.map(hello_from, xrange(3)) Size of pool 2 Size of pool 2 Size of pool 1 [None, None, None]1234567891011121314import timeimport geventfrom gevent.threadpool import ThreadPool def my_func(text, num): print text, num pool = ThreadPool(2)start = time.time()for i in xrange(10): pool.spawn(my_func, "Hello", i)pool.join()delay = time.time() - startprint('Take %.3f seconds' % delay) HelloHello 01 Hello Hello2 3Hello Hello4 5Hello Take 0.013 seconds Hello 6 7Hello 8Hello 912345678910111213import timeimport gevent.poolpool=gevent.pool.Pool(2)def func(n): print('Size of pool %s' % len(pool)) print nfor i in range(10): pool.add(gevent.spawn(func,i))pool.join() Size of pool 2 0 Size of pool 2 1 Size of pool 2 2 Size of pool 1 3 Size of pool 1 4 Size of pool 0 5 Size of pool 1 6 Size of pool 1 7 Size of pool 0 8 Size of pool 1 9 True12]]></content>
  </entry>
  <entry>
    <title><![CDATA[Python中的多进程和多进程]]></title>
    <url>%2F2020%2F08%2F16%2FPython%E4%B8%AD%E7%9A%84%E5%A4%9A%E8%BF%9B%E7%A8%8B%E5%92%8C%E5%A4%9A%E8%BF%9B%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[多线程和多进程多线程（Threading）多线程的理念是分批的想法。如果我有一大批数据，使用一个cpu，可能需要10秒，但是如果我把数据分成两批，同时使用2个CPU进行处理，时间就可以减少到5秒。 添加线程12345678910111213141516171819202122import threadingdef thread_job(): print('This is added Thread, number is %s' % threading.current_thread())def main(): print(threading.active_count()) print(threading.enumerate()) print(threading.current_thread()) add_thread = threading.Thread(target=thread_job) add_thread.start() print(threading.active_count()) print(threading.enumerate()) print(threading.current_thread())if __name__ == '__main__': main() 5This is added Thread, number is &lt;Thread(Thread-7, started 123145512390656)&gt; [&lt;_MainThread(MainThread, started 4485215680)&gt;, &lt;Thread(Thread-2, started daemon 123145490296832)&gt;, &lt;Heartbeat(Thread-3, started daemon 123145495552000)&gt;, &lt;HistorySavingThread(IPythonHistorySavingThread, started 123145501880320)&gt;, &lt;ParentPollerUnix(Thread-1, started daemon 123145507135488)&gt;] &lt;_MainThread(MainThread, started 4485215680)&gt; 5 [&lt;_MainThread(MainThread, started 4485215680)&gt;, &lt;Thread(Thread-2, started daemon 123145490296832)&gt;, &lt;Heartbeat(Thread-3, started daemon 123145495552000)&gt;, &lt;HistorySavingThread(IPythonHistorySavingThread, started 123145501880320)&gt;, &lt;ParentPollerUnix(Thread-1, started daemon 123145507135488)&gt;] &lt;_MainThread(MainThread, started 4485215680)&gt;join1234567891011121314151617181920import threadingimport timedef thread_job(): print('T1 Start \n') for i in range(10): time.sleep(0.1) print('T1 Finish \n')def main(): add_thread = threading.Thread(target=thread_job, name='T1') add_thread.start() print('all done \n')if __name__ == '__main__': main() all done T1 Start T1 Finish 12345678910111213141516171819202122import threadingimport timedef thread_job(): print('T1 Start \n') for i in range(10): time.sleep(0.1) print('T1 Finish \n')def main(): add_thread = threading.Thread(target=thread_job, name='T1') add_thread.start() add_thread.join() print('all done \n')if __name__ == '__main__': main() T1 Start T1 Finish all done ​ 123456789101112131415161718192021222324252627282930import threadingimport timedef thread_job(): print('T1 Start \n') for i in range(10): time.sleep(0.1) print('T1 Finish \n')def thread_job2(): print('T2 Start \n') print('T2 Finish \n')def main(): add_thread = threading.Thread(target=thread_job, name='T1') add_thread2 = threading.Thread(target=thread_job2, name='T2') add_thread.start() add_thread.join() add_thread2.start() add_thread2.join() print('all done \n')if __name__ == '__main__': main() T1 Start T1 Finish T2 Start all done T2 Finish 123456789101112131415161718192021222324252627282930import threadingimport timedef thread_job(): print('T1 Start \n') for i in range(10): time.sleep(0.1) print('T1 Finish \n')def thread_job2(): print('T2 Start \n') print('T2 Finish \n')def main(): add_thread = threading.Thread(target=thread_job, name='T1') add_thread2 = threading.Thread(target=thread_job2, name='T2') add_thread.start() add_thread2.start() add_thread.join() add_thread2.join() print('all done \n')if __name__ == '__main__': main() T1 Start T2 Start T2 Finish T1 Finish all done Queue123456789101112131415161718192021222324252627282930import timeimport threadingfrom queue import Queuedef job(l, q): for i in range(len(l)): l[i] = l[i]**2 q.put(l)def multithreading(data): q = Queue() threads = [] for i in range(4): t = threading.Thread(target=job, args=(data[i], q)) t.start() threads.append(t) for thread in threads: thread.join() result = list() for _ in range(4): result.append(q.get()) print(result)if __name__ == '__main__': data = [[1, 1, 1], [2, 2, 2], [3, 3, 3], [4, 4, 4]] multithreading(data) [[1, 1, 1], [4, 4, 4], [9, 9, 9], [16, 16, 16]]GIL不一定有效率python中的多线程，其实并不并行的，在多线程运行的时候，GIL会把其他的线程锁住，只让一个线程在同一时间运行一个东西。本质上，python的多线程其实是在多个线程之间切换的。而不是并行的。 123456789101112131415161718192021222324252627282930313233343536373839import threadingfrom queue import Queueimport copyimport timedef job(l, q): res = sum(l) q.put(res)def multithreading(l): q = Queue() threads = [] for i in range(4): t = threading.Thread( target=job, args=(copy.copy(l), q), name='T%i' % i) t.start() threads.append(t) [t.join() for t in threads] total = 0 for _ in range(4): total += q.get() print(total)def normal(l): total = sum(l) print(total)if __name__ == '__main__': l = list(range(1000000)) s_t = time.time() normal(l * 4) print('normal: ', time.time() - s_t) s_t = time.time() multithreading(l) print('multithreading: ', time.time() - s_t) 1999998000000 normal: 0.09590601921081543 1999998000000 multithreading: 0.07823991775512695线程锁 Lock全局变量线程不安全 ！！！ 12345678910111213141516171819202122232425262728293031import threadingimport timeclass A(object): def __init__(self): self.value = 1def thread1(a): a.value = a.value + 1 print('T1, stage 1, a : &#123;0&#125; \n'.format(a.value)) time.sleep(0.1) print('T1, stage 2, a : &#123;0&#125; \n'.format(a.value))def thread2(a): a.value = a.value + 1 print('T2, stage 1, a : &#123;0&#125; \n'.format(a.value)) time.sleep(0.1) print('T2, stage 2, a : &#123;0&#125; \n'.format(a.value))if __name__ == '__main__': a = A() Thread1 = threading.Thread(target=thread1, name='T1', args=(a, )) Thread2 = threading.Thread(target=thread2, name='T2', args=(a, )) Thread1.start() Thread2.start() Thread1.join() Thread2.join() T1, stage 1, a : 2 T2, stage 1, a : 3 T1, stage 2, a : 3 T2, stage 2, a : 3 1234567891011121314151617181920212223242526272829import threadingimport timedef job1(): global A for i in range(10): A += 1 print('job1 &#123;0&#125; \n'.format(A)) time.sleep(0.1)def job2(): global A for i in range(10): A += 10 print('job2 &#123;0&#125; \n'.format(A)) time.sleep(0.1)if __name__ == '__main__': lock = threading.Lock() A = 0 t1 = threading.Thread(target=job1) t2 = threading.Thread(target=job2) t1.start() t2.start() t1.join() t2.join() job1 1 job2 11 job1 12 job2 22 job1 23 job2 33 job1 34 job2 44 job1 45 job2 55 job1 56 job2 66 job1 67 job2 77 job2 87 job1 88 job2 98 job1 99 job2 109 job1 110 123456789101112131415161718192021222324252627282930313233import threadingimport timedef job1(): global A, lock lock.acquire() for i in range(10): A += 1 print('job1 &#123;0&#125; \n'.format(A)) time.sleep(0.1) lock.release()def job2(): global A, lock lock.acquire() for i in range(10): A += 10 print('job2 &#123;0&#125; \n'.format(A)) time.sleep(0.1) lock.release()if __name__ == '__main__': lock = threading.Lock() A = 0 t1 = threading.Thread(target=job1) t2 = threading.Thread(target=job2) t1.start() t2.start() t1.join() t2.join() job1 1 job1 2 job1 3 job1 4 job1 5 job1 6 job1 7 job1 8 job1 9 job1 10 job2 20 job2 30 job2 40 job2 50 job2 60 job2 70 job2 80 job2 90 job2 100 job2 110 多进程创建进程123456789101112131415import multiprocessing as mpimport threading as tddef job(a, b): print(a, b)t1 = td.Thread(target=job, args=(1, 2))p1 = mp.Process(target=job, args=(1, 2))t1.start()p1.start()t1.join()p1.join() 1 2 1 21234567891011import multiprocessing as mpdef job(a, b): print(a, b)if __name__ == '__main__': p1 = mp.Process(target=job, args=(1, 2)) p1.start() p1.join() 1 2Queue123456789101112131415161718192021import multiprocessing as mpdef job(q): res = 0 for i in range(1000): res += i + i**2 + i**3 q.put(res) #queueif __name__ == '__main__': q = mp.Queue() p1 = mp.Process(target=job, args=(q, )) p2 = mp.Process(target=job, args=(q, )) p1.start() p2.start() p1.join() p2.join() res1 = q.get() res2 = q.get() print(res1 + res2) 499667166000效率对比1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556import multiprocessing as mpimport threading as tdimport timedef normal(): res = 0 for _ in range(2): for i in range(1000000): res += i + i**2 + i**3 print('normal:', res)def job(q): res = 0 for i in range(1000000): res += i + i**2 + i**3 q.put(res) # queuedef multithread(): q = mp.Queue() # thread可放入process同样的queue中 t1 = td.Thread(target=job, args=(q, )) t2 = td.Thread(target=job, args=(q, )) t1.start() t2.start() t1.join() t2.join() res1 = q.get() res2 = q.get() print('multithread:', res1 + res2)def multicore(): q = mp.Queue() p1 = mp.Process(target=job, args=(q, )) p2 = mp.Process(target=job, args=(q, )) p1.start() p2.start() p1.join() p2.join() res1 = q.get() res2 = q.get() print('multicore:', res1 + res2)if __name__ == '__main__': st = time.time() normal() st1 = time.time() print('normal time:', st1 - st) multithread() st2 = time.time() print('multithread time:', st2 - st1) multicore() print('multicore time:', time.time() - st2) normal: 499999666667166666000000 normal time: 1.3003261089324951 multithread: 499999666667166666000000 multithread time: 1.2527809143066406 multicore: 499999666667166666000000 multicore time: 0.645003080368042进程池123456789101112131415161718192021import multiprocessing as mpdef job(x): return x * xdef multicore(): pool = mp.Pool(processes=3) result = pool.map(job, range(10)) print(result) res = pool.apply_async(job, (2, )) print(res.get()) multi_res = [pool.apply_async(job, (i, )) for i in range(10)] print([res.get() for res in multi_res])if __name__ == '__main__': multicore() [0, 1, 4, 9, 16, 25, 36, 49, 64, 81] 4 [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]共享内存多进程之间是独立的，在不做任何处理的情况下，是无法共同访问和修改同一个变量的。 12345import multiprocessing as mp# 设置共享内存，多核和多进程可以同时访问value = mp.Value('d',0)array = mp.Array('i',[1,2,3]) 进程锁123456789101112131415161718192021222324252627import multiprocessing as mpimport timedef job1(A): for i in range(10): A.value += 1 print('job1: &#123;0&#125; \n'.format(A.value)) time.sleep(0.1)def job2(A): for i in range(10): A.value += 1 print('job2: &#123;0&#125; \n'.format(A.value)) time.sleep(0.1)if __name__ == '__main__': A = mp.Value('i', 0) print(A) p1 = mp.Process(target=job1, args=(A, )) p2 = mp.Process(target=job2, args=(A, )) p1.start() p2.start() p1.join() p2.join() &lt;Synchronized wrapper for c_int(0)&gt; job1: 1 job2: 2 job1: 3 job2: 4 job1: 5 job2: 6 job1: 7 job2: 8 job1: 9 job2: 10 job1: 11 job2: 12 job1: 13 job2: 13 job1: 14 job2: 15 job1: 16 job2: 17 job1: 18 job2: 18 ​ 1234567891011121314151617181920212223242526272829303132import multiprocessing as mpimport timedef job1(A, lock): lock.acquire() for i in range(10): A.value += 1 print('job1: &#123;0&#125; \n'.format(A.value)) time.sleep(0.1) lock.release()def job2(A, lock): lock.acquire() for i in range(10): A.value += 1 print('job2: &#123;0&#125; \n'.format(A.value)) time.sleep(0.1) lock.release()if __name__ == '__main__': lock = mp.Lock() A = mp.Value('i', 0) print(A) p1 = mp.Process(target=job1, args=(A, lock)) p2 = mp.Process(target=job2, args=(A, lock)) p1.start() p2.start() p1.join() p2.join() &lt;Synchronized wrapper for c_int(0)&gt; job1: 1 job1: 2 job1: 3 job1: 4 job1: 5 job1: 6 job1: 7 job1: 8 job1: 9 job1: 10 job2: 11 job2: 12 job2: 13 job2: 14 job2: 15 job2: 16 job2: 17 job2: 18 job2: 19 job2: 20]]></content>
  </entry>
  <entry>
    <title><![CDATA[Java中的线程安全]]></title>
    <url>%2F2020%2F08%2F16%2FJava%E4%B8%AD%E7%9A%84%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%2F</url>
    <content type="text"><![CDATA[线程安全什么是线程安全？ 通俗点说，就是线程访问时不产生资源冲突。 “一个类可以被多个线程安全调用就是线程安全的” ​ ————《Java编程并发实践》 变量的线程安全静态变量：线程非安全用public修饰的static成员变量和成员方法本质是变量和全局方法，当声明它的类的对象时，不生成static变量的副本，而是类的所有实例共享同一个static变量。 静态变量也称为类变量，属于类对象所有，位于方法区，为所有对象共享，共享一份内存，一旦值被修改，则其他对象均对修改可见，故线程非安全。 123456789101112131415161718192021/** * Created by Mark on 2018/1/5. */public class HelloInstance implements Runnable &#123; private static int num; @Override public void run() &#123; num = 3; System.out.println("当前线程是：" + Thread.currentThread().getName() + ",num的值是：" + num); num = 5; System.out.println("当前线程是：" + Thread.currentThread().getName() + ",num的值是：" + num * 2); &#125; public static void main(String[] args) &#123; for (int i = 1; i &lt;= 50; i++) &#123; new Thread(new HelloInstance(), "Thread=" + 1).start(); &#125; &#125;&#125; 如果线程安全，应该只会有3和10。但是出现了5的情况。说明线程非安全。 result: 123当前线程是：Thread=1,num的值是：3当前线程是：Thread=1,num的值是：10当前线程是：Thread=1,num的值是：5 实例变量：单例时线程非安全，非单例时线程安全实例变量是实例对象私有的，系统只存在一个实例对象，则在多线程环境下，如果值改变后，则其它对象均可见，故线程非安全； 如果每个线程都在不同的实例对象中执行，则对象与对象间的修改互不影响，故线程安全。 单例12345678910111213141516171819202122/** * Created by Mark on 2018/1/5. */public class HelloInstance implements Runnable &#123; private int num; @Override public void run() &#123; num = 3; System.out.println("当前线程是：" + Thread.currentThread().getName() + ",num的值是：" + num); num = 5; System.out.println("当前线程是：" + Thread.currentThread().getName() + ",num的值是：" + num * 2); &#125; public static void main(String[] args) &#123; HelloInstance helloInstance = new HelloInstance(); for (int i = 1; i &lt;= 50; i++) &#123; new Thread(helloInstance, "Thread=" + 1).start(); &#125; &#125;&#125; result：非安全 123当前线程是：Thread=1,num的值是：10当前线程是：Thread=1,num的值是：5当前线程是：Thread=1,num的值是：3 多例123456789101112131415161718192021/** * Created by Mark on 2018/1/5. */public class HelloInstance implements Runnable &#123; private int num; @Override public void run() &#123; num = 3; System.out.println("当前线程是：" + Thread.currentThread().getName() + ",num的值是：" + num); num = 5; System.out.println("当前线程是：" + Thread.currentThread().getName() + ",num的值是：" + num * 2); &#125; public static void main(String[] args) &#123; for (int i = 1; i &lt;= 50; i++) &#123; new Thread(new HelloInstance(), "Thread=" + 1).start(); &#125; &#125;&#125; result：线程安全 12345当前线程是：Thread=1,num的值是：3当前线程是：Thread=1,num的值是：3当前线程是：Thread=1,num的值是：10当前线程是：Thread=1,num的值是：10当前线程是：Thread=1,num的值是：3 局部变量：线程安全每个线程执行时都会把局部变量放在各自的帧栈的内存空间中，线程间不共享，故不存在线程安全问题。 123456789101112131415161718192021/** * Created by Mark on 2018/1/5. */public class HelloInstance implements Runnable &#123; @Override public void run() &#123; int num = 3; System.out.println("当前线程是：" + Thread.currentThread().getName() + ",num的值是：" + num); num = 5; System.out.println("当前线程是：" + Thread.currentThread().getName() + ",num的值是：" + num * 2); &#125; public static void main(String[] args) &#123; HelloInstance helloInstance = new HelloInstance(); for (int i = 1; i &lt;= 50; i++) &#123; new Thread(helloInstance, "Thread=" + 1).start(); &#125; &#125;&#125; result: 1234当前线程是：Thread=1,num的值是：3当前线程是：Thread=1,num的值是：10当前线程是：Thread=1,num的值是：3当前线程是：Thread=1,num的值是：10 静态方法的线程安全性静态方法中如果没有使用静态变量，则没有线程安全的问题； Java是线程安全的，即对任何方法（包括静态方法）都可以不考虑线程冲突，但有一个前提，就是不能存在全局变量。如果存在全局变量，则需要使用同步机制。 Java在执行静态方法时，如果静态方法所在的类里面没有静态的变量，那么线程访问就是安全的。 Java在执行静态方法时，如果使用静态变量，同时类的函数设计时使用到了静态数据，最好在调用函数时使用synchronized关键字，否则会导致数据的不一致行。 加静态全局的变量，在多线程访问下定会出现数据的不一致行，最好使用synchronized关键字，确保数据的一致性，典型的代表就是单例模式。 静态方法不使用静态变量线程安全 StaticThread： 1234567891011121314/** * Created by Mark on 2018/1/6. */public class StaticThread implements Runnable &#123; @Override public void run() &#123; StaticAction.print(); &#125; public static void main(String[] args) &#123; for (int i = 0; i &lt; 100; i++) &#123; new Thread(new StaticThread()).start(); &#125; &#125;&#125; StaticAction： 123456789101112131415161718/** * Created by Mark on 2018/1/6. */public class StaticAction &#123; public static int i = 0; public static void print() &#123; int sum = 0; for (int i = 0; i &lt; 10; i++) &#123; System.out.print("step " + i + " is running."); sum += i; &#125; if (sum != 45) &#123; System.out.println("Thread error!"); System.exit(0); &#125; System.out.println("sum is " + sum); &#125;&#125; 结果：线程安全 1没有出现：Thread error! 静态方法使用静态变量有可能会产生线程不安全 线程不安全代码： StaticThread： 1234567891011121314151617181920/** * Created by Mark on 2018/1/6. */public class StaticThread implements Runnable &#123; @Override public void run() &#123; StaticAction.incValue(); &#125; public static void main(String[] args) &#123; for (int i = 0; i &lt; 100; i++) &#123; new Thread(new StaticThread()).start(); &#125; try &#123; Thread.sleep(1000); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; System.out.println(StaticAction.i); &#125;&#125; StaticAction: 12345678910111213141516/** * Created by Mark on 2018/1/6. */public class StaticAction &#123; public static int i = 0; public static void incValue() &#123; int temp = StaticAction.i; try &#123; Thread.sleep(1); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; temp++; StaticAction.i = temp; &#125;&#125; 第一次运行结果： 116 第二次运行结果： 111 线程安全代码（synchronized）： 12345678910111213141516/** * Created by Mark on 2018/1/6. */public class StaticAction &#123; public static int i = 0; public static void incValue() &#123; int temp = StaticAction.i; try &#123; Thread.sleep(1); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; temp++; StaticAction.i = temp; &#125;&#125; 结果： 1100 经验经验一Hello_Main: 123456789101112/** * Created by Mark on 2018/1/4. */public class Hello_Main &#123; public static void main(String[] args) throws IllegalAccessException, InstantiationException, ClassNotFoundException &#123; Employee employee = new Employee(); employee.reload(); for (int i = 1; i &lt;= 50; i++) &#123; new Thread(new HelloInstance(), "Thread=" + 1).start(); &#125; &#125;&#125; HelloInstance: 1234567891011121314/** * Created by Mark on 2018/1/5. */public class HelloInstance implements Runnable &#123; @Override public void run() &#123; ExtraProcessor GKprocessor = Employee.configMap.get("GKprocessor"); int num = 3; System.out.println("当前线程是：" + Thread.currentThread().getName() + ",num的值是：" + String.valueOf(GKprocessor.getProcessor(num))); num = 5; System.out.println("当前线程是：" + Thread.currentThread().getName() + ",num的值是：" + String.valueOf(GKprocessor.getProcessor(num))); &#125;&#125; Employee: 12345678910import java.util.HashMap;import java.util.Map;public class Employee&#123; public static Map&lt;String, ExtraProcessor&gt; configMap = new HashMap&lt;&gt;(); public void reload() throws ClassNotFoundException, IllegalAccessException, InstantiationException &#123; configMap.put("GKprocessor", (ExtraProcessor) Class.forName("GKprocessor").newInstance()); &#125;&#125; ExtraProcessor: 123456/** * Created by Mark on 2018/1/6. */public interface ExtraProcessor &#123; int getProcessor(int a);&#125; GKprocessor: 123456789/** * Created by Mark on 2018/1/6. */public class GKprocessor implements ExtraProcessor&#123; @Override public int getProcessor(int a) &#123; return a; &#125;&#125;]]></content>
  </entry>
  <entry>
    <title><![CDATA[负采样]]></title>
    <url>%2F2020%2F08%2F16%2FNegative%20Sampling%2F</url>
    <content type="text"><![CDATA[负采样问题对word2vec中的三层神经网络而言，以CBOW举例：上下文来预测中间的词 $x_i$，也就是说，对于第三层的V个词来讲，只有$x_i$的softmax的概率是1，其他的(V-1)个词的概率应该都是0，那么最后一层需要更新的权重有多少呢？ 300*N个。 这仅仅是训练一个样本中的一个单词的最后一层，就需要更新的权重就这么多，如果是全量的样本加进来，训练的资源消耗是非常大的。 负采样就是为了解决这个问题的。 负采样简介负采样每次训练，只更新除了”positive”之外，只更新”negative”最多20个词的权重。 对于小规模数据集，选择5-20个negative words会比较好，对于大规模数据集可以仅选择2-5个negative words。 这样权重就编程了300*21就好训练多了。 那么negative中的20个是怎么获得的呢？ 负采样频率单词在文本中出现的频次：$p(w_i)$ 该负样本被选中的概率为:$$P(w_i) = \frac{f(w_i)^{\frac{3}{4}}}{\sum_{j=0}^{n}(f(w_i)^{\frac{3}{4}})}$$ 工程实现 已经计算出每个单词的$p(w_i)$,乘以1亿，得出每个单词在1亿的数组中出现的次数。由此得到一个1亿的数组，包含了所有的单词(每个单词N个)。概率越大的词，在1亿数组中出现的次数越多，概率越小的词，在1亿数组中出现的次数越小。 在0-1亿中，生成一个随机数，做为1亿的数组的索引，来选择单词。也就是负样本的词。一个单词的负采样概率越大，在表中出现的次数就越多，被选中的概率就越大。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hackintosh踩坑盘点]]></title>
    <url>%2F2020%2F08%2F16%2FHackintosh%E8%B8%A9%E5%9D%91%E7%9B%98%E7%82%B9%2F</url>
    <content type="text"><![CDATA[Hackintosh 那些踩过的坑常识kext 驱动程序 详细信息 备注 FakeSMC.kext 安装hackintosh的核心程序，没有它就没法在你的电脑上面运行macOS 必备 Lilu.kext 内核扩展程序，离开它，下面的几个程序都无法正常运行 必备 WhateverGreen.kext 显卡综合修复，整合了核显、AMD、NVIDIA的综合修复，包括 （单卡启动黑屏，唤醒黑屏 等等）(依赖于Lilu) 必备 RealtekRTL8xxx.kext Realtek 8xxx网卡驱动程序 必选 NvidiaGraphicsFixup.kext 修复N卡的卡顿问题 VoodooPS2Controller.kext Voodoo键盘/鼠标驱动程序 VoodooHDA.kext 万能声卡驱动 具体问题]]></content>
  </entry>
  <entry>
    <title><![CDATA[Huffman Tree]]></title>
    <url>%2F2020%2F08%2F16%2FHuffman%20Tree%2F</url>
    <content type="text"><![CDATA[Huffman TreeHuffman Tree的构建定义哈夫曼树给定n个权值作为n个子节点，构建一颗二叉树，若该树的带权路径长度最小，则此树为哈夫曼树。 结点的带权路径长度对于一个结点： 结点的带权路径长度 = 该结点的路径长度*该结点权值 = $w_k * l_k$ 树的带权路径长度n个结点带权路径之和 树的带权路径长度 = $\sum_{k=1}^{n}w_kl_k$ 构建构建方法： 从下往上构建，倒数第一层，选取两个权值最小的值，构建左右树，小的在左，大的在右，构成新的二叉树，该二叉树结点值为两个权值之和。同一层，再选除了这两个之外最小的，构建二叉树。 倒数第二层，选两个新的最小的二叉树结点值，构建新的二叉树，重复。 直到构建到顶部。 例子1 例子2 Word2vec中的哈夫曼树假设word2vec的最后面构建了一个Huffman Tree，这个Tree点叶结点是词汇表的没个词，为N个。构建的方法为按照词等频次进行构建。 那么对于每一个词，从根节点算上去，向左为1，向右为0。那么每一个词，其实都是有唯一的一个编码表示，例如：的=’010011’。 假设word2vec的最后一层，有K个神经元。 K个神经中的每一个，与每一个内接点(分叉口)相连。有参数$w$来影响这个结点往左走还是往右走。也就是对于每一内接点，向左为$P_i$，向右为$1 - P_i$。 当CBOW或者SKIP-GRAM，我们根据上下文，来预测 ‘的’ 的时候，可以根据K个神经元，连接内接点的目前的参数，算出’010011’的概率为：$w(n) = \prod_0^{i}Pi \ or\ (1-Pi)$ 那么损失函数就是：$$1 - w(n)$$也就可以通过SGD来训练word2vec和huffman Tree中的参数了。 如果不用Huffman Tree，计算复杂度为O(N)，但是用了Huffman Tree之后，计算复杂度为O(long2(N))]]></content>
  </entry>
  <entry>
    <title><![CDATA[Java中的反射]]></title>
    <url>%2F2020%2F08%2F16%2FJava%E4%B8%AD%E7%9A%84%E5%8F%8D%E5%B0%84%2F</url>
    <content type="text"><![CDATA[反射机制理解Class类 Class是一个类。 Class是用来描述类的类。 Class是一个类，封装了当前对象所对应的类的信息。 一个类中有属性，方法，构造器等，比如说有一个Person类，现在需要一个类，用来描述类，这就是Class，它应该有Person的类名，属性，方法，构造器等。 Class类：是一个对象照镜子的结果，对象可以看到自己有哪些属性，方法，构造器，实现了哪些接口等等。 对于每个类而言，JRE 都为其保留一个不变的 Class 类型的对象。一个 Class 对象包含了特定某个类的有关信息。 Class 对象：只能由系统建立对象，一个类（而不是一个对象）在 JVM 中只会有一个Class实例。 获取Class对象的三种方式 通过类名获取 类名.class 通过对象获取 对象名.getClass() 通过全类名获取 Class.forName(全类名) Person类： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public class Person &#123; private String name; private int age; //新增一个私有方法 private void privateMthod() &#123; &#125; public Person() &#123; System.out.println("无参构造器"); &#125; public Person(String name, int age) &#123; System.out.println("有参构造器"); this.name = name; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; /** * @param age 类型用Integer，不用int */ public void setName(String name, int age) &#123; System.out.println("name: " + name); System.out.println("age:" + age); &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; @Override public String toString() &#123; return "Person&#123;" + "name='" + name + '\'' + ", age=" + age + '&#125;'; &#125;&#125; ReflectionTest： 123456789101112131415161718192021222324252627public class ReflectionTest &#123; public static void main(String[] args) throws ClassNotFoundException &#123; Class clazz = null; //1.通过类名 clazz = Person.class; //2.通过对象名 //这种方式是用在传进来一个对象，却不知道对象类型的时候使用 Person person = new Person(); clazz = person.getClass(); //上面这个例子的意义不大，因为已经知道person类型是Person类，再这样写就没有必要了 //如果传进来是一个Object类，这种做法就是应该的 Object obj = new Person(); clazz = obj.getClass(); //3.通过全类名(会抛出异常) //一般框架开发中这种用的比较多，因为配置文件中一般配的都是全类名，通过这种方式可以得到Class实例 String className="Person"; clazz = Class.forName(className); //字符串的例子 clazz = String.class; clazz = "javaTest".getClass(); clazz = Class.forName("java.lang.String"); System.out.println(); &#125;&#125; Class类的常用方法ReflectionTest: 12345678910111213141516171819202122232425262728293031323334353637383940414243import java.lang.reflect.Constructor;import java.lang.reflect.Field;import java.lang.reflect.Method;public class ReflectionTest &#123; public static void main(String[] args) throws ClassNotFoundException &#123; Class clazz = null; Object obj = new Person(); clazz = obj.getClass(); //返回此Class对象所表示的实体（类、接口、数组类、基本类型或void）名称 System.out.println("------------&gt;getName&lt;------------"); System.out.println("获取此Class对象所表示的实体（类、接口、数组类、基本类型或void）名称："); System.out.println(clazz.getName()); // System.out.println("------------&gt;getConstructors&lt;------------"); System.out.println("获取Person的构造方法："); Constructor&lt;Person&gt;[] constructors = (Constructor&lt;Person&gt;[]) Class.forName("Person").getConstructors(); for (Constructor&lt;Person&gt; constructor : constructors) &#123; System.out.println(constructor); &#125; //获取Person的方法名 System.out.println("------------&gt;getMethods&lt;------------"); System.out.println("获取Person的方法名："); Method[] methods = clazz.getMethods(); for (Method method : methods) &#123; System.out.println(method.getName()); &#125; //获取Person的属性 System.out.println("------------&gt;getDeclaredFields&lt;------------"); System.out.println("获取Person的属性："); Field[] field = clazz.getDeclaredFields(); for (Field f : field) &#123; System.out.println(f.getName()); &#125; System.out.println(); &#125;&#125; result: 123456789101112131415161718192021222324252627------------&gt;getName&lt;------------获取此Class对象所表示的实体（类、接口、数组类、基本类型或void）名称：Person------------&gt;getConstructors&lt;------------获取Person的构造方法：public Person(java.lang.String,int)public Person()------------&gt;getMethods&lt;------------获取Person的方法名：toStringgetNamesetNamesetNamegetAgesetAgewaitwaitwaitequalshashCodegetClassnotifynotifyAll------------&gt;getDeclaredFields&lt;------------获取Person的属性：nameage newInstance（）方法 通过反射创建实例的时候，实际调用的是类的无参数的构造器。 所以在我们在定义一个类的时候，定义一个有参数的构造器，作用是对属性进行初始化，还要写一个无参数的构造器，作用就是反射时候用。 一般地、一个类若声明一个带参的构造器，同时要声明一个无参数的构造器 ReflectionTest: 12345678910public class ReflectionTest &#123; public static void main(String[] args) throws ClassNotFoundException, IllegalAccessException, InstantiationException &#123; Class clazz = null; clazz = Class.forName("Person"); Object object = clazz.newInstance(); System.out.println(object); &#125;&#125; result: 12无参构造器Person&#123;name=&apos;null&apos;, age=0&#125; ClassLoader类装载器是用来把类(class)装载进 JVM 的。 JVM在运行时会产生3个类加载器组成的初始化加载器层次结构 ，如下图所示： 1234567891011121314151617181920212223242526272829public class ReflectionTest &#123; public static void main(String[] args) throws ClassNotFoundException, IllegalAccessException, InstantiationException &#123; //1. 获取一个系统的类加载器(可以获取，当前这个类PeflectTest就是它加载的) ClassLoader classLoader = ClassLoader.getSystemClassLoader(); System.out.println(classLoader); //2. 获取系统类加载器的父类加载器（扩展类加载器，可以获取）. classLoader = classLoader.getParent(); System.out.println(classLoader); //3. 获取扩展类加载器的父类加载器（引导类加载器，不可获取）. classLoader = classLoader.getParent(); System.out.println(classLoader); //4. 测试当前类由哪个类加载器进行加载（系统类加载器）: classLoader = Class.forName("ReflectionTest") .getClassLoader(); System.out.println(classLoader); //5. 测试 JDK 提供的 Object 类由哪个类加载器负责加载（引导类） classLoader = Class.forName("java.lang.Object") .getClassLoader(); System.out.println(classLoader); &#125;&#125; result: 12345jdk.internal.loader.ClassLoaders$AppClassLoader@4f8e5cdejdk.internal.loader.ClassLoaders$PlatformClassLoader@16f65612nulljdk.internal.loader.ClassLoaders$AppClassLoader@4f8e5cdenull 反射反射 是Java被视为动态语言的关键，反射机制允许程序在执行期借助于Reflection API取得任何类的內部信息，并能直接操作任意对象的内部属性及方法。 Java反射机制主要提供了以下功能： 在运行时构造任意一个类的对象 在运行时获取任意一个类所具有的成员变量和方法 在运行时调用任意一个对象的方法（属性） 生成动态代理 获取方法getMethods 不能获取private方法 会获取从父类继承来的所有方法 getDeclaredMethods 可以获取private方法 不能获取从父类继承来的方法，只能获取本类 getDeclaredMethod 获取指定名称的方法 invoke 执行某个对象的方法，改变对象的属性值 ReflectionTest： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import java.lang.reflect.Method;public class ReflectionTest &#123; public static void main(String args[]) throws Exception&#123; Class clazz = Class.forName("Person"); //1.获取方法 // 1.1 获取取clazz对应类中的所有方法--方法数组（一） // 不能获取private方法,且获取从父类继承来的所有方法 System.out.println("--------------getMethods--------------"); Method[] methods = clazz.getMethods(); for(Method method:methods)&#123; System.out.println(" "+method.getName()); &#125; System.out.println(); // 1.2.获取所有方法，包括私有方法 --方法数组（二） // 所有声明的方法，都可以获取到，且只获取当前类的方法 System.out.println("--------------getDeclaredMethods--------------"); methods = clazz.getDeclaredMethods(); for(Method method:methods)&#123; System.out.println(" "+method.getName()); &#125; System.out.println(); // 1.3.获取指定的方法 // 需要参数名称和参数列表，无参则不需要写 // 对于方法public void setName(String name) &#123; &#125; Method method = clazz.getDeclaredMethod("setName", String.class); System.out.println(method); // 而对于方法public void setAge(int age) &#123; &#125; method = clazz.getDeclaredMethod("setAge", Integer.class); System.out.println(method); // 这样写是获取不到的，如果方法的参数类型是int型 // 如果方法用于反射，那么要么int类型写成Integer： public void setAge(Integer age) &#123; &#125; //2.执行方法 // invoke第一个参数表示执行哪个对象的方法，剩下的参数是执行方法时需要传入的参数 System.out.println("--------------invoke--------------"); Object obje = clazz.newInstance(); System.out.println("setAge : 2" ); method.invoke(obje,2); Method getAgeMethod = clazz.getDeclaredMethod("getAge"); int age = (int) getAgeMethod.invoke(obje); System.out.println("getAge : " + String.valueOf(age)); &#125;&#125; 控制台： 12345678910111213141516171819202122232425262728293031--------------getMethods-------------- toString getName setName setName setAge getAge wait wait wait equals hashCode getClass notify notifyAll--------------getDeclaredMethods-------------- toString getName setName setName privateMthod setAge getAgepublic void Person.setName(java.lang.String)public void Person.setAge(java.lang.Integer)--------------invoke--------------无参构造器setAge : 2getAge : 2 方法反射调用工具这种反射实现的主要功能是可配置和低耦合。只需要类名和方法名，而不需要一个类对象就可以执行一个方法。如果我们把全类名和方法名放在一个配置文件中，就可以根据调用配置文件来执行方法 对象和方法名做参数main: 1234567891011121314151617181920212223242526import java.lang.reflect.Method;public class Test &#123; public static void main(String[] args) throws Exception &#123; Object obj = new Person(); invoke(obj, "test", "wang", 1); &#125; public static Object invoke(Object obj, String methodName, Object... args) throws Exception&#123; //1. 获取 Method 对象 // 因为getMethod的参数为Class列表类型，所以要把参数args转化为对应的Class类型。 Class [] parameterTypes = new Class[args.length]; for(int i = 0; i &lt; args.length; i++)&#123; parameterTypes[i] = args[i].getClass(); System.out.println(parameterTypes[i]); &#125; Method method = obj.getClass().getDeclaredMethod(methodName, parameterTypes); //如果使用getDeclaredMethod，就不能获取父类方法，如果使用getMethod，就不能获取私有方法 //2. 执行 Method 方法 //3. 返回方法的返回值 return method.invoke(obj, args); &#125;&#125; Person: 12345public Class Person&#123; public void test(String name,Integer age)&#123; System.out.println("调用成功"); &#125;&#125; 控制台： 1调用成功 类名和方法名做参数main: 123456789101112131415161718192021222324252627import java.lang.reflect.Method;public class Test &#123; public static void main(String[] args) throws Exception &#123; invoke("Person", "test", "wang", 1); &#125; public static Object invoke(String className, String methodName, Object... args) throws Exception&#123; Object obj = null; obj = Class.forName(className).newInstance(); //1. 获取 Method 对象 // 因为getMethod的参数为Class列表类型，所以要把参数args转化为对应的Class类型。 Class [] parameterTypes = new Class[args.length]; for(int i = 0; i &lt; args.length; i++)&#123; parameterTypes[i] = args[i].getClass(); System.out.println(parameterTypes[i]); &#125; Method method = obj.getClass().getDeclaredMethod(methodName, parameterTypes); //如果使用getDeclaredMethod，就不能获取父类方法，如果使用getMethod，就不能获取私有方法 //3. 返回方法的返回值 return method.invoke(obj, args); &#125;&#125; #####]]></content>
  </entry>
  <entry>
    <title><![CDATA[FastText]]></title>
    <url>%2F2020%2F08%2F16%2FFastText%2F</url>
    <content type="text"><![CDATA[FastText神经网络结构本质上还是一个三层的DNN，但是和word2vec哪里不同呢？ 一层层的看： 输入层输入层 字符一个序列的字符Bag of words。 例如：”我爱天安门”。 输入就是：”我”，”爱”，”天”，”安”，”门”，五个词的one-hot向量。 N-GramN-Gram特征向量的原因是为了弥补Bag of Words词语顺序信息丢失的弊端。一般取2-Gram。 例如：”我爱天安门” 输入：”我爱”，”爱天”，”天安”，”安门”，四个词对的one-hot向量。 加平均也就是说，fasttext中词表不再仅仅是，字符的个数，还包涵所有的2-Gram，并一起取one-hot。当然2-Gram有可能非常大，我们会在第二层做hash分桶。 得到字符和N-Gram的向量一直，加在一起取平均就可以了。 这样对于一个文本来讲，输入层的向量表示为：1*20000 隐藏层隐藏层还是作为映射层。权重矩阵的大小为：20000*300。 所以经过隐藏层得到的是一个：1*300的向量。 但是隐藏层这里fasttext为了加速运算做了一个trick。对于2-gram 相似的Hash，使用同一个wordvec。 分类层分类层还是使用了Huffman Tree。具体结构如下： 对于从隐藏层得到了任何一个向量：1*300。与Huffman Tree中的正确的(比方说”cat”)路径中经过的每一个内接点相乘，取softmax，并叠层。 那么对于任何一个内结点，都存在一个300*1的向量。 拿1结点来讲，在给定context的条件下，向右走的概率为(这里每一个内结点的softmax，其实就是左右两种情况)： 即：$$P(1_{right}|context) = softmax((1300)(300*1) + b1)$$那么向左走的概率为:$$1 - P(1_{right}|context)$$如果我们的目标是cat ，那么概率为:$$P(‘cat’|context) = (1 - P(1_{right}|context)) \cdot P(2_{right}|context) \cdot P(5_{right}|context)$$ Why Fast我们看$(1300)(3001)$，其实$(1300)$的意思是对于Tree中的每一个内接点，第二层中的300个神经元都与他是连接的。而$(300*1)$只是权重的个数而已。 对于传统的softmax来讲，隐藏层到分类层到参数个数为300*20000。这些参数都需要更新的话，简直没有办法计算了。 但是对于Huffman Tree来讲，拿cat举例，我只需要更新。 $(1-P(1_{right}|context)) \cdot P(2_{right}|context) \cdot P(5_{right}|context) ——&gt;1$ 就好了。 也就是参数为：300*300*3。 说到底是训练每一个二叉树的分类能力，而与具体的每个词不相关了。]]></content>
  </entry>
  <entry>
    <title><![CDATA[GBDT]]></title>
    <url>%2F2020%2F08%2F16%2FGBDT%2F</url>
    <content type="text"><![CDATA[GBDTGBDT回归和分类的相同 根据样本，预测初始值$F_0(x)$ 根据损失函数，求一阶导数负梯度，得到残差的计算公式，并根据$F_0{x}$计算残差。 建立决策树，对残差进行回归。 根据建立好的回归树，对残差进行预测$\gamma_{jm}$。 $F_1(x) = F_0(x) + \gamma_{jm}$ GBDT回归和分类的不同GBDT中的回归和分类思路都是相似度，都是利用损失函数的一阶导数计算残差，并对残差进行回归，并不断的更新预测的残差，进而得到预测值。 只是由于回归和分类的损失函数不同，导致求残差的方式不同，预测残差的方式也不同。 而根据下面的四点不同，GBDT在代码实现的时候，实际上用到都是同一个回归树。只是有两个地方分别会调用自己的方法： 初始化参数。 回归树建立好之后，残差值的预测。 损失函数不同回归$$Loss = \frac{1}{2} * (y_i - F(x_i))^2$$ 分类$$p_i = \frac{1}{1 + e^{(-F_m(x_i))}}$$ $$Loss = - {y_i logp_i + (1 - y_i)log(1-p_i)}= -{y_iF_m(x_i) - log(1 + e^{F_m(x_i)})}$$ 初始化方式不同回归$$F_0(x) = \bar{y}$$ 分类$$F_0(x) = log(\frac{\sum y_i}{\sum(1 - y_i)})$$ 负梯度（残差）的求值不同回归$$y_i - F_{m-1}(x_i)$$ 分类$$y_i - \frac{1}{1 + e^{(-F_{m-1}(x_i))}} = y_i - p_{i}^{m-1}$$ 残差的预测值不同回归$\tilde{y_i}$决策树左边(右边)的残差(1..n)，但是不同的分类模型残差不同，也就是不同的分类模型，负梯度不同。$$\gamma_{jm} = average(\tilde{y_i})$$ 分类$$\gamma_{jm} = \frac{\sum_{i=1}^{N}\tilde y_i}{\sum_{i=1}^{N}\tilde (y_i - y_i) *(1 - y_i + y_i)}$$ 举例回归10个样本，每个样本一个维度，例如样本1：(1, 5,56)… STEP 1初始化初始值 $F_0(x_m) = avrage(\tilde y_i) = 7.307$ STEP 2计算负梯度$$\tilde y_i = - \frac{\partial Loss}{F(x_i)} = (y_i - F_0(xi))$$ STEP 3建立回归树，拟合负梯度，数据为上面的数据集。 STEP 4预测残差值 $\gamma_{jm} = average(\tilde{y_i})$ $\gamma_{11} = \frac{\tilde y_1 + \tilde y_2 + \tilde y_3 + \tilde y_4 + \tilde y_5 + \tilde y_6 }{6} = -1.0703$ $\gamma_{21} = \frac{\tilde y_7 + \tilde y_8 + \tilde y_9 + \tilde y_10 }{4} = 1.6055$ STEP 5$F_1(x) = F_0(x) + \gamma_{jm}$ $F_1(x_1) = F_0(x_1) + (-1.0703) = 7.307 - 1.0703 = 6.2367$ $F_2(x_2) = ……..$ ….. 由此得到$F_2(x_i)$ 第一轮的迭代结束，开始第二轮的迭代。。。。 分类10个样本，每个样本一维特征，分类为0或者1，举例样本1：(1，0) STEP 1初始化初始值 $F_0(x) =log(\frac{\sum y_i}{\sum(1 - y_i)})$ $F_0(x) = log(\frac{4}{6}) = −0.4054$ STEP 2计算负梯度 $y_i - \frac{1}{1 + e^{(-F_{m-1}(x_i))}}$ STEP 3建立回归树，拟合梯度，数据为上面的数据集。 STEP 4预测残差值 $\gamma_{jm} = \frac{\sum_{i=1}^{N}\tilde y_i}{\sum_{i=1}^{N}\tilde (y_i - y_i) *(1 - y_i + y_i)}$ $\gamma_{11} = \frac{\tilde y_1 + \tilde y_2 + \tilde y_3 + \tilde y_4 + \tilde y_5 + \tilde y_6 + \tilde y_7 + \tilde y_8}{(y_1 - \tilde y_1)(1 - y_1 - \tilde{y_1}) + (y_2 - \tilde y_2)(1 - y_2 - \tilde{y_2}) + (y_3 - \tilde y_3)(1 - y_3 - \tilde{y_3}) + (y_4 - \tilde y_4)(1 - y_4 - \tilde{y_4}) + (y_5 - \tilde y_5)(1 - y_5 - \tilde{y_5}) + (y_6 - \tilde y_6)(1 - y_6 - \tilde{y_6}) + (y_7 - \tilde y_7)(1 - y_7 - \tilde{y_7}) + (y_8 - \tilde y_8)(1 - y_8 - \tilde{y_8})} \ = \frac{-1.2}{1.92}= −0.625$ $\gamma_{21} = \frac{1.2}{0.48} = 2.5$ STEP 5$F_1(x) = F_0(x) + \gamma_{jm}$ $F_1(x_1) = F_0(x_1) + (−0.625) = −0.4054 −0.625 = -1.03$ $F_2(x_2) = ……..$ ….. 由此得到$F_2(x_i)$ 第一轮的迭代结束，开始第二轮的迭代。。。。]]></content>
  </entry>
  <entry>
    <title><![CDATA[HMM & CRF]]></title>
    <url>%2F2020%2F08%2F16%2FHMM%20%26%20CRF%2F</url>
    <content type="text"><![CDATA[HMM &amp; CRFHMMHMM的原理推导Language 序列如下： $x$是可以观察到的，而y是隐藏的状态。 转移概率：$P(PN|start)$，是从开始到第一个单词为专有名词的概率，同样的$P(V|PN)$是从专有名词，到动词的概率。 发射概率：$P(John|PN)$，是当词性为专有名词的时候，该词为John的概率。 那么对于如下的序列，产生该序列的概率为：$$P(x,y) = P(PN|start) * P(John|PN) P(V|PN)P(saw|V)P(D|V)P(the|D)P(N|D)P(saw|D) \= P(y) * P(x|y)$$也就是说，序列的随机场等于，该位置的转移概率发射概率，然后每一个位置叠成。简化后公式如下：$$P(x,y) = P(y)P(x|y)$$ $$P(y) = P(y_1|start) \times \prod_{l=1}^{L-1}P(y_{l+1}|y_l) \times P(end|y_L)$$ $$P(x|y) = \prod_{l=1}^{L}P(x_l|y_l)$$ HMM预测最有可能发生 也就是，给我们一个可以看到的序列，我们去预测最可能的隐藏状态。举例子来讲就是，给了一句话，去预测每个词等词性。在真正的使用时，其实是我们自己假设遍历所有可能得隐藏状态序列，然后计算出所有的序列的概率，然后取概率最大的一个就是最有可能发生的情况。其实也就是所有的可能中最大的一个$P(x,y)$ ，这里显然就是一个动态规划问题，也就是用维比特进行计算求全局最优解就是了。 用公式表示就是：$$y = \mathop{\arg\min}{y\epsilon Y} P(y|x) = \mathop{\arg\min}{y\epsilon Y} \frac{P(x,y)}{P(x)} = \mathop{\arg\min}{y\epsilon Y} P(x,y) \= \mathop{\arg\min}{y\epsilon Y} \ \ P(y_1|start)\prod_{l=1}^{L-1}P(y_{l+1}|y_l)P(end|y_L)\prod_{l=1}^{L}P(x_l|y_l)$$ HMM训练推测依据 我们自然会问，你遍历所有的可能计算的概率，其实你必须有每一个的转移概率和发射概率，那么这些概率你是怎么得来的，难道是瞎猜的么。 不是的，这些概率是需要从历史的样本中去观察的。 这个过程叫做训练模型，实际上，我们真的是只用观察就可以统计到所有的转移概率和发射概率。 就比方说，我们有1000个序列，找一些语言学家，帮助我们标注好，那些词是动词，那些是名词。 那么我们就可以这样统计： 转移概率：$P(V|PN) = \frac{PN后接V总数}{PN后接V、D等总数}$ 发射概率：$P(John|PN) = \frac{PN发射为John的总数}{PN发射为John、Mark等总数}$ 写的完整一点就是：$$P(x,y) = P(y_1|start)\prod_{l=1}^{L-1}P(y_{l+1}|y_l)P(end|y_L)\prod_{l=1}^{L}P(x_l|y_l)$$ $$P(y_{l+1}=s’|y_l =s) = \frac{count(s \to s’)}{count(s)}$$ $$P(x_l =t|y_l=s) = \frac{count(s \to t)}{count(s)}$$ 其实HMM的训练过程是一个统计过程，HMM的预测过程就是叠乘，而为了减少HMM预测过程遍历导致的运算量的优化算法，就是维特比算法。 其实HMM到这里就已经结束了。 可是为什么还要有CRF或者Structure SVM接下来的算法呢？ 因为HMM有他的算法缺陷 HMM的弱点其实HMM是隐马尔可夫模型。那么什么是马尔可夫呢？ 下面是一系列定义： 随机场 ：随机场是由若干个位置组成的整体，当给每一个位置中按照某种分布随机赋予一个值之后，其全体就叫做随机场。还是举词性标注的例子：假如我们有一个十个词形成的句子需要做词性标注。这十个词每个词的词性可以在我们已知的词性集合（名词，动词…)中去选择。当我们为每个词选择完词性后，这就形成了一个随机场。 马尔科夫随机场：马尔科夫随机场是随机场的特例，它假设随机场中某一个位置的赋值仅仅与和它相邻的位置的赋值有关，和与其不相邻的位置的赋值无关。 条件随机场是马尔科夫随机场的特例，它假设马尔科夫随机场中只有X和Y两种变量，X一般是给定的，而Y一般是在给定X的条件下我们的输出。这样马尔科夫随机场就特化成了条件随机场。 线性链条件随机场：X和Y有相同的结构的CRF就构成了线性链条件随机场。 其实马尔可夫随机场的假设，是当前的位置的赋值，只和临近的位置相关，和其他位置无关，其实这种假设是非常粗糙的。这也是我们接下来举得例子会有问题的原因。 假如: N-V-c = 9 P-V-a =9 N-D-a= 1 那么： $P(V|N) = \frac{9}{10}, P(D|N) = \frac{1}{10},P(V|P)=1​$ $P(a|V) = \frac{1}{2}, P(c|V) = \frac{1}{2},P(a|D)=1​$ 可以计算： $P(N, V, a) = \frac{9}{20}$ $P(N, V,c) = \frac{9}{20}$ $P(N,D,a) = \frac{1}{10}$ 可以看到一个在样本中从未出现的序列$P(N,V,a)$比在样本中出现过的$P(N,D,a)$还要大。 这是非常不正常的。 而这个问题是可以用CRF来解决的。 CRFCRF原理推导CRF和HMM最大的不同就是，HMM中的$P(y_1|start), P(x_l|y_l)$等是通过统计的方式获得的，但是在CRF中是经过SGD等方法收敛求解的。 先从HMM的$P(x, y)​$看：$$P(x,y) = P(y_1|start)\prod_{l=1}^{L-1}P(y_{l+1}|y_l)P(end|y_L)\prod_{l=1}^{L}P(x_l|y_l)$$两边求对数可得：$$logP(x,y) = logP(y_1|start) + \sum_{l=1}^{L-1}logP(y_{l+1}|y_l) + logP(end|y_L) + \sum_{l=1}^{L}logP(x_l|y_l)$$也就是说，上面的每一项，都是通过SGD求得的。 以$\sum_{l=1}^LlogP(x_l|y_l)$举例：$$\sum_{l=1}^{L}logP(x_l|y_l) = logP(the|D) + logP(dog|N) + logP(ate|V) + logP(the|D) + logP(homework|N) \= logP(the|D) \times 2 + logP(dog|N) \times 1 + logP(ate|V) \times 1 + logP(homework|N) \times 1$$ 从上面的推导中可以看到：$$\sum_{l=1}^{L}logP(x_l|y_l) = \sum_{s,t}logP(t|s) \times N_{s,t}(x,y)$$其中： $P(t|s)$：类似$P(the|D)$，在s词性的情况下，是单词t的概率。该值和HMM中的发射概率是一个东西，但是HMM中可以通过简单的统计获得，而在CRF中，这个是未知的，也是SGD待求的参数。 $N_{s,t}(x,y)$：代表的是，在给的样本序列中，对应的P(t|s)的个数。 那么同理的其他三项也可以同样的表示：$$\sum_{l=1}^{L}logP(x_l|y_l) = \sum_{s,t}logP(t|s) \times N_{s,t}(x,y)$$ $$logP(y_1|start) = \sum_S logP(s|start) \times N_{start,s}(x,y)$$ $$logP(y_{l+1}|y_l) = \sum_{S,S’} logP(s’|s) \times N_{S,S’}(x,y)$$ $$logP(y_{end}|y_L) = \sum_{S} logP(end|S) \times N_{S,end}(x,y)$$ 其中，$logP(y_1|start)$ 的意义为： 在给定的样本中，$y_1$为序列开始的概率 $\times$ 所有以$y_1$为开始的数量。 其他亦然。 那么：$logP(y_1|start), logP(y_{l+1}|y_l), logP(y_{end}|y_L), logP(x_l,y_l)$都是待求的 参数。 那么：$$logP(x,y) = logP(y_1|start) + \sum_{l=1}^{L-1}logP(y_{l+1}|y_l) + logP(end|y_L) + \sum_{l=1}^{L}logP(x_l|y_l)$$ $$= \left[\begin{matrix} .\.\ logP(t|s)\.\.\logP(s|start)\.\.\logP(s’|s)\.\.\logP(end|s)\.\.\ \end{matrix}\right]\tag{1} \cdot \left[\begin{matrix} .\.\ N_{s,t}(x,y)\.\.\N_{start,t}(x,y)\.\.\N_{s,s’}(x,y)\.\.\N_{s,end}(x,y)\.\.\ \end{matrix}\right] = \omega \cdot \phi(x,y)$$ 其中： $w$：为待求的参数矩阵。为发射概率和转移概率的矩阵。 $\phi(x,y)$：为count得到的值。对应的是某一个发射概率，或者某一个转移概率在样本中出现的次数。 由此可得，$$P(x,y) = exp(\omega \cdot \phi(x,y))$$但是其实他不是相等的，是正相关的，也就是让$P(x,y)$取得最大值的y和$exp(\omega \cdot \phi(x,y))$取得最大值的y是同一个y。$$P(x,y) \varpropto exp(\omega \cdot \phi(x,y))$$ CRF 训练$(x^n, y^n)​$是training data，$x^n​$是给出的词，$\hat{y}^n​$是预测正确的词性，$y’​$是预测的错误的词性。 目标函数$$O(w) = \sum_{n=1}^N logP(\hat{y}^n|x^n)$$ 其中： $x^n$：给定的观察到的序列. $\hat{y}^n$：正确的隐藏状态。 也就是去求一个$w$使得：$$\arg\max_{w} O(w)$$解释： 任务是最大化目标函数。也就是对于一个序列，在$x^n$发生的情况下，使得$\hat{y}^n$最大的$w$矩阵。 第一阶段：贝叶斯对于$ logP(\hat{y}^n|x^n)$: 由于：$$P(y|x) =\frac{P(x, y)}{\sum_{y’}P(x, y’)}$$则：$$logP(\hat{y}^n|x^n) \=log \frac{P(x^n, \hat{y}^n)}{P(x^n)} \=log \frac{P(x^n, \hat{y}^n)}{\sum_{y’} P(x^n, y’)}\=logP(x^n, \hat{y}^n) - log\sum_{y’}P(x^n, y’)$$物理意义： 通过以上可以知道，使得$O(w)​$最大的物理意义，就是让预测正确的概率越大越好，让其他的预测错误的概率越小越好。 $$O(w) = \sum_{n=1}^{N}logP(\hat{y}^n|x^n) = \sum_{n=1}^{N}O^n(w)$$ 而： $$O^n(w) = logP(\hat{y}^n |x^n)=logP(x^n, y^n) - log\sum_{y’}P(x^n, y’)$$ 第二阶段：求偏导对$O^n(w)​$求偏导： $$\frac{\partial{O^n(w)}}{\partial{w_{s,t}}} \=\frac{\partial{(logP(\hat{y}^n |x^n))}}{\partial{w_{s,t}}} \= \frac{\partial{logP(x^n, \hat{y}^n)}}{\partial{w_{s,t}}} - \frac{\partial{(log\sum_y’P(x^n, y’))}}{\partial{w_(s,t)}}$$分别求偏导： 偏导一$$P(x^n , y^n) = exp(w \phi(x^n, y^n))$$ 对于$w\phi(x^n, y^n)​$: $$w\phi(x^n, y^n) = \sum_{s,t}w_{s,t} \cdot N_{s,t}(x^n, \hat{y}^n) + \sum_{s,s’}w_{s,s’} \cdot N_{s,s’}(x^n, \hat{y}^n)$$则： $$\frac{\partial{logP(x^n, y^n)}}{\partial{w_{s,t}}} \= \frac{\partial{(w\phi(x^n, y^n))}}{\partial{w_{s,t}}} \= N_{s,t}(x^n, \hat{y}^n)$$ 偏导二$$P(x^n , y’) = exp(w \phi(x^n, y’))$$ 令： $$Z(x^n) = \sum_{y’}exp(w \cdot \phi(x^n, y’)) = \sum_{y’}exp(\sum_{s,t}w_{s,t} \cdot N_{s,t}(x^n, y’) + \sum_{s,s’}w_{s,s’} \cdot N_{s,s’}(x^n, y’))=\sum_{y’}P(x^n, y’)$$则： $$\frac{\partial{(log\sum_{y’}P(x^n, y’))}}{\partial{w_(s,t)}} \=\frac{\partial(logZ(x^n))}{\partial w_{s,t}} \= \frac{1}{Z(x^n)} \cdot \frac{\partial{Z(x^n)}}{\partial{w_{s,t}}}$$而：$$\frac{\partial{Z(x^n)}}{\partial{w_{s,t}}} \= Z(x^n) \cdot N_{s,t}(x^n, y’) \=\sum_{y’}P(x^n, y’) \cdot N_{s,t}(x^n, y’) \= \sum_{y’}\frac{P(x^n, y’)}{1}\cdot N_{s,t}(x^n, y’) \= \sum_{y’}\frac{P(x^n, y’)}{P(x^n)}\cdot N_{s,t}(x^n, y’) \= \sum_{y’}P(y’|x^n)\cdot N_{s,t}(x^n, y’)$$这里是对指数函数求导数，和前面的不太一样。 这里$P(x^n) = 1$，其实是考虑到所有的$x^n$都是给定的，忽略不会有什么影响。 第三阶段：合并$$\frac{\partial{O^n(w)}}{\partial{w_{s,t}}} \=\frac{\partial{(logP(\hat{y}^n |x^n))}}{\partial{w_{s,t}}} \= \frac{\partial{logP(x^n, \hat{y}^n)}}{\partial{w_{s,t}}} - \frac{\partial{(log\sum_y’P(x^n, y’))}}{\partial{w_(s,t)}} \=N_{s,t}(x^n, \hat{y}^n) - \sum_{y’}P(y’|x^n)\cdot N_{s,t}(x^n, y’)$$ 前面的一项不用说，后面的一项是可以用维比特算法来解的。 物理意义 当前，预测正确的，词性和词 共同出现的词对。减去。所有预测不对的概率，再乘以预测不对的共同出现的词对。 CRF预测推导过程如下：$$y = \mathop{\arg\max {y\epsilon Y}P(y|x)} = \mathop{\arg\max{y\epsilon Y}}P(x,y) = \mathop{\arg\max_{y\epsilon Y} \omega \cdot \phi(x,y)}$$训练过程，基于样本$\omega$矩阵是已经得到的了，那么使用维比特算法，求一个动态规划问题，全局最优解就可以了。 HMM和CRF的比较HMM做的事情是最大化$P(x,\hat{y})$。 CRF不进最大化$P(x, \hat{y})$，还最小化$P(x,y’)$，所以CRF要比HMM更有可能得到最优的结果。 RNN和CRF的比较RNN Deep CRF 如果采用的是单方向RNN，每一个时间步，得到的输出，其实不是看完整个sequence得到的，是基于之前的信息得到的。这一点CRF求得是全局最优解。 可以对label进行强干预。比方说，动词后面不能接动词，我是可以直接去修改转移概率矩阵，使得在维比特求解的时候，动词后面接动词的路径都不走。而RNN很难做到这一点，RNN也不是不可以学到这些信息，但是需要更多的数据，需要更多的训练资源。 RNN中的cost和error不一定是很直接相关的。你可能训练了一个loss非常小的模型，但是你考核error的方式和loss的方式不是一样的，导致loss可能很小，但是error不一定很小。但是CRF的cost和error是直接相关的。 RNN和CRF的结合HMM和CRF中：$$P(x,y) = P(y_1|start)\prod_{l=1}^{L-1}P(y_{l+1}|y_l)P(end|y_L)\prod_{l=1}^{L}P(x_l|y_l)$$我们把$P(x_l|y_l)$用RNN每一步$P(y_l|x_l)$的输出来代替。 但是RNN的概率分布输出，到CRF的发射概率，需要有一个转化的过程：$$P(x_l|y_l) = \frac{P(x_l,y_l)}{P(y_l)} = \frac{P(y_l|x_l)P(x_l)}{P(y_l)} = \frac{P(y_l|x_l)}{P(y_l)}$$其中： $P(y_l|x_l)$ ： 是RNN的输出。 $P(y_l)$ : 样本中某一个label出现的几率。count可得。 $P(x_l)$ : $x_l$是给定的，忽略不影响结果，大部分框架都是如此实现。]]></content>
  </entry>
  <entry>
    <title><![CDATA[BP算法]]></title>
    <url>%2F2020%2F08%2F16%2FBP%E7%AE%97%E6%B3%95%E6%8E%A8%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[BP算法前向传播对于k，k+1 层的前向传播为： $$z^{(k)} = w^{(k)}*n^{(k-1)} + b^{(k)}, n^{(k-1)} = f(z^{(k-1)})$$ $$z^{(k+1)} = w^{(k+1)}*n^{(k)} + b^{(k+1)}, n^{(k)} = f(z^{(k)})$$ 反向传播损失函数$$L（y,\hat{y}）$$ Loss对k层$w^{(k)}, b^{(k)}$偏导数为： $$\frac{\partial{L(y, \hat{y})}}{\partial{w^{(k)}}} = \frac{\partial{L(y, \hat{y})}}{\partial{z^{(k)}}} \cdot \frac{\partial{z^{(k)}}}{\partial{w^{(k)}}}$$ $$\frac{\partial{L(y, \hat{y})}}{\partial{b^{(k)}}} = \frac{\partial{L(y, \hat{y})}}{\partial{z^{(k)}}} \cdot \frac{\partial{z^{(k)}}}{\partial{b^{(k)}}}$$ 假设：$\delta^{(k)} = \frac{\partial{L(y, \hat{y})}}{\partial{z^{(k)}}}$ 计算$z^{(k)}$的导数$$\delta^{(k)} \ = \frac{\partial{L(y, \hat{y})}}{\partial{z^{(k)}}} \ = \frac{\partial{L(y, \hat{y})}}{\partial{z^{(k+1)}}} \cdot \frac{\partial{z^{(k+1)}}}{\partial{n^{(k)}}} \cdot \frac{\partial{n^{(k)}}}{\partial{z^{(k)}}} \= \delta^{(k+1)} \cdot w^{(k+1)} \cdot f_k’(z^{(k)})$$ 计算$\frac{\partial{z^{(k)}}}{\partial{w^{(k)}}}$$$\frac{\partial{z^{(k)}}}{\partial{w^{(k)}}} = n^{(k-1)}$$ 计算$\frac{\partial{z^{(k)}}}{\partial{b^{(k)}}}$$$\frac{\partial{z^{(k)}}}{\partial{b^{(k)}}} = 1$$ 计算反向传播 $$\frac{\partial{L(y,\hat{y})}}{\partial{w^{(k)}}} = \delta^{(k)} \cdot \frac{\partial{z^{(k)}}}{\partial{w^{(k)}}} = \delta^{(k)} \cdot n^{(k-1)}$$ $$\frac{\partial{L(y,\hat{y})}}{\partial{b^{(k)}}} = \delta^{(k)} \cdot \frac{\partial{z^{(k)}}}{\partial{b^{(k)}}} = \delta^{(k)}$$]]></content>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch]]></title>
    <url>%2F2020%2F08%2F16%2FElasticSearch%2F</url>
    <content type="text"><![CDATA[ElasticSearch基础为了搜索安装和运行1./bin/elasticsearch 测试 Elasticsearch 是否启动成功 1curl &quot;http://localhost:9200/?pretty&quot; 安装Sense下载kibana 1https://download.elastic.co/elastic/sense/sense-latest.tar.gz 在 Kibana 目录下运行下面的命令，下载并安装 Sense app 1./bin/kibana plugin --install elastic/sense win： 1bin\kibana.bat plugin --install elastic/sense 启动 Kibana 1./bin/kibana win: 1bin\kibana.bat 访问： 1http://localhost:5601 和ES交互计算集群中文档的数量： 123456GET /_count&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;&#125; 索引 一个 Elasticsearch 集群可以包含多个索引 相应的每个索引可以包含多个类型 这些不同的类型存储着多个文档 每个文档又有多个属性 索引： 一个索引类似于传统关系数据库中的一个数据库，是一个存储关系型文档的地方。 索引一个文档 就是存储一个文档到一个索引（名词）中以便它可以被检索和查询到。 关系型数据库通过增加一个索引比如一个B树（B-tree）索引到指定的列上，以便提升数据检索速度。 一个文档中的每一个属性都是被索引的（有一个倒排索引）和可搜索的。一个没有倒排索引的属性是不能被搜索到的。 索引一个文档： 1234567891011PUT /megacorp/employee/1&#123; &quot;first_name&quot;: &quot;John&quot;, &quot;last_name&quot;: &quot;Smith&quot;, &quot;age&quot;: 25, &quot;about&quot;: &quot;I love to go rock climbing&quot;, &quot;interests&quot;: [ &quot;sports&quot;, &quot;music&quot; ]&#125; megacorp：索引名称 employee：类型名称 1：特定雇员的ID 检索文档增1234567891011PUT /megacorp/employee/1&#123; &quot;first_name&quot;: &quot;John&quot;, &quot;last_name&quot;: &quot;Smith&quot;, &quot;age&quot;: 25, &quot;about&quot;: &quot;I love to go rock climbing&quot;, &quot;interests&quot;: [ &quot;sports&quot;, &quot;music&quot; ]&#125; 删1DELETE /megacorp/employee/1 改1234567891011PUT /megacorp/employee/1&#123; &quot;first_name&quot;: &quot;John&quot;, &quot;last_name&quot;: &quot;Smith&quot;, &quot;age&quot;: 25, &quot;about&quot;: &quot;I love to go rock climbing&quot;, &quot;interests&quot;: [ &quot;sports&quot;, &quot;music&quot; ]&#125; 查1GET /megacorp/employee/1 1HEAD /megacorp/employee/1 轻量搜索搜索所有雇员： 1GET /megacorp/employee/_search 高亮搜索 1GET /megacorp/employee/_search?q=last_name:Smith 使用查询表达式搜索12345678POST /megacorp/employee/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;last_name&quot;: &quot;Smith&quot; &#125; &#125;&#125; 短语搜索 精确匹配一系列单词或者短语 。 比如， 我们想执行这样一个查询，仅匹配同时包含 “rock” 和 “climbing” 并且 二者以短语 “rock climbing” 的形式紧挨着的雇员记录。 12345678GET /megacorp/employee/_search&#123; &quot;query&quot; : &#123; &quot;match_phrase&quot; : &#123; &quot;about&quot; : &quot;rock climbing&quot; &#125; &#125;&#125; 高亮搜索12345678910111213GET /megacorp/employee/_search&#123; &quot;query&quot;: &#123; &quot;match_phrase&quot;: &#123; &quot;about&quot;: &quot;rock climbing&quot; &#125; &#125;, &quot;highlight&quot;: &#123; &quot;fields&quot;: &#123; &quot;about&quot;: &#123;&#125; &#125; &#125;&#125; 12345&quot;highlight&quot;: &#123; &quot;about&quot;: [ &quot;I love to go &lt;em&gt;rock&lt;/em&gt; &lt;em&gt;climbing&lt;/em&gt;&quot; ] &#125; 分析12345678910GET /megacorp/employee/_search&#123; &quot;aggs&quot;: &#123; &quot;all_interests&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;interests&quot; &#125; &#125; &#125;&#125; 12345678910111213141516171819202122&#123; ... &quot;hits&quot;: &#123; ... &#125;, &quot;aggregations&quot;: &#123; &quot;all_interests&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;music&quot;, &quot;doc_count&quot;: 2 &#125;, &#123; &quot;key&quot;: &quot;forestry&quot;, &quot;doc_count&quot;: 1 &#125;, &#123; &quot;key&quot;: &quot;sports&quot;, &quot;doc_count&quot;: 1 &#125; ] &#125; &#125;&#125; 查询特定兴趣爱好员工的平均年龄： 12345678910111213GET /megacorp/employee/_search&#123; &quot;aggs&quot; : &#123; &quot;all_interests&quot; : &#123; &quot;terms&quot; : &#123; &quot;field&quot; : &quot;interests&quot; &#125;, &quot;aggs&quot; : &#123; &quot;avg_age&quot; : &#123; &quot;avg&quot; : &#123; &quot;field&quot; : &quot;age&quot; &#125; &#125; &#125; &#125; &#125;&#125; 12345678910111213141516171819202122232425&quot;all_interests&quot;: &#123; &quot;buckets&quot;: [ &#123; &quot;key&quot;: &quot;music&quot;, &quot;doc_count&quot;: 2, &quot;avg_age&quot;: &#123; &quot;value&quot;: 28.5 &#125; &#125;, &#123; &quot;key&quot;: &quot;forestry&quot;, &quot;doc_count&quot;: 1, &quot;avg_age&quot;: &#123; &quot;value&quot;: 35 &#125; &#125;, &#123; &quot;key&quot;: &quot;sports&quot;, &quot;doc_count&quot;: 1, &quot;avg_age&quot;: &#123; &quot;value&quot;: 25 &#125; &#125; ] &#125; 集群内的原理如果我们启动了一个单独的节点，里面不包含任何的数据和 索引，那我们的集群看起来就是一个包含空内容节点的集群。一个运行中的 Elasticsearch 实例称为一个 节点，而集群是由一个或者多个拥有相同 cluster.name 配置的节点组成， 它们共同承担数据和负载的压力。当有节点加入集群中或者从集群中移除节点时，集群将会重新平均分布所有的数据。 一个节点被选举成为 主 节点时， 它将负责管理集群范围内的所有变更，例如增加、删除索引，或者增加、删除节点等。 而主节点并不需要涉及到文档级别的变更和搜索等操作，所以当集群只拥有一个主节点的情况下，即使流量的增加它也不会成为瓶颈。 任何节点都可以成为主节点。我们的示例集群就只有一个节点，所以它同时也成为了主节点。 作为用户，我们可以将请求发送到 集群中的任何节点 ，包括主节点。 每个节点都知道任意文档所处的位置，并且能够将我们的请求直接转发到存储我们所需文档的节点。 无论我们将请求发送到哪个节点，它都能负责从各个包含我们所需文档的节点收集回数据，并将最终结果返回給客户端。 Elasticsearch 对这一切的管理都是透明的。 集群健康1GET _cluster/health 1234567891011121314151617&#123; &quot;cluster_name&quot;: &quot;elasticsearch&quot;, &quot;status&quot;: &quot;yellow&quot;, &quot;timed_out&quot;: false, &quot;number_of_nodes&quot;: 1, &quot;number_of_data_nodes&quot;: 1, &quot;active_primary_shards&quot;: 5, &quot;active_shards&quot;: 5, &quot;relocating_shards&quot;: 0, &quot;initializing_shards&quot;: 0, &quot;unassigned_shards&quot;: 5, &quot;delayed_unassigned_shards&quot;: 0, &quot;number_of_pending_tasks&quot;: 0, &quot;number_of_in_flight_fetch&quot;: 0, &quot;task_max_waiting_in_queue_millis&quot;: 0, &quot;active_shards_percent_as_number&quot;: 50&#125; status的三种颜色含义如下： green 所有的主分片和副本分片都正常运行。 yellow 所有的主分片都正常运行，但不是所有的副本分片都正常运行。 red 有主分片没能正常运行。 集群的健康状况为 yellow 则表示全部 主 分片都正常运行（集群可以正常服务所有请求），但是 副本 分片没有全部处在正常状态。 实际上，所有3个副本分片都是 unassigned —— 它们都没有被分配到任何节点。 在同一个节点上既保存原始数据又保存副本是没有意义的，因为一旦失去了那个节点，我们也将丢失该节点上的所有副本数据。 添加索引索引实际上是指向一个或者多个物理 分片 的 逻辑命名空间 。 一个 分片 是一个底层的 工作单元 ，它仅保存了 全部数据中的一部分。一个分片是一个 Lucene 的实例，以及它本身就是一个完整的搜索引擎。我们的文档被存储和索引到分片内，但是应用程序是直接与索引而不是与分片进行交互。 分片是数据的容器，文档保存在分片内，分片又被分配到集群内的各个节点里。 当你的集群规模扩大或者缩小时， Elasticsearch 会自动的在各节点中迁移分片，使得数据仍然均匀分布在集群里。 一个分片可以是 主 分片或者 副本 分片。 索引内任意一个文档都归属于一个主分片，所以主分片的数目决定着索引能够保存的最大数据量。 技术上来说，一个主分片最大能够存储 Integer.MAX_VALUE - 128 个文档，但是实际最大值还需要参考你的使用场景：包括你使用的硬件， 文档的大小和复杂程度，索引和查询文档的方式以及你期望的响应时长。 一个副本分片只是一个主分片的拷贝。 副本分片作为硬件故障时保护数据不丢失的冗余备份，并为搜索和返回文档等读操作提供服务。 在索引建立的时候就已经确定了主分片数，但是副本分片数可以随时修改。 索引在默认情况下会被分配5个主分片， 但是为了演示目的，我们将分配3个主分片和一份副本（每个主分片拥有一个副本分片）： 1234567PUT /blogs&#123; &quot;settings&quot;: &#123; &quot;number_of_shards&quot;: 3, &quot;number_of_replicas&quot;: 1 &#125;&#125; 当第二个节点加入到集群后，3个 副本分片 将会分配到这个节点上——每个主分片对应一个副本分片。 这意味着当集群内任何一个节点出现问题时，我们的数据都完好无损。 所有新近被索引的文档都将会保存在主分片上，然后被并行的复制到对应的副本分片上。这就保证了我们既可以从主分片又可以从副本分片上获得文档。 拥有6个分片（3个主分片和3个副本分片）的索引可以最大扩容到6个节点，每个节点上存在一个分片，并且每个分片拥有所在节点的全部资源。 如果我们想要扩容超过6个节点怎么办呢？ 主分片的数目在索引创建时 就已经确定了下来。实际上，这个数目定义了这个索引能够 存储 的最大数据量。（一个主分片最大能够存储 Integer.MAX_VALUE（21 4748 3647） - 128 ） 但是，读操作——搜索和返回数据——可以同时被主分片 或 副本分片所处理，所以当你拥有越多的副本分片时，也将拥有越高的吞吐量。 在运行中的集群上是可以动态调整副本分片数目的 ，我们可以按需伸缩集群。让我们把副本数从默认的 1 增加到 2 ： 1234PUT /blogs/_settings&#123; &quot;number_of_replicas&quot; : 2&#125; 应对故障如果我们关闭第一个节点 我们关闭的节点是一个主节点。而集群必须拥有一个主节点来保证正常工作，所以发生的第一件事情就是选举一个新的主节点： Node 2 。 在我们关闭 Node 1 的同时也失去了主分片 1 和 2 ，并且在缺失主分片的时候索引也不能正常工作。 如果此时来检查集群的状况，我们看到的状态将会为 red ：不是所有主分片都在正常工作。 幸运的是，在其它节点上存在着这两个主分片的完整副本， 所以新的主节点立即将这些分片在 Node 2 和 Node 3 上对应的副本分片提升为主分片， 此时集群的状态将会为 yellow 。 这个提升主分片的过程是瞬间发生的，如同按下一个开关一般。 数据输入和输出文档元数据三个必须的元数据元素如下： _index : 文档在哪存放 _type ：文档表示的对象类别 _id ： 文档唯一标识 _index一个 索引 应该是因共同的特性被分组到一起的文档集合。例如，你可能存储所有的产品在索引 products 中，而存储所有销售的交易到索引 sales 中。 实际上，在 Elasticsearch 中，我们的数据是被存储和索引在 分片 中，而一个索引仅仅是逻辑上的命名空间， 这个命名空间由一个或者多个分片组合在一起。 然而，这是一个内部细节，我们的应用程序根本不应该关心分片，对于应用程序而言，只需知道文档位于一个 索引 内。 Elasticsearch 会处理所有的细节。 索引名，名字必须小写，不能以下划线开头，不能包含逗号。 _type_type 命名可以是大写或者小写，但是不能以下划线或者句号开头，不应该包含逗号， 并且长度限制为256个字符 _id索引文档使用自定义ID123456PUT /website/blog/123&#123; &quot;title&quot;: &quot;My first blog entry&quot;, &quot;text&quot;: &quot;Just trying this out...&quot;, &quot;date&quot;: &quot;2014/01/01&quot;&#125; 自生成ID123456POST /website/blog/&#123; &quot;title&quot;: &quot;My second blog entry&quot;, &quot;text&quot;: &quot;Still trying this out...&quot;, &quot;date&quot;: &quot;2014/01/01&quot;&#125; 取回一个文档返回文档一部分1GET /website/blog/123?_source=title,text 1234567891011&#123; &quot;_index&quot; : &quot;website&quot;, &quot;_type&quot; : &quot;blog&quot;, &quot;_id&quot; : &quot;123&quot;, &quot;_version&quot; : 1, &quot;found&quot; : true, &quot;_source&quot; : &#123; &quot;title&quot;: &quot;My first blog entry&quot; , &quot;text&quot;: &quot;Just trying this out...&quot; &#125;&#125; 检查文档是否存在1HEAD megacorp/employee/1 1200 - OK 1HEAD megacorp/employee/5 1404 - Not Found 更新整个文档123456PUT /website/blog/123&#123; &quot;title&quot;: &quot;My first blog entry&quot;, &quot;text&quot;: &quot;I am starting to get the hang of this...&quot;, &quot;date&quot;: &quot;2014/01/02&quot;&#125; 1234567891011121314&#123; &quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;blog&quot;, &quot;_id&quot;: &quot;123&quot;, &quot;_version&quot;: 2, &quot;result&quot;: &quot;updated&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;_seq_no&quot;: 1, &quot;_primary_term&quot;: 1&#125; version:2。代表被更新了 在内部，Elasticsearch 已将旧文档标记为已删除，并增加一个全新的文档。 尽管你不能再对旧版本的文档进行访问，但它并不会立即消失。当继续索引更多的数据，Elasticsearch 会在后台清理这些已删除文档。 update API 从旧文档构建 JSON 更改该 JSON 删除旧文档 索引一个新文档创建新文档 搜索文档中的每个字段都将被索引并且可以被查询 。搜索（search） 可以做到： 在类似于 gender 或者 age 这样的字段 上使用结构化查询，join_date 这样的字段上使用排序，就像SQL的结构化查询一样。 全文检索，找出所有匹配关键字的文档并按照相关性（relevance） 排序后返回结果。 以上二者兼而有之。空搜索 1GET /_search 12345678910111213141516171819202122232425262728&#123; &quot;hits&quot; : &#123; &quot;total&quot; : 14, &quot;hits&quot; : [ &#123; &quot;_index&quot;: &quot;us&quot;, &quot;_type&quot;: &quot;tweet&quot;, &quot;_id&quot;: &quot;7&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;date&quot;: &quot;2014-09-17&quot;, &quot;name&quot;: &quot;John Smith&quot;, &quot;tweet&quot;: &quot;The Query DSL is really powerful and flexible&quot;, &quot;user_id&quot;: 2 &#125; &#125;, ... 9 RESULTS REMOVED ... ], &quot;max_score&quot; : 1 &#125;, &quot;took&quot; : 4, &quot;_shards&quot; : &#123; &quot;failed&quot; : 0, &quot;successful&quot; : 10, &quot;total&quot; : 10 &#125;, &quot;timed_out&quot; : false&#125; hits包含 total 字段来表示匹配到的文档总数，并且一个 hits 数组包含所查询结果的前十个文档 _score衡量了文档与查询的匹配程度，返回的文档是按照 _score 降序排列的。 tooktook 值告诉我们执行整个搜索请求耗费了多少毫秒。 timeouttimed_out 值告诉我们查询是否超时。如果低响应时间比完成结果更重要，你可以指定 timeout 为 10 或者 10ms（10毫秒），或者 1s（1秒）： 1GET /_search?timeout=10ms 应当注意的是 timeout 不是停止执行查询，它仅仅是告知正在协调的节点返回到目前为止收集的结果并且关闭连接。在后台，其他的分片可能仍在执行查询即使是结果已经被发送了。使用超时是因为 SLA(服务等级协议)对你是很重要的，而不是因为想去中止长时间运行的查询。 多索引，多类型/索引/类型/_search 1234567891011121314/_search在所有的索引中搜索所有的类型/gb/_search在 gb 索引中搜索所有的类型/gb,us/_search在 gb 和 us 索引中搜索所有的文档/g*,u*/_search在任何以 g 或者 u 开头的索引中搜索所有的类型/gb/user/_search在 gb 索引中搜索 user 类型/gb,us/user,tweet/_search在 gb 和 us 索引中搜索 user 和 tweet 类型/_all/user,tweet/_search在所有的索引中搜索 user 和 tweet 类型 分页from 和 size 参数size：显示应该返回的结果数量，默认是 10 from：显示应该跳过的初始结果数量，默认是 0 栗子如果每页展示 5 条结果，可以用下面方式请求得到 1 到 3 页的结果： 123GET /_search?size=5GET /_search?size=5&amp;from=5GET /_search?size=5&amp;from=10 请记住一个请求经常跨越多个分片，每个分片都产生自己的排序结果，这些结果需要进行集中排序以保证整体顺序是正确的 轻量搜索两种形式的 搜索 API“轻量的” 查询字符串 “+” 前缀表示必须与查询条件匹配 “-“ 前缀表示一定不与查询条件匹配 没有 “+” 或者 “-“ 的所有其他条件都是可选的 “:” 包含 例子： 1GET /_all/tweet/_search?q=tweet:elasticsearch tweet 类型中 tweet 字段包含 elasticsearch 单词的所有文档 1GET /_all/tweet/_search?q=+name:john +tweet:mary name 字段中包含 john 并且在 tweet 字段中包含 mary 的文档 1GET /_search?q=%2Bname%3Ajohn+%2Btweet%3Amary 字符串参数URL编码 _all1GET /_search?q=mary 当索引一个文档的时候，Elasticsearch取出所有字段的值拼接成一个大的字符串，作为 _all 字段进行索引。 123456&#123; &quot;tweet&quot;: &quot;However did I manage before Elasticsearch?&quot;, &quot;date&quot;: &quot;2014-09-14&quot;, &quot;name&quot;: &quot;Mary Jones&quot;, &quot;user_id&quot;: 1&#125; 这就好似增加了一个名叫 _all 的额外字段： 1&quot;However did I manage before Elasticsearch? 2014-09-14 Mary Jones 1&quot; 更复杂的查询栗子： name 字段中包含 mary 或者 john date 值大于 2014-09-10 all 字段包含 aggregations 或者 geo 1+name:(mary john) +date:&gt;2014-09-10 +(aggregations geo) 映射和分析1GET /索引/_mapping/类型 1GET /feed_sit/_mapping/feed_sit 1234567891011121314151617181920212223&#123; &quot;gb&quot;: &#123; &quot;mappings&quot;: &#123; &quot;tweet&quot;: &#123; &quot;properties&quot;: &#123; &quot;date&quot;: &#123; &quot;type&quot;: &quot;date&quot;, &quot;format&quot;: &quot;strict_date_optional_time||epoch_millis&quot; &#125;, &quot;name&quot;: &#123; &quot;type&quot;: &quot;string&quot; &#125;, &quot;tweet&quot;: &#123; &quot;type&quot;: &quot;string&quot; &#125;, &quot;user_id&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125; &#125; &#125; &#125; &#125;&#125; 精确值和全文Elasticsearch 中的数据可以概括的分为两类：精确值和全文。 倒排索引 Elasticsearch使用一种称为倒排索引的结构，它适用于快速的全文搜索。 一个倒排索引由文档中所有不重复词的列表构成，对于其中每个词，有一个包含它的文档列表。 ‘+’ 前缀表明这个词必须存在。 分词和标准化的过程称为 分析 分析和分析器分析 包含下面的过程： 首先，将一块文本分成适合于倒排索引的独立的 词条 之后，将这些词条统一化为标准格式以提高它们的“可搜索性”，或者 recall 字符过滤器首先，字符串按顺序通过每个 字符过滤器 。他们的任务是在分词前整理字符串。一个字符过滤器可以用来去掉HTML，或者将 &amp; 转化成 and。 分词器其次，字符串被 分词器分为单个的词条。一个简单的分词器遇到空格和标点的时候，可能会将文本拆分成词条。 Token 过滤器最后，词条按顺序通过每个 token 过滤器 。这个过程可能会改变词条（例如，小写化 Quick ），删除词条（例如， 像 a，and，the 等无用词），或者增加词条（例如，像 jump 和 leap 这种同义词）。 内置分析器标准分析器根据 Unicode 联盟 定义的 单词边界 划分文本。删除绝大部分标点。最后，将词条小写。 简单分析器简单分析器在任何不是字母的地方分隔文本，将词条小写。 语言分析器特定语言分析器可用于很多语言。英语分析器附带了一组英语无用词（常用单词，例如 and 或者 the，它们对相关性没有多少影响），它们会被删除。由于理解英语语法的规则，这个分词器可以提取英语单词的 词干 。 测试分析器12345GET /_analyze&#123; &quot;analyzer&quot;: &quot;standard&quot;, &quot;text&quot;: &quot;Text to analyze&quot;&#125; 指定分析器当Elasticsearch在你的文档中检测到一个新的字符串域 ，它会自动设置其为一个全文 字符串 域，使用 标准 分析器对它进行分析。 你想使用一个不同的分析器，适用于你的数据使用的语言。有时候你想要一个字符串域就是一个字符串域–不使用分析，直接索引你传入的精确值，例如用户ID或者一个内部的状态域或标签。 要做到这一点，我们必须手动指定这些域的映射。 映射Elasticsearch需要知道每个域中数据的类型。这个信息包含在映射中。 核心简单域类型Elasticsearch 支持 如下简单域类型： 字符串: string 整数 : byte, short, integer, long 浮点数: float, double 布尔型: boolean 日期: date 当你索引一个包含新域的文档–之前未曾出现– Elasticsearch会使用动态映射，通过JSON中基本数据类型，尝试猜测域类型。 这意味着如果你通过引号(“123”)索引一个数字，它会被映射为string类型，而不是long。但是，如果这个域已经映射为long，那么Elasticsearch 会尝试将这个字符串转化为long，如果无法转化，则抛出一个异常。 查看映射1GET /gb/_mapping/tweet 1234567891011121314151617181920212223&#123; &quot;gb&quot;: &#123; &quot;mappings&quot;: &#123; &quot;tweet&quot;: &#123; &quot;properties&quot;: &#123; &quot;date&quot;: &#123; &quot;type&quot;: &quot;date&quot;, &quot;format&quot;: &quot;strict_date_optional_time||epoch_millis&quot; &#125;, &quot;name&quot;: &#123; &quot;type&quot;: &quot;string&quot; &#125;, &quot;tweet&quot;: &#123; &quot;type&quot;: &quot;string&quot; &#125;, &quot;user_id&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125; &#125; &#125; &#125; &#125;&#125; 自定义域映射 域最重要的属性是 type 。对于不是 string 的域，你一般只需要设置 type ： type = string(text) index : 怎样索引字符串 analyzed : 首先分析字符串，然后索引它 索引这个域，所以它能够被搜索，但索引的是精确值。不会对它进行分析 不索引这个域。这个域不会被搜索到。 analyzer: standard、whitespace、simple、english 123456&#123; &quot;tag&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;index&quot;: &quot;not_analyzed&quot; &#125;&#125; 123456&#123; &quot;tweet&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;english&quot; &#125;&#125; 更新映射1DELETE /gb 12345678910111213141516171819202122PUT /gb&#123; &quot;mappings&quot;: &#123; &quot;tweet&quot;: &#123; &quot;properties&quot;: &#123; &quot;tweet&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;english&quot; &#125;, &quot;date&quot;: &#123; &quot;type&quot;: &quot;date&quot; &#125;, &quot;name&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;user_id&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125; &#125; &#125; &#125;&#125; 增加一个新的名为 tag 的 not_analyzed 的文本域： 123456789PUT /gb/_mapping/tweet&#123; &quot;properties&quot; : &#123; &quot;tag&quot; : &#123; &quot;type&quot; : &quot;string&quot;, &quot;index&quot;: &quot;not_analyzed&quot; &#125; &#125;&#125; 不需要再次列出所有已存在的域，因为无论如何我们都无法改变它们。新域已经被合并到存在的映射中。 请求体查询空查询12POST /index/type1,type2/_search&#123;&#125; from 和 size 参数12345POST /_search&#123; &quot;from&quot;: 30, &quot;size&quot;: 10&#125; 查询表达式查询语句的结构123456POST /_search&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;&#125; 12345678GET /_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;tweet&quot;: &quot;elasticsearch&quot; &#125; &#125;&#125; 合并查询语句一条复合语句可以合并任何其它查询语句，包括复合语句。这就意味着，复合语句之间可以互相嵌套，可以表达非常复杂的逻辑。 1234567891011121314151617181920212223242526272829POST /feed_sit/feed_sit/_search?pretty=true&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: &#123; &quot;match&quot;: &#123; &quot;info_title&quot;: &quot;区块链&quot; &#125; &#125;, &quot;must_not&quot;: &#123; &quot;match&quot;: &#123; &quot;name&quot;: &quot;mary&quot; &#125; &#125;, &quot;should&quot;: &#123; &quot;match&quot;: &#123; &quot;tweet&quot;: &quot;full text&quot; &#125; &#125;, &quot;filter&quot;: &#123; &quot;range&quot;: &#123; &quot;age&quot;: &#123; &quot;gt&quot;: 30 &#125; &#125; &#125; &#125; &#125;&#125; 嵌套 12345678910111213141516171819202122232425262728293031323334POST /feed_sit/feed_sit/_search?pretty=true&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: &#123; &quot;match&quot;: &#123; &quot;email&quot;: &quot;business opportunity&quot; &#125; &#125;, &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;starred&quot;: true &#125; &#125;, &#123; &quot;bool&quot;: &#123; &quot;must&quot;: &#123; &quot;match&quot;: &#123; &quot;folder&quot;: &quot;inbox&quot; &#125; &#125;, &quot;must_not&quot;: &#123; &quot;match&quot;: &#123; &quot;spam&quot;: true &#125; &#125; &#125; &#125; ], &quot;minimum_should_match&quot;: 1 &#125; &#125;&#125; 查询与过滤 从 Elasticsearch2.0开始，过滤（filters）已经从技术上被排除了，同时所有的查询（queries）拥有变成不评分查询的能力。 我们用 “filter” 这个词表示不评分、只过滤情况下的查询。你可以把 “filter” 、 “filtering query” 和 “non-scoring query” 这几个词视为相同的。 过滤（filtering）的目标是减少那些需要通过评分查询（scoring queries）进行检查的文档。 最重要的查询match_all查询match_all 查询简单的匹配所有文档。在没有指定查询方式时，它是默认的查询： 1&#123; &quot;match_all&quot;: &#123;&#125;&#125; match查询1&#123; &quot;match&quot;: &#123; &quot;tweet&quot;: &quot;About Search&quot; &#125;&#125; 对于精确值的查询，你可能需要使用 filter 语句来取代query，因为 filter 将会被缓存 multi_match查询在 content_keywords、title_keywords字段中，查询“区块链”关键字的信息 123456789POST /feed_sit/feed_sit/_search?pretty=true&#123; &quot;query&quot;: &#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;区块链&quot;, &quot;fields&quot;: [&quot;content_keywords&quot;,&quot;title_keywords&quot;] &#125; &#125;&#125; range查询1234567891011POST /feed_sit/feed_sit/_search?pretty=true&#123; &quot;query&quot;: &#123; &quot;range&quot;: &#123; &quot;info_push_time&quot;: &#123; &quot;gte&quot;: &quot;2018-03-05 16:51:04&quot;, &quot;lt&quot;: &quot;2018-03-08 16:51:04&quot; &#125; &#125; &#125;&#125; term查询term 查询被用于精确值匹配，这些精确值可能是数字、时间、布尔或者那些 not_analyzed 的字符串。 12345678910POST /feed_sit/feed_sit/_search?pretty=true&#123; &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;info_push_time&quot;: &#123; &quot;value&quot;: &quot;2018-03-05 20:36:01&quot; &#125; &#125; &#125;&#125; terms查询允许你指定多值进行匹配。如果这个字段包含了指定值中的任何一个值，那么这个文档满足条件。 1234567891011POST /feed_sit/feed_sit/_search?pretty=true&#123; &quot;query&quot;: &#123; &quot;terms&quot;: &#123; &quot;FIELD&quot;: [ &quot;VALUE1&quot;, &quot;VALUE2&quot; ] &#125; &#125;&#125; exists 查询和 missing 查询用来查找有无该字段的文档 12345678POST /feed_sit/feed_sit/_search?pretty=true&#123; &quot;query&quot;: &#123; &quot;exists&quot;:&#123; &quot;field&quot;:&quot;info_title&quot; &#125; &#125;&#125; 组合多查询需要在多个字段上查询多种多样的文本，并且根据一系列的标准来过滤。 可以用 bool 查询来实现需求。这种查询将多查询组合在一起，成为用户自己想要的布尔查询。 相关性得分是如何组合的。每一个子查询都独自地计算文档的相关性得分。一旦他们的得分被计算出来，bool查询就将这些得分进行合并并且返回一个代表整个布尔操作的得分。 参数： must文档 必须 匹配这些条件才能被包含进来。 must_not文档 必须不 匹配这些条件才能被包含进来。 should如果满足这些语句中的任意语句，将增加 _score ，否则，无任何影响。它们主要用于修正每个文档的相关性得分。 filter必须 匹配，但它以不评分、过滤模式来进行。这些语句对评分没有贡献，只是根据过滤标准来排除或包含文档。 栗子： 下面的查询用于查找： title 字段匹配 how to make millions 并且tag 字段不被标识为 spam 的文档 那些被标识为 starred或在2014之后的文档，将比另外那些文档拥有更高的排名 如果 两者都满足，那么它排名将更高： 12345678910&#123; &quot;bool&quot;: &#123; &quot;must&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;how to make millions&quot; &#125;&#125;, &quot;must_not&quot;: &#123; &quot;match&quot;: &#123; &quot;tag&quot;: &quot;spam&quot; &#125;&#125;, &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;tag&quot;: &quot;starred&quot; &#125;&#125;, &#123; &quot;range&quot;: &#123; &quot;date&quot;: &#123; &quot;gte&quot;: &quot;2014-01-01&quot; &#125;&#125;&#125; ] &#125;&#125; 如果没有 must 语句，那么至少需要能够匹配其中的一条 should 语句。但，如果存在至少一条must语句，则对should语句的匹配没有要求。 过滤器123456789101112&#123; &quot;bool&quot;: &#123; &quot;must&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;how to make millions&quot; &#125;&#125;, &quot;must_not&quot;: &#123; &quot;match&quot;: &#123; &quot;tag&quot;: &quot;spam&quot; &#125;&#125;, &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;tag&quot;: &quot;starred&quot; &#125;&#125; ], &quot;filter&quot;: &#123; &quot;range&quot;: &#123; &quot;date&quot;: &#123; &quot;gte&quot;: &quot;2014-01-01&quot; &#125;&#125; &#125; &#125;&#125; 如果你需要通过多个不同的标准来过滤你的文档，bool查询本身也可以被用做不评分的查询。 1234567891011121314151617181920&#123; &quot;bool&quot;: &#123; &quot;must&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;how to make millions&quot; &#125;&#125;, &quot;must_not&quot;: &#123; &quot;match&quot;: &#123; &quot;tag&quot;: &quot;spam&quot; &#125;&#125;, &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;tag&quot;: &quot;starred&quot; &#125;&#125; ], &quot;filter&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;range&quot;: &#123; &quot;date&quot;: &#123; &quot;gte&quot;: &quot;2014-01-01&quot; &#125;&#125;&#125;, &#123; &quot;range&quot;: &#123; &quot;price&quot;: &#123; &quot;lte&quot;: 29.99 &#125;&#125;&#125; ], &quot;must_not&quot;: [ &#123; &quot;term&quot;: &#123; &quot;category&quot;: &quot;ebooks&quot; &#125;&#125; ] &#125; &#125; &#125;&#125; constant_score查询它被经常用于你只需要执行一个filter而没有其它查询（例如，评分查询）的情况下。 可以使用它来取代只有filter语句的bool查询。在性能上是完全相同的，但对于提高查询简洁性和清晰度有很大帮助。 1234567&#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;term&quot;: &#123; &quot;category&quot;: &quot;ebooks&quot; &#125; &#125; &#125;&#125; 验证查询排序和相关性排序在 Elasticsearch 中， 相关性得分 由一个浮点数进行表示，并在搜索结果中通过_score 参数返回， 默认排序是 _score 降序。 123456789101112GET /_search&#123; &quot;query&quot; : &#123; &quot;bool&quot; : &#123; &quot;filter&quot; : &#123; &quot;term&quot; : &#123; &quot;user_id&quot; : 1 &#125; &#125; &#125; &#125;&#125; 此时返回的评分没有意义，可以采用下面的写法： 123456789101112GET /_search&#123; &quot;query&quot; : &#123; &quot;constant_score&quot; : &#123; &quot;filter&quot; : &#123; &quot;term&quot; : &#123; &quot;user_id&quot; : 1 &#125; &#125; &#125; &#125;&#125; 评分默认为：1 按照字段的值进行排序123456789GET /_search&#123; &quot;query&quot; : &#123; &quot;bool&quot; : &#123; &quot;filter&quot; : &#123; &quot;term&quot; : &#123; &quot;user_id&quot; : 1 &#125;&#125; &#125; &#125;, &quot;sort&quot;: &#123; &quot;date&quot;: &#123; &quot;order&quot;: &quot;desc&quot; &#125;&#125;&#125; 计算 _score 的花销巨大，通常仅用于排序； 我们并不根据相关性排序，所以记录_score 是没有意义的。如果无论如何你都要计算 _score，你可以将track_scores参数设置为true 。 多级排序结合使用 date 和 _score进行查询，匹配的结果首先按照日期排序，然后按照相关性排序。 结果首先按第一个条件排序，仅当结果集的第一个sort值完全相同时才会按照第二个条件进行排序，以此类推。 12345678910111213GET /_search&#123; &quot;query&quot; : &#123; &quot;bool&quot; : &#123; &quot;must&quot;: &#123; &quot;match&quot;: &#123; &quot;tweet&quot;: &quot;manage text search&quot; &#125;&#125;, &quot;filter&quot; : &#123; &quot;term&quot; : &#123; &quot;user_id&quot; : 2 &#125;&#125; &#125; &#125;, &quot;sort&quot;: [ &#123; &quot;date&quot;: &#123; &quot;order&quot;: &quot;desc&quot; &#125;&#125;, &#123; &quot;_score&quot;: &#123; &quot;order&quot;: &quot;desc&quot; &#125;&#125; ]&#125; 字符段排序与多字段什么是相关性默认情况下，返回结果是按相关性倒序排列的。 fuzzy 查询会计算与关键词的拼写相似程度，terms查询会计算找到的内容与关键词组成部分匹配的百分比。 Elasticsearch 的相似度算法 被定义为检索词频率/反向文档频率， TF/IDF ，包括以下内容： 检索词频率： 检索词在该字段出现的频率？出现频率越高，相关性也越高。 字段中出现过 5 次要比只出现过 1 次的相关性高。 反向文档频率： 每个检索词在索引中出现的频率？频率越高，相关性越低。检索词出现在多数文档中会比出现在少数文档中的权重更低。 字段长度准则： 字段的长度是多少？长度越长，相关性越低。 检索词出现在一个短的 title 要比同样的词出现在一个长的 content 字段权重更大。 如果多条查询子句被合并为一条复合查询语句 ，比如 bool 查询，则每个查询子句计算得出的评分会被合并到总的相关性评分中。 理解评分标准1234GET /_search?explain &#123; &quot;query&quot; : &#123; &quot;match&quot; : &#123; &quot;tweet&quot; : &quot;honeymoon&quot; &#125;&#125;&#125; 12345678910111213141516171819202122232425262728293031&quot;_explanation&quot;: &#123; //honeymoon 相关性评分计算的总结 &quot;description&quot;: &quot;weight(tweet:honeymoon in 0) [PerFieldSimilarity], result of:&quot;, &quot;value&quot;: 0.076713204, &quot;details&quot;: [ &#123; &quot;description&quot;: &quot;fieldWeight in 0, product of:&quot;, &quot;value&quot;: 0.076713204, &quot;details&quot;: [ &#123; //检索词频率 &quot;description&quot;: &quot;tf(freq=1.0), with freq of:&quot;, &quot;value&quot;: 1, &quot;details&quot;: [ &#123; &quot;description&quot;: &quot;termFreq=1.0&quot;, &quot;value&quot;: 1 &#125; ] &#125;, &#123; //反向文档频率 &quot;description&quot;: &quot;idf(docFreq=1, maxDocs=1)&quot;, &quot;value&quot;: 0.30685282 &#125;, &#123; //字段长度准则 &quot;description&quot;: &quot;fieldNorm(doc=0)&quot;, &quot;value&quot;: 0.25, &#125; ] &#125; ]&#125; 输出 explain 结果代价是十分昂贵的，它只能用作调试工具。千万不要用于生产环境。 检索词频率: 检索词 honeymoon 在这个文档的 tweet 字段中的出现次数。 反向文档频率: 检索词 honeymoon 在索引上所有文档的 tweet 字段中出现的次数。 字段长度准则: 在这个文档中， tweet 字段内容的长度 – 内容越长，值越小。 深入搜索结构化搜索精确值查找当进行精确值查找时， 我们会使用过滤器（filters）。过滤器很重要，因为它们执行速度非常快，不会计算相关度（直接跳过了整个评分阶段）而且很容易被缓存。请尽可能多的使用过滤式查询。 term查询数字12345&#123; &quot;term&quot; : &#123; &quot;price&quot; : 20 &#125;&#125; 不评分： 123456789101112GET /my_store/products/_search&#123; &quot;query&quot; : &#123; &quot;constant_score&quot; : &#123; &quot;filter&quot; : &#123; &quot;term&quot; : &#123; &quot;price&quot; : 20 &#125; &#125; &#125; &#125;&#125; term查询文本123SELECT productFROM productsWHERE productID = &quot;XHDK-A-1293-#fJ3&quot; 123456789101112GET /my_store/products/_search&#123; &quot;query&quot; : &#123; &quot;constant_score&quot; : &#123; &quot;filter&quot; : &#123; &quot;term&quot; : &#123; &quot;productID&quot; : &quot;XHDK-A-1293-#fJ3&quot; &#125; &#125; &#125; &#125;&#125; 12345GET /my_store/_analyze&#123; &quot;field&quot;: &quot;productID&quot;, &quot;text&quot;: &quot;XHDK-A-1293-#fJ3&quot;&#125; 123456789101112131415161718192021222324252627&#123; &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;xhdk&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 4, &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot; : 1 &#125;, &#123; &quot;token&quot; : &quot;a&quot;, &quot;start_offset&quot; : 5, &quot;end_offset&quot; : 6, &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot; : 2 &#125;, &#123; &quot;token&quot; : &quot;1293&quot;, &quot;start_offset&quot; : 7, &quot;end_offset&quot; : 11, &quot;type&quot; : &quot;&lt;NUM&gt;&quot;, &quot;position&quot; : 3 &#125;, &#123; &quot;token&quot; : &quot;fj3&quot;, &quot;start_offset&quot; : 13, &quot;end_offset&quot; : 16, &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot; : 4 &#125; ]&#125; Elasticsearch 用 4 个不同的 token 而不是单个 token 来表示这个 UPC 。 所有字母都是小写的。 所有字母都是小写的。组合过滤器布尔过滤器 1234567&#123; &quot;bool&quot; : &#123; &quot;must&quot; : [], &quot;should&quot; : [], &quot;must_not&quot; : [], &#125;&#125; must 所有的语句都 必须（must） 匹配，与 AND 等价must_not 所有的语句都 不能（must not） 匹配，与 NOT 等价。should 至少有一个语句要匹配，与 OR 等价。栗子： 1234SELECT productFROM productsWHERE (price = 20 OR productID = &quot;XHDK-A-1293-#fJ3&quot;) AND (price != 30) 123456789101112131415161718GET /my_store/products/_search&#123; &quot;query&quot; : &#123; &quot;filtered&quot; : &#123; &quot;filter&quot; : &#123; &quot;bool&quot; : &#123; &quot;should&quot; : [ &#123; &quot;term&quot; : &#123;&quot;price&quot; : 20&#125;&#125;, &#123; &quot;term&quot; : &#123;&quot;productID&quot; : &quot;XHDK-A-1293-#fJ3&quot;&#125;&#125; ], &quot;must_not&quot; : &#123; &quot;term&quot; : &#123;&quot;price&quot; : 30&#125; &#125; &#125; &#125; &#125; &#125;&#125; 嵌套布尔过滤器栗子： 12345SELECT documentFROM productsWHERE productID = &quot;KDKE-B-9947-#kL5&quot; OR ( productID = &quot;JODL-X-1937-#pV7&quot; AND price = 30 ) 1234567891011121314151617181920GET /my_store/products/_search&#123; &quot;query&quot; : &#123; &quot;filtered&quot; : &#123; &quot;filter&quot; : &#123; &quot;bool&quot; : &#123; &quot;should&quot; : [ &#123; &quot;term&quot; : &#123;&quot;productID&quot; : &quot;KDKE-B-9947-#kL5&quot;&#125;&#125;, &#123; &quot;bool&quot; : &#123; &quot;must&quot; : [ &#123; &quot;term&quot; : &#123;&quot;productID&quot; : &quot;JODL-X-1937-#pV7&quot;&#125;&#125;, &#123; &quot;term&quot; : &#123;&quot;price&quot; : 30&#125;&#125; ] &#125;&#125; ] &#125; &#125; &#125; &#125;&#125; 管理、监控和部署监控Marvel监控Marvel安装need to install two components: An Elasticsearch plugin that collects data from each node. A Kibana app that provides the Marvel monitoring UI. 默认，Marvel数据存放在ES安装的目录。recommend that you store the Marvel data in a separate monitoring cluster.这样即使你的ES处于不健康的状态，Mavrel也可以对其进行监控。 部署重要配置的修改其它数据库可能需要调优，但总得来说，Elasticsearch不需要。如果你遇到了性能问题，解决方法通常是更好的数据布局或者更多的节点。 指定名称Elasticsearch 默认启动的集群名字叫 elasticsearch 。简单修改成 elasticsearch_production。 elasticsearch.yml 文件中修改： 1cluster.name: elasticsearch_production 最好也修改你的节点名字。 elasticsearch.yml 文件中修改： 1node.name: elasticsearch_005_data 路径默认情况下， Elasticsearch会把插件、日志以及你最重要的数据放在安装目录下。这会带来不幸的事故，如果你重新安装Elasticsearch的时候不小心把安装目录覆盖了。如果你不小心，你就可能把你的全部数据删掉了。 最好的选择就是把你的数据目录配置到安装目录以外的地方，同样你也可以选择转移你的插件和日志目录。 可以更改如下： 1234567path.data: /path/to/data1,/path/to/data2 # Path to log files:path.logs: /path/to/logs# Path to where plugins are installed:path.plugins: /path/to/plugins]]></content>
  </entry>
  <entry>
    <title><![CDATA[Add Two Numbers]]></title>
    <url>%2F2019%2F05%2F11%2FAdd%20Two%20Numbers%2F</url>
    <content type="text"><![CDATA[Add Two Numbers题目 给出两个 非空 的链表用来表示两个非负的整数。其中，它们各自的位数是按照 逆序 的方式存储的，并且它们的每个节点只能存储 一位 数字。 如果，我们将这两个数相加起来，则会返回一个新的链表来表示它们的和。 您可以假设除了数字 0 之外，这两个数都不会以 0 开头。 示例： 1234&gt; 输入：(2 -&gt; 4 -&gt; 3) + (5 -&gt; 6 -&gt; 4)&gt; 输出：7 -&gt; 0 -&gt; 8&gt; 原因：342 + 465 = 807&gt; 解法123456789101112131415161718192021222324252627from typing import List# Definition for singly-linked list.class ListNode: def __init__(self, x): self.val = x self.next = Noneclass Solution: def addTwoNumbers(self, l1: ListNode, l2: ListNode) -&gt; ListNode: carry = 0 root = n = ListNode(0) while l1 or l2 or carry: v1 = v2 = 0 if l1: v1 = l1.val l1 = l1.next if l2: v2 = l2.val l2 = l2.next carry, val = divmod(v1+v2+carry, 10) n.next = ListNode(val) n = n.next return root.next 释义这个题目有一下几个点： divmod函数是对参数x/y 取整和求余。代表的是数字想加中的逢10进1位。而carry中存的其实下一位有可能进的数，可以是0，也可以是1。 root和n的关系，其实是一个变量地址和内存地址的引用问题，下面有引用的示意图。 v1和v2初始化是为了防止，比方说下一轮的l1和l2都已经是None了，但是carry还不是0，这样就会陷入死循环。 if l1，if l2的判断是为了防止carry= 1, 而 l1 和 l2 = None，导致的l1.val 报错。 图解]]></content>
      <tags>
        <tag>Add Two Numbers</tag>
      </tags>
  </entry>
</search>
