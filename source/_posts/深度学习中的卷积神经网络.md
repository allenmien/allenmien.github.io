---
title:      深度学习中的卷积神经网络
---
# 卷积神经网络

## 神经网络到卷积神经网络(CNN)

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-08-13-135713.png)

- 当层次开始非常多的时候，计算量会非常大。 
- 过拟合的问题。最根本的原因的神经元过多了。

问题：

- 把w的个数降下来。
- 又有很强的学习能力。

## 卷积神经网络之层级结构

- 数据输入层（Input layer）
- 卷积计算层（CONV layer）

- 激励层(ReLU layer)
- 池化层(Pooling layer)
- 全连接层（FC layer）

### 数据输入层

三种常见的图像数据处理方式

- 去均值

  - 把输入数据各个纬度都中心化到0

- 归一化

  - 幅度归一化到同样的范围

    ![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-08-14-130833.png)

- PCA/白化

  - 用PCA降维 
  - 白化是对数据每个特征轴上的幅度归一化

  ![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-08-14-130905.png)


### 卷积层（CONV layer）

- 局部关联

- 窗口滑动（receptive field）滑动，filter对全局数据计算


![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-08-14-131906.png)

#### 权重共享

- 概念理解
  - 某个神经元，只负责观测某一个维度的信息，再做汇总。
  - 一个框在左右上下滑，直到所有的数据都在这个框中至少出现过一次。
  - 整个面是32 * 32维的，窗口大小为3 * 3的，那么对于第一个神经元，对于视野内的9个像素点，有9个连接，也就是有9个权重
  - 当移动窗口进行移动的时候，按照道理，换了不同的数据，9条连接的权重应该是会变化的。
  - 卷积做了一件事件，他让这个权重，不管窗口的移动位置，是保持不变的。
  - 上述说的都是一个神经元做的事情。
  - 那么对于第二个神经元，他做的事情和第一个神经元做的事情是相同的，只是这9个 连接的权重与第一个神经元不同。
- 意义
  - 第一个神经元在观测颜色，第二个神经元在观测纹理，第三个神经元在观测轮廓。
  - 这只是一个通俗的解释，实际的意思是，每一个神经元都在观测某一个维度，但是不一定是在物理上是可以解释的。
  - 这样的话，确实是在降低w和b的个数，也确实是在减少参数。

#### 几个概念

##### 深度（depth）

下一层的神经元的个数

##### 步长(stride)

举例：

- 3 * 3  的窗口进行滑动的时候，不一定的边和边紧挨着进行滑动的。
- 比方下一个滑动的位置竖向上，滑动了一个单元，而实际上有2列是重复的。
- 那么这个步长其实就是1
- 他只滑动了一个位置

##### 填充值(zero-padding)

- 他的意思的说你设定好了窗口和步长，但是整个32  *  32并不一定被3 * 3和步长所整除，所以一定会出现滑不下去的情况。
- 为了解决这个问题，在滑不下去的时候，就在32 * 32的外面补个两列0,使得是能够继续滑下去的。

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/18-8-15/35203090.jpg)

### 激励层

进行下一层传递的时候，信息的约束。

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/18-8-15/19178950.jpg)

#### 常用的激活函数

- Sigmoid
- Tanh(双曲正切 )
- ReLU(修正线性单元--最常用)
- Leaky ReLU（下面的几个都是ReLU的几个变种）
- ELU
- Maxout

##### Sigmoid

Sigmoid函数的问题在于，当值非常大或者非常小的时候，斜率非常的小，趋近于0。

所以反向传播的时候，迭层之后可能为0。就起不到进行传播的作用了。

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/18-8-15/80493976.jpg)

##### Tanh

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/18-8-15/79263016.jpg)

##### ReLU

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/18-8-15/93102514.jpg)

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/18-8-15/90652084.jpg)

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/18-8-15/56730385.jpg)

##### Leaky ReLU

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/18-8-15/69685225.jpg)

##### ELU

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/18-8-15/54393948.jpg)

##### Maxout

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/18-8-15/47526086.jpg)

#### 实际经验

1. 不要用sigmoid！不要用sigmoid！不要用sigmoid！
2. 首先试RELU，因为快，但要小心点
3. 如果2失效，请用Leaky ReLU或者Maxout
4. 某些情况下tanh倒是有不错的结果，但是很少

### 池化层

- 夹在连续的卷积层中间
- 池化层是没有参数的。
- 他做了一个下采样，或者数据压缩，减少过拟合。

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-08-15-151906.png)

- 下采样的时候是会取一个数据窗口的。
- 在这个数据窗口中，经过池化层的作用之后，只输出一个值。

#### 两种常用的方式

- Max pooling(用的比较多)

  - 可以理解为在窗口内取最大值

  ![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-08-15-151942.png)

- Average pooling

  - 可以理解为在窗口内，取平均值

### 全连接层

- 两层之间的所有神经元都有权重连接
- 通常全连接在卷积神经元的网络尾部

### CNN的结构

- INPUT
- [[CONV -> RELU]*N -> POOL?]\*M    
- [FC -> RELU]*K
- FC

### 训练算法

- 先定义Loss Function
- 找到最小化损失函数的w和b，CNN中使用的算法是SGD
- Softmax是逻辑回归的升级版，是多分类的
- 还是使用BP算法，反向传播

#### 总结

- BP算法利用链式法则，逐级相乘直到求出dW和db
- 利用SGD随机梯度下降，迭代和更新W和 b

### 优缺点

#### 优点

- 共享卷积核，对高维数据处理无压力
- 无需手动选取特征，训练好权重，即得特征
- 分类效果好

#### 缺点

- 需要调参，需要大样本量，训练最好要GPU
- 物理涵义不明确

### 典型的CNN案例

实际上，我们大部分的工作都是在下面的 一些神经网络上进行的训练，并没有重新搭建新的神经网络。

- LeNet，这是最早用于数字识别的CNN
- AlexNet，2012 ILSVRC比赛远超第2名的CNN，比LeNet更深，用多层小卷积层叠加替换单大卷积层。
- ZF Net，2013 ILSVRC比赛冠军
- GoogLeNet，2014 ILSVRC比赛冠军
- VGGNet，2014 ILSVRC比赛中的模型，图像识别略差于GoogLeNet，但是在很多图像转化学习问题(比如object detection)上效果奇好
- ResNet，2015年 ILSVRC比赛的冠军，微软打造，层次极深(152层)

#### LeNet

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/18-8-16/24325019.jpg)

#### AlexNet

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/18-8-16/43613785.jpg)

### fine-tuning

#### 概念

使用已用于其他目标，训练好模型的权重或者部分权重，作为初始值开始训练

#### 原因

- 自己从头训练卷积神经网络容易出现问题
- fine-tuning能很快收敛到一个较理想的状态

#### 做法

- 复用相同层的权重，新定义层取随机权重初始值（前面的层可以使用别人的权重，后面的层，可以自己添加随机初始化权重）
- 调大新定义层的学习率，调小复用层学习率

### 常用框架

-  Caffe
  - 源于Berkeley的主流CV工具包，支持C++,python,matlab
  - Model Zoo中有大量预训练好的模型供使用
- Mxnet
  - Chentianqi团队主导的package
  - 对显存利用率非常高
  - 定义网络层简单
- TensorFlow
  - Google的深度学习框架
  - TensorBoard可视化很方便，实时监控
  - 数据和模型并行化好，速度快