---
title:      吴恩达deep-learning序列模型-序列模型与注意力机制
---
# 序列模型与注意力机制

## 基础模型

### 机器翻译（seq2seq）

编码网络和解码网络

这是一个RNN的结构

- 首先，把法语输入一个RNN，能够得到一个向量。--编码网络
- 再把这个向量作为输入，输入一个RNN，得到输出的英文。--解码网络

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-08-26-072208.png)

### 图像描述(image2seq)

- 首先把图片输入一个CNN，得到一个向量。
- 再把这个向量作为输入，输入到一个RNN，得到图片对应的描述。

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-08-26-072248.png)

## 选择最可能的句子

可以把机器翻译想象成一个条件语言模型

### 贪心算法

![](https://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-09-30-075835.png)

贪心算法在预测的时候，把编码生成的向量进行输入，然后先预测出第一个位置上最大概率的一个词，再预测第二个位置上最大概率的词，依次。。

这样的缺点是，仅仅是预测在第一个确定的情况下，第二个概率最大的词，这样虽然是概率最大的，但不一定是最好的。

比方说:图片中明显第一个的翻译会比较好。但是第二个最可能会被生成。因为在法语中，going 跟在 Jane is 后面的概率要比 visiting 跟在Jane is 后面的概率要大。

贪心算法的另外一个缺点是：

假如，字典都10,000个词。而我们要预测10个词的句子。

那么我们需要计算，10,000的10次方的概率，并从中挑选出概率最大的。这样会导致计算量过大。

## 定向搜索

###  集束算法

Beam Width ： for example, B = 3

Beam Search 不会只选出概率最大的那一个，他会选出3个。

当编码得到向量，输出第一个词的时候，选择概率最大的3个，而不是把10,000全部都保存下来。

计算第二个词的时候，是基于前面的3个词分别计算后面的10000个的概率的。所以共30000个的概率，从中选择前3的概率，保存下来，去进行第三个词的预测。

## 改进定向搜索

Beam Search的目标就是最大化的字符相乘的概率，总是取前3个。

最开始的时候，是采用下面的公式进行评价的：

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-10-02-072718.png)

但是这样的问题在于，叠成的概率会非常的小，导致计算机在计算的时候4舌5入掉很多浮点数，所以取了对数，如下：

![image-20181002152527241](/var/folders/bc/2lw168yd2w76klbbzb9fnkyc0000gn/T/abnerworks.Typora/image-20181002152527241.png)

但是这样也是有问题的，因为每个队概率都是非常的小，所以会导致，每多一个单词，概率都会小很多，所以在前面做了一个归一化的处理，并根据经验，给了一个参数用于调节归一化的程度，如下：

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-10-02-072205.png)

### 如何选择Beam Width

10

## 定向搜索的误差分析

### 如果Beam Search出现错误怎么办？

如果结果不理想，如何确认这个误差是由于Beam Search没搜索出来，还是因为RNN模型有问题呢？

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-10-02-085605.png)

解释：

- 第一句：法语
- 第二句 ：人工翻译的结果，作为Target
- 第三句： 模型预测的Sequence

Seq2Seq，是一个encoder和一个decoder，再加上Beam Search选出的Sequence。那么如果效果不理想，那么是RNN出了问题，还是Beam Search出了问题呢？

### 确定误差出现在哪里的方法

#### 第一种：

相当于，Beam Search 选出了一个最大的Sequence，但是这个选出的最大的还没有target到大，这就表明，其实是target理应存在在所有的Beam Width里面的，但是没有被搜索出来，这就说明，是Beam Search出了问题。

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-10-02-084032.png)

#### 第二种：

相当于，Beam Search已经选出了最大的概率的Sequence，当然正确的Target也是已经搜索出来了，只是在概率排名上，错误的比正确的高，这就说明Beam Search是生效的，但是RNN都模型是计算有偏差的。

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-10-02-084044.png)

#### 遍历并比较

变量每一预测的Sequence，与Target进行比较。只有出现大量的Bean Search的问题，才需要调整Beam Search 的Beam Width。如果RNN出现的问题较多，就可以去调节RNN。

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-10-02-085211.png)

## Bleu Score

Bilingual evaluation understudy(双语评估系统)

p_1:一元词组

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-10-03-072929.png)

p_2:二元词组

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-10-03-072902.png)

公式：

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-10-03-072837.png)

## 注意力模型直观理解

 如果一个非常长的句子，需要把整个句子记住再进行翻译，效果不是很好。

人类去翻译一段话，是一句一句翻译的，也就是一段话，从前往后根据上下文进行翻译的。

注意力模型也是类似的概念。再进行翻译的时候，输入翻译到该词的附近的词的特征作为输入。

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-10-03-130121.png)

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-10-03-130243.png)

##  注意力模型

我来总结一下我的理解：

- encoder其实是针对输入的句子训练一个RNN模型，这个RNN模型一般是双向是LSTM之类的，那么每一个词，每一步都是有a的。
- 训练一个另外的注意力模型，这个模型也是一个RNN。
  - 这个RNN的输入有，S和C。
  - S最开始为0向量。
  - C为所有输入的词的a * 对这个词的注意力权重
  - 注意力权重的训练方法也是一个小型的神经网络
    - 最后是一个softmax
    - 输入是改注意力模型该步的S和这个词的a

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-10-03-083053.png)

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-10-03-124452.png)

## 语音辩识

- 注意力模型
- CTC损失函数 
  - ![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-10-03-132547.png)

## 触发字检测
