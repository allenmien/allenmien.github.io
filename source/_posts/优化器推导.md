---
title: 深度学习中的优化器进化推导
---
# 深度学习中的优化器进化推导

## 指数加权平均

说到优化器，需要先了解一下指数加权平均的基础。

### 什么是指数加权平均

**指数加权平均**(exponentially weighted averges)，也叫指数加权移动平均，是一种常用的序列数据处理方式。

它的计算公式如下：

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2019-03-16-082342.png)

其中：

- θ_t：为第 t 天的实际观察值，
- V_t: 是要代替 θ_t      的估计值，也就是第 t 天的指数加权平均值，
- β： 为 V_{t-1}      的权重，是可调节的超参。( 0 < β < 1 )

例如：

我们有这样一组气温数据，图中横轴为一年中的第几天，纵轴为气温：

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2019-03-16-082415.png)

直接看上面的数据图会发现噪音很多，

这时，我们**可以用 指数加权平均 来提取这组数据的趋势，**

按照前面的公式计算：

这里先设置 β = 0.9，首先初始化 V_0 ＝ 0，然后计算出每个 V_t：

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2019-03-16-082434.png)

将计算后得到的 V_t 表示出来，就得到红色线的数值：

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2019-03-16-082500.png)

可以看出，红色的数据比蓝色的原数据更加平滑，**少了很多噪音**，并且**刻画了原数据的趋势**。

指数加权平均，作为原数据的**估计值**，不仅可以 **1. 抚平短期波动，起到了平滑的作用，2. 还能够将长线趋势或周期趋势显现出来**。

所以应用比较广泛，在处理统计数据时，在股价等时间序列数据中，CTR 预估中，美团外卖的收入监控报警系统中的 hot-winter 异常点平滑，深度学习的优化算法中都有应用。

### 为什么在优化算法中使用指数加权平均

上面提到了一些 指数加权平均 的应用，这里我们着重看一下在优化算法中的作用。

以 Momentum 梯度下降法为例，

**Momentum 梯度下降法**，就是计算了梯度的指数加权平均数，并以此来更新权重，它的运行**速度几乎总是快于标准的梯度下降算法**。

**这是为什么呢？**

让我们来看一下这个图，

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2019-03-16-082601.png)

例如这就是我们要优化的成本函数的形状，图中红点就代表我们要达到的最小值的位置，

假设我们**从左下角这里出发开始用梯度下降法**，那么蓝色曲线就是一步一步迭代，一步一步向最小值靠近的轨迹。

可以看出**这种上下波动，减慢了梯度下降法的速度**，而且无法使用更大的学习率，因为如果用较大的学习率，可能会偏离函数的范围。

如果有一种方法，可以使得在纵轴上，学习得慢一点，减少这些摆动，但是在横轴上，学习得快一些，快速地从左向右移移向红点最小值，那么训练的速度就可以加快很多。

这个方法就是动量 Momentum 梯度下降法，它**在每次计算梯度的迭代中，对 dw 和 db 使用了指数加权平均法的思想** ：

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2019-03-16-082634.png)

这样我们就可以得到如图红色线的轨迹：

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2019-03-16-082705.png)

可以看到：

**纵轴方向**，平均过程中正负摆动相互抵消，平均值接近于零，摆动变小，学习放慢。

**横轴方向**，因为所有的微分都指向横轴方向，因此平均值仍然较大，向最小值运动更快了。

在抵达最小值的路上减少了摆动，加快了训练速度。

### β 如何选择

根据前面的计算式子：

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2019-03-16-082737.png)

将 V_{100} 展开得到：

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2019-03-16-082752.png)

这里可以看出，V_t 是对每天温度的加权平均，之所以称之为指数加权，是因为加权系数是随着时间以指数形式递减的，**时间越靠近，权重越大**，越靠前，权重越小。

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2019-03-16-082815.png)

再来看下面三种情况：

当 β = 0.9 时，指数加权平均最后的结果如图**红色线**所示，代表的是最近 10 天的平均温度值；

当 β = 0.98 时，指结果如图**绿色线**所示，代表的是最近 50 天的平均温度值；

当 β = 0.5 时，结果如下图**黄色线**所示，代表的是最近 2 天的平均温度值；

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2019-03-16-082836.png)

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2019-03-16-082846.png)

**β 越小，噪音越多**，虽然能够很快的适应温度的变化，但是更容易出现奇异值。

**β 越大，得到的曲线越平坦**，因为多平均了几天的温度，这个曲线的波动更小。

但有个缺点是，因为只有 0.02 的权重给了当天的值，而之前的数值权重占了 0.98 ，

曲线进一步右移，在温度变化时就会适应地更缓慢一些，会出现一定延迟。

通过上面的内容可知，β 也是一个很重要的超参数，不同的值有不同的效果，需要调节来达到最佳效果，**一般 0.9 的效果就很好**。

## 优化器

### 几种优化方法之间的关系

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2019-03-16-082927.jpg)

### 梯度下降

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2019-03-16-082957.jpg)

基本的梯度下降法每次使用所有训练样本的平均损失来更新参数；

- 因此，经典的梯度下降在每次对模型参数进行更新时，需要遍历所有数据；
- 当训练样本的数量很大时，这需要消耗相当大的计算资源，在实际应用中基本不可行。

### 随机梯度下降(SGD)

随机梯度下降（SGD）每次使用单个样本的损失来近似平均损失

### 小批量随机梯度下降（BSGD）

- 为了降低随机梯度的方差，使模型迭代更加稳定，实践中会使用一批随机数据的损失来近似平均损失。
- 使用批训练的另一个主要目的，是为了利用高度优化的矩阵运算以及并行计算框架。

#### 随机梯度下降存在的问题

- 随机梯度下降（SGD）放弃了梯度的准确性，仅采用一部分样本来估计当前的梯度；因此      SGD 对梯度的估计常常出现偏差，造成目标函数收敛不稳定，甚至不收敛的情况。

- 无论是经典的梯度下降还是随机梯度下降，都可能陷入局部极值点；除此之外，SGD      还可能遇到“峡谷”和“鞍点”两种情况

- - 峡谷类似一个带有坡度的狭长小道，左右两侧是“峭壁”；在峡谷中，准确的梯度方向应该沿着坡的方向向下，但粗糙的梯度估计使其稍有偏离就撞向两侧的峭壁，然后在两个峭壁间来回震荡。
  - 鞍点的形状类似一个马鞍，一个方向两头翘，一个方向两头垂，而中间区域近似平地；一旦优化的过程中不慎落入鞍点，优化很可能就会停滞下来。

- ![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2019-03-16-083116.png)

### 指数加权平均

#### 例子

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2019-03-16-083152.jpg)

#### 公式

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2019-03-16-083210.jpg)

### Momentum

在当前的梯度上做文章，对当钱的梯度做指数加权，但是学习率不变。

#### 公式推导

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2019-03-16-083253.jpg)

#### 图像示意

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2019-03-16-083312.jpg)

### AdaGrad

在学习率上做文章

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2019-03-16-083337.jpg)

#### 优点

当梯度比较大的时候，G也就比较大，但是学习率会降低，也就是在陡峭的地方学习的会慢一些。

但是当梯度比较小的时候，G比较小，学习率比较大，也就是在平缓的比方会下降的快一些，不会停滞不前。

#### 缺点

由于是累积，导致G最后会累积的很大，导致到最后学习率会趋近于0，即使当钱是有梯度的，也无法继续梯度下降了。

### RMSprop

还是在学习率上做文章，但是是对学习率做了指数加权。

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2019-03-16-083428.jpg)

### Adam

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2019-03-16-083443.jpg)

