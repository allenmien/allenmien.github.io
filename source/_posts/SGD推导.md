---
title: 奇异值分解
---
# 奇异值分解

奇异值分解是一个有着很明显的物理意义的一种方法，它可以将一个比较复杂的矩阵用更小更简单的几个子矩阵的相乘来表示，这些小矩阵描述的是矩阵的重要的特性。

就像是描述一个人一样，给别人描述说这个人长得浓眉大眼，方脸，络腮胡，而且带个黑框的眼镜，这样寥寥的几个特征，就让别人脑海里面就有一个较为清楚的认识。

让机器学会抽取重要的特征。在机器学习领域，有相当多的应用与奇异值都可以扯上关系，比如做feature reduction的PCA，做数据压缩（以图像压缩为代表）的算法，还有做搜索引擎语义层次检索的LSI（Latent Semantic Indexing）。

## 奇异值分解详解

PCA的实现一般有两种，一种是用特征值分解去实现的，一种是用奇异值分解去实现的。

### 特征值分解

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/18-5-7/60105981.jpg)

特征值分解可以得到特征值与特征向量。

特征值表示的是这个特征到底有多重要。

而特征向量表示这个特征是什么。

可以将每一个特征向量理解为一个线性的子空间，我们可以利用这些线性的子空间干很多的事情。不过，特征值分解也有很多的局限，比如说变换的矩阵必须是方阵。

### 奇异值分解

特征值分解是一个提取矩阵特征很不错的方法，但是它只是对方阵而言的。

奇异值分解是一个能适用于任意的矩阵的一种分解的方法：

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/18-5-7/95163835.jpg)

 假设A是一个N * M的矩阵，那么得到的U是一个N * N的方阵（里面的向量是正交的，U里面的向量称为左奇异向量），Σ是一个N * M的矩阵（除了对角线的元素都是0，对角线上的元素称为奇异值），V’(V的转置)是一个M* M的矩阵，里面的向量也是正交的，V里面的向量称为右奇异向量）

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/18-5-7/20657806.jpg)



![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/18-5-7/88773392.jpg)

