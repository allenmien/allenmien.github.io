---
title: Word2Vec
---
# Word2Vec

## 神经网络结构

Word2Vec的架构是一个三层的神经网络，如下图：

![](https://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2019-04-04-034834.jpg)

假如共有10000个词汇。

第一层：输入一个词，维度为：1*10000.

第二层：10000*300权重矩阵。(没有激活函数)

第三层：300*10000。(对10000个可能做softmax)

共参数：10000\*300 + 300\*10000 = 600万的参数。

## 词向量

词向量何来？

第二层：

![](https://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2019-04-04-035137.jpg)



对于任何一个onehot单词，乘以第二层的矩阵，其实是得到的第二层10000行中的一行，也就是其实第二层的权重矩阵中的每一行 ，对应的是一个单词的1*300维度词向量。

![](https://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2019-04-04-035519.jpg)

这也是为什么这个矩阵称为映射矩阵的原因，其实10000行其实是对高维的onehot做了一层映射，那么到第三层的预测的时候，其实是拿该词的1\*300的词向量作为输入去进行相乘训练的，于是其实训练的其实就该词向量本身，这也是该矩阵能成为词向量的原因。

图示如下：

![](https://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2019-04-04-035549.jpg)

直觉上的理解：

如果两个不同的单词有着非常相似的“上下文”（也就是窗口单词很相似，比如“Kitty climbed the tree”和“Cat climbed the tree”），也就是输出很相似，那么通过我们的模型训练，这两个单词的嵌入向量将非常相似。

## 训练中的Trick

### 词对

类似"New York"专有名词，其实是可以作为一个词进行训练的。

### 高频词抽样

#### 高频词抽样原因

1. 对于"the"这种词，并不会给"the", "fox" 增加更多的语义信息。
2. 由于"the"这种词在文中出现的频次非常多，包涵"the"的样本数量，远远大于我们需要学习"the"这个词需要的训练样本。

#### 高频词抽样的结果

1. 删除"the"后，窗口内再也不会出现"the"，减少了训练的计算资源。
2. 删除"the"后，"the"作为input word的训练样本至少减少10个。

#### 抽样率

$Z(w_i)$ : 该词在语聊中出现的频次。

$P(w_i)$ ：代表保留某个单词的概率。
$$
P(w_i) = (\sqrt{\frac{Z(w_i)}{0.001}} + 1) \times \frac{0.001}{Z(w_i)}
$$

- $Z(w_i) < = 0.0026$ 时，$P(w_i)$ = 1。
- $Z(w_i) = 0.00746$ 时，$P(w_i)$ = 0.5。
- $Z(w_i) = 1.0$ 时，$P(w_i)$ = 0.033。

### 负采样

负采样每次训练，只更新除了"positive"之外，只更新"negative"最多20个词的权重。

> 对于小规模数据集，选择5-20个negative words会比较好，对于大规模数据集可以仅选择2-5个negative words。

这样权重就编程了300*21就好训练多了。

那么negative中的20个是怎么获得的呢？

#### 负采样频率

单词在文本中出现的频次：$p(w_i)$

该负样本被选中的概率为:
$$
P(w_i) = \frac{f(w_i)^{\frac{3}{4}}}{\sum_{j=0}^{n}(f(w_i)^{\frac{3}{4}})}
$$

#### 工程实现

1. 已经计算出每个单词的$p(w_i)$,乘以1亿，得出每个单词在1亿的数组中出现的次数。由此得到一个1亿的数组，包含了所有的单词(每个单词N个)。概率越大的词，在1亿数组中出现的次数越多，概率越小的词，在1亿数组中出现的次数越小。
2. 在0-1亿中，生成一个随机数，做为1亿的数组的索引，来选择单词。也就是负样本的词。一个单词的负采样概率越大，在表中出现的次数就越多，被选中的概率就越大。