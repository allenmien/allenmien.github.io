---
title: 负采样
---
# 负采样

### 问题

对word2vec中的三层神经网络而言，以CBOW举例：上下文来预测中间的词 $x_i$，也就是说，对于第三层的V个词来讲，只有$x_i$的softmax的概率是1，其他的(V-1)个词的概率应该都是0，那么最后一层需要更新的权重有多少呢？

![](https://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2019-04-03-032152.jpg)

300*N个。

这仅仅是训练一个样本中的一个单词的最后一层，就需要更新的权重就这么多，如果是全量的样本加进来，训练的资源消耗是非常大的。

负采样就是为了解决这个问题的。

### 负采样简介

负采样每次训练，只更新除了"positive"之外，只更新"negative"最多20个词的权重。

> 对于小规模数据集，选择5-20个negative words会比较好，对于大规模数据集可以仅选择2-5个negative words。

这样权重就编程了300*21就好训练多了。

那么negative中的20个是怎么获得的呢？

#### 负采样频率

单词在文本中出现的频次：$p(w_i)$

该负样本被选中的概率为:
$$
P(w_i) = \frac{f(w_i)^{\frac{3}{4}}}{\sum_{j=0}^{n}(f(w_i)^{\frac{3}{4}})}
$$

#### 工程实现

1. 已经计算出每个单词的$p(w_i)$,乘以1亿，得出每个单词在1亿的数组中出现的次数。由此得到一个1亿的数组，包含了所有的单词(每个单词N个)。概率越大的词，在1亿数组中出现的次数越多，概率越小的词，在1亿数组中出现的次数越小。
2. 在0-1亿中，生成一个随机数，做为1亿的数组的索引，来选择单词。也就是负样本的词。一个单词的负采样概率越大，在表中出现的次数就越多，被选中的概率就越大。