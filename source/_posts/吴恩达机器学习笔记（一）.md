---
title:      吴恩达机器学习笔记（一）
---
# Machine Learning

## Introduction

### What is machine learning

Tom Mitchell (1998) Well-posed Learning Problem: A computer program is said to learn

from experience E with respect to some task T and some performance measure P, if its
performance on T, as measured by P, improves with experience E.

#### 学习算法

- 监督学习
- 非监督学习

### 监督学习

#### 例子1：

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-05-23-144802.jpg)

如果一个人有房子，面积750，他想知道能卖多少钱？

可以用一条直线进行拟合，也可以用二元曲线进行拟合。

监督学习是指我们给算法一个数据集，其中包含了正确的答案。

这是一个回归问题。

#### 例子2：

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-05-23-150117.jpg)

假设你想看医疗记录，并且设法预测乳腺癌是恶性的还是良性的。

假设某人发现了一个乳腺肿瘤，机器学习的问题，是你能否估计出，这个肿瘤是良性的还是恶性的概率。

这是一个分类问题。

分类是指，我们设法预测一个离散值输出，0 or 1，

在这个问题中，只是使用了肿瘤大小这一个特征来表述，实际可能会有多个特征和多个属性。

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-05-23-151424.jpg)

机器学习做的，是画一条线，区分你的朋友是良性还是恶性肿瘤。

机器学习算法，不仅能处理两个到五个特征，而且能处理无穷多个特征。

### 无监督学习

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-05-23-153450.jpg)

我们得到一个数据集，却不知道拿他来干什么，能在其中找到某种结构吗？

机器学习可以判定该数据集包含两个不同的簇。这就是聚类算法。

一个聚类算法的应用例子就是谷歌新闻。谷歌自动的搜索新闻，并把他们分成一个又一个的专题。

### 模型描述

#### 线性回归

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/18-5-24/1556237.jpg)

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/18-5-24/21420988.jpg)

## 单变量线性回归

### 代价函数

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/18-5-24/36106145.jpg)

目标即是求代价函数的最小值，也就是能最拟合的模型。

也称平方误差函数，平方误差代价函数。

平方误差代价函数对大多数问题，都是一个合理的选择。是解决回归问题最常用的手段。

### 代价函数（一）

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-05-28-145815.png)

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-05-28-150347.png)

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-05-28-152502.jpg)



![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-05-28-152748.jpg)

### 代价函数（二）

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-05-28-153947.jpg)

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-05-28-153958.jpg)

当为3个参数时：

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-05-28-154102.png)

### 梯度下降法



![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-05-29-135839.png)

![ ](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-05-29-140347.png)

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-05-29-142953.jpg)

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-06-03-091331.png)

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-06-03-091854.png)

### 线性回归的梯度下降

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-06-07-131509.jpg)

![image-20180607211527357](/var/folders/zd/xt3vvjjx3bn0l5vwcjp4yszh0000gn/T/abnerworks.Typora/image-20180607211527357.png)

Batch 梯度下降算法

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-06-07-132556.jpg)

根据不同的初始起点，有可能到达不同的位置

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-06-07-132046.png)

线性回归的代价函数总是一个弓形函数，也就是凸函数

这个函数没有局部最优解，只有全局最优解，只要使用线性回归，总会收敛

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-06-07-132141.png)



## 线性回归回顾

### 矩阵和向量

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-06-07-135956.jpg)

### 矩阵的加法和乘法运算

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-06-09-132143.jpg)

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-06-10-022117.jpg)

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-06-10-032929.jpg)



## 多变量线性回归

### 多功能

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-06-10-094422.png)

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-06-10-095251.jpg)

多元线性回归

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-06-10-100414.jpg)

### 多元梯度下降法

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-06-10-101836.jpg)

### 多元梯度下降法演练1

#### 特征缩放

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-06-10-103540.jpg)

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-06-10-103554.jpg)

#### 均值归一化

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-06-10-104648.jpg)

### 元梯度下降法2--学习率

 ![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-06-10-110848.jpg)



![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-06-10-110857.jpg)

### 特征和多项式回归

### 正规方程

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-06-11-130142.jpg)

![](http://markdocpicture.oss-cn-hangzhou.aliyuncs.com/iPic/2018-06-11-133018.jpg)


 